Лекция «Введение в машинное обучение»
Санкт-Петербург
2019
Содержание
1 Введение 2
2 Задачи машинного обучения 4
2.1 Модельный пример и случайные величины . . . . . . . . . . . . 4
2.2 А на чем и как учиться? . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 А не искусственный ли интеллект? . . . . . . . . . . . . . . . . . 7
2.4 Карта мира машинного обучения . . . . . . . . . . . . . . . . . . 8
3 Классификация алгоритмов машинного обучения 9
3.1 Классическое обучение: обучение с учителем . . . . . . . . . . . 9
3.1.1 Задача регрессии . . . . . . . . . . . . . . . . . . . . . . . 11
3.1.2 Задача классификации . . . . . . . . . . . . . . . . . . . . 13
3.2 Классическое обучение: обучение без учителя . . . . . . . . . . . 14
3.2.1 Кластеризация . . . . . . . . . . . . . . . . . . . . . . . . 15
3.2.2 Ассоциации . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.2.3 Уменьшение размерности . . . . . . . . . . . . . . . . . . 17
3.3 Обучение с подкреплением . . . . . . . . . . . . . . . . . . . . . 18
3.4 Ансамблевые методы . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.5 Совсем немного о нейросетях и глубоком обучении . . . . . . . . 20
4 Чуть-чуть необходимой общей статистики 21
1
1 Введение
Добрый день, уважаемые слушатели! Добро пожаловать на курс «Вве-
дение в машинное обучение». Данный курс состоит из 5 лекций, в каждой
из которых мы расскажем и наглядно покажем современные подходы к ста-
тистической обработке данных и построению моделей в машинном обучении
(МО). Для усвоения предлагаемого материала будет достаточно элементар-
ного курса высшей математики. Все требующиеся факты и понятия мы будем
вводить (или напоминать) по мере необходимости. Ну что, приступим?
Для начала давайте все-таки разберемся, а что же такое машинное обу-
чение и при чем тут какая-то статистика? Ведь современные СМИ просто
переполнены такими соблазнительными и манящими вбросами, как: искус-
ственный интеллект, машинное обучение, восстание машин, программисты
программируют умных роботов, дата сайнтист, профессии будущего, и куча
всякого такого. И эти сказки – один из самых популярных и общедоступ-
ных типов статей по сабжу. Второй тип статей, и их меньшинство, – это
многостраничные томики, расписанные теоремами, узорчатыми формулами
с непонятными значками, графиками и много-много чем. И что мы получаем?
От СМИ мы получаем всяческие обещания прекрасного будущего, пере-
довой науки, полной автоматизации и вообще, о боже, чуть ли не создания
искусственного разумного человека. А от второго типа статей, который, вро-
де как, обеспечивает эти блестящие перспективы, мы бежим, как от огня,
ведь там почти ничего не понять и не разобрать. В итоге, сами что-то по-
пробовать сделать мы не можем, и верим в то, что кто-то, может быть, все
же сделает это для нас, а обещания превратятся в реальность. Да и вообще,
просто верим.
Идеяэтогокурса–пролитьсветнаосновныезадачииметодымашинного
обучения. Как мы покажем, многие задачи машинного обучения – это не что-
то из области фантастики. Это задачи, с которыми сталкивается каждый из
нас даже просто-напросто в быту. В то же время, способы решения этих
задач, конечно, основаны на математике, которую мы постараемся изложить
в максимально понятной и доступной форме.
В этой лекции мы, во-первых, договоримся о терминологии и ответим на
такие вопросы:
1. А зачем вообще обучать машины?
2. Из чего состоит обучение?
3. На чем и как учатся машины?
4. Где граничат и чем отличаются искусственный интеллект и машинное
обучение?
2
5. И многие другие...
Кроме того, мы расскажем об основных направлениях обучения: с учителем,
без учителя, с подкреплением, а также приведем основные примеры и классы
задач, решаемых в рамках этих подходов.
Начиная что-то изучать, неплохо бы хоть немного понимать, что рань-
ше: курица или яйцо, ну или хотя бы откуда ноги растут. Посему, начнем с
небольшого экскурса в историю. Идея машинного обучения сама по себе не
нова. Так в1950 уже были известны простейшие алгоритмы, в60-ых разрабо-
таны байесовские методы, но в70-ых наступила так называемая зима искус-
ственного интеллекта. Вторую жизнь МО стало приобретать с90-ых годов
прошлого века. В последние же годы, как и многое другое, машинное обуче-
ние претерпевает некоторое переосмысление. Это связано в первую очередь
с ростом производительности компьютеров, когда современный бюджетный
смартфон по характеристикам превосходит профессиональные компьютеры
15-ти летней давности.
В масштабах науки, МО – это одно из направлений искусственного ин-
теллекта.Машинноеобучениеиспользуетстатистическиеметоды,чтобыдать
компьютерамвозможность«учиться»,аосновнаяидеязаключаетсявисполь-
зовании предыдущего опыта для принятия решений в будущем. Так что на
традиционный провокационный вопрос: нужна ли программисту математика,
мы можем смело ответить – в данной области просто необходима.
Машинное обучение часто синонимично называют статистическим обу-
чением. Основная задача МО состоит в обнаружении и формализации раз-
личных закономерностей или в создании некоего правила, основываясь на
предложенных примерах. Именно благодаря такому подходу можно обнару-
жить сложные закономерности в данных (если они, конечно, есть), которые
не видны или неочевидны человеку. А имея достаточно большой набор ис-
ходных данных становится возможным научить машину вести себя похожим
образом, как и моделируемый объект.
Для обработки этого огромного количества данных и построения моде-
лей используется аппарат математической статистики. Чтобы понять с како-
го рода задачами нам придется сталкиваться, и как мы их будем описывать
математическим языком, разберем конкретный пример.
3
2 Задачи машинного обучения
2.1 Модельный пример и случайные величины
Давайте предположим, что мы хотим купить мобильный телефон, и при-
кидываем, какую сумму нужно накопить для его покупки. Посмотрев кучу
объявлений в интернете, мы делаем вывод, что новый телефон подходящей
марки c интересующими нас функциями стоит около 1000 долларов. При
этом полугодовалый такой же телефон стоит уже900 долларов, а годовалый
– 800. Наверное, у каждого в голове напрашивается вывод: цена телефона
каждые полгода падает на100 долларов. Мы решили задачу, которую в ма-
шинном обучении называют задачей регрессии – задачу предсказания числа
по некоторым входным данным.
Вообще, мы, люди, постоянно решаем в уме задачу регрессии. Мы оце-
ниваем сколько стоит тот или иной автомобиль, взглянув на внешний вид,
«внутренности» и узнав объем двигателя. Или задумываемся, а сколько про-
дуктов нужно брать с собой на дачу на выходные, если собирается вся семья.
Ну что, ничего не смущает? Неужели есть формула, описывающая все на
свете? Или все-таки во всех приводимых примерах есть очевидные изъяны?
Конечно, есть. Ведь странно считать, что остаточная стоимость мобильного
телефона зависит только от его возраста. А если у него, например, разбит
экран, или не работает камера? Видимо, цена должна упасть еще больше, не
так ли? А если телефон лежит в коробке, даже не распакованный, точно ли
его цена упадет так же стремительно, как и цена телефона, который активно
используют? Сомнительно, правда ведь? И, кстати, наша формула не учиты-
вает никакие такие детали, а ведь их масса! Ну или, например, с автомоби-
лями: ведь на реальную цену влияют наполнение конкретной комплектации,
техническое состояние автомобиля, пробег, страна сборки, использования и
многое-многое другое. И каков вывод?
А вывод таков: человек просто не может учесть все, ведь факторов, вли-
яющих на то или иное явление – очень много. Есть и еще одна мысль, которая
может перевернуть ваше сознание: иногда мы даже не знаем и не догадываем-
ся о тех факторах, которые влияют на интересующее нас явление, а значит и
тем более не можем их учесть. Тут-то на помощь и приходят машины. Конеч-
но, с терминами типа «факторы» и «то или иное явление» далеко не уедешь,
поэтому приведем еще один пример, на основе которого введем понятия, ко-
торыми и будем оперировать.
Предположим, что у нас есть данные о 100000 квартир в Санкт-
Петербурге, причем мы знаем такие параметры, как: площадь квартиры, ко-
личество комнат, этаж, район, наличие парковки, расстояние до ближайшей
станции метро и так далее. Кроме того, известна стоимость каждой кварти-
4
ры. Можем ли мы построить какую-то модель, которая будет предсказывать
стоимость квартиры по заданным параметрам?
В рассматриваемом примере площадь квартиры, количество комнат,
этаж и прочее – входные переменные. Их часто называют предиктора-
ми, независимыми переменными или просто переменными и обозначают
𝑋1, 𝑋2, ..., 𝑋𝑝. Конкретные их значения чаще обозначают маленькими бук-
вами 𝑥1, 𝑥2, ..., 𝑥𝑝.
Величина на выходе – это цена квартиры, которая основывается на неко-
торых значениях предикторов. Ее часто называют откликом или зависимой
переменной и обозначают𝑌 , а ее значения маленькими буквами𝑦.
Давайте поясним (как можно менее занудно) отличие больших и ма-
леньких букв. Это отличие, кстати, мотивировано не только математикой.
Ну, например, пусть 𝑋1 – это площадь квартиры. И что вы понимаете из
этого «например»? Площадь какой квартиры? Вашей, или квартиры друзей?
Или сотни квартир из объявлений? Они же все отличаются! Значит, пока мы
не говорим о конкретной квартире, о конкретном наблюдении, мы не можем
сказать, чему в точности равно значение этой самой площади𝑋1. В мате-
матике 𝑋1 принято называть случайной величиной. А вот при рассмотрении
конкретного наблюдения, конкретной квартиры, мы уже можем сказать, что
вот у этой-то квартиры площадь равна𝑥1. И это значение𝑥1 и есть значение
случайной величины𝑋1 для конкретной квартиры. Понятно?
Аналогично мотивируется и ситуация с𝑌 и 𝑦. Не зная конкретных зна-
чений случайных величин𝑋1, 𝑋2, ..., 𝑋𝑝, конкретного значения отклика𝑌 мы
тоже не знаем, значит и𝑌 – это случайная величина. Вот!
Ну хорошо, терминологию ввели, но что нам делать с этими ста тыся-
чами объявлений с кучей параметров? Ни один человек не может охватить
такой набор данных, выявить закономерности, отбросить лишнее. Конечно,
как мы уже намекали, нужно заставить работать машины.
2.2 А на чем и как учиться?
Итак, для того, чтобы машина работала, то есть предсказывала резуль-
тат по входным данным, ее нужно обучить. Чем разнообразнее обучающие
данные, чем их больше, тем больше прецедентов наблюдает машина, а зна-
чит тем ей проще найти закономерности и, вроде как, тем точнее результат.
Значит, во-первых, нам требуются данные.
Хотите предсказывать цену квартиры – отберите кучу объявлений о про-
даже. Хотите определять спам – дайте примеры спам-писем. Хотите предуга-
дывать курс доллара – возьмите историю многолетней давности. Чем больше
всяких разнообразных данных – тем лучше, ведь обучение основано на пре-
цедентах, то есть на принципе: «если было так, а стало сяк, то, когда снова
будет так, в результате будет сяк». Наивно? Возможно, но что вы еще хотите
5
сказать про будущее? Если данные точны, а условия одинаковы (кстати, это
тоже данные), то и выхлоп должен быть одинаков. Это отсылка и к законам
физики, и математики, и много к чему. Только мы все время натыкаемся на
одну и ту же проблему – а все ли мы учитываем, и не учитываем ли что-то
лишнее?
Как собирать данные, откуда они берутся? Да откуда угодно. Их можно
собирать вручную, что, видимо, приводит к меньшему числу ошибок, а мож-
но автоматически. Любимый пример – гугловскаяReCaptcha, которая просит
искать то светофоры, то автомобили, то дорожные знаки. Так вот, каждый
раз, когда вы устало тыкаете по нужным квадратикам, вы передаете «экс-
пертно оцененные» данные серверу, а потом эти данные формируют хороший
датасет для обучения беспилотных автомобилей и проч. За качественными
датасетами идет охота. В настоящее время известное высказывание Архиме-
да давно перефразировали: «Дайте мне качественные данные, и я переверну
мир».
Из огромного набора данных нам требуется выделить так называемые
признаки – это и есть те предикторы и отклики, на которые машина должна
смотреть при обучении. Когда предикторов много, модель учится и работает
медленнее, значит и менее эффективно. Иногда лишние предикторы вооб-
ще могут мешать работе модели. В прочем, есть и алгоритмы, помогающие
отобрать наиболее интересные признаки из большого набора, мы к этому еще
вернемся.
Рис. 1: Диаграммы терминов
Ну и последнее – это алгоритм. Одну и ту же задачу часто можно решать
по-разному. От выбора метода решения будет зависеть и скорость работы
модели, и точность ее предсказаний. Но есть одно важное НО! Если данные,
6
кхм, не очень, то ни один алгоритм делу не поможет.
В итоге все сказанное можно изобразить на такой вот картинке, которая
показывает достаточно субъективное, но все же отличие машинного обучения
от таких хайповых терминов, как: наука о данных (DataScience), интеллек-
туальный анализ данных (DataMining) и от стандартного программирования
(ClassicalProgramming).
2.3 А не искусственный ли интеллект?
Давайте еще немного скажем о «множественных включениях», а именно
ответим на вопрос: как соотносятся машинное обучение и искусственный ин-
теллект? Часто вообще термины искусственный интеллект, машинное обуче-
ние, нейронные сети отождествляются и употребляются бессистемно. Между
тем, это не одно и то же.
Искусственный интеллект (ИИ) появился в1956, а его целью было, в
общем-то, как и сейчас, заставить компьютер решать задачи, которые, как
считается, подвластны в основном только людям, то есть те задачи, которые
требуют интеллекта. Изначально исследователи работали над различными
головоломками и задачей игры в шашки. Сейчас ИИ может относиться к
чему угодно – от компьютерных программ для игры в те же шашки, до го-
лосовых помощников, способных не только распознавать речь, но и отвечать
на вопросы. Можно сказать, что искусственный интеллект относится к вы-
водам компьютера: компьютер делает что-то «неглупое», а значит проявляет
интеллект. Интеллект, в свою очередь, искусственный.
Рис. 2: МО, как часть ИИ
7
Машинное обучение – это одно из направлений искусственного интел-
лекта. Основной принцип МО – обучение на заранее данном наборе данных.
В прочем, об этом мы уже говорили в примерах.
Глубокое обучение – это, так сказать, подмножество машинного обуче-
ние. Оно использует аппарат нейронных сетей, имитирующих человеческое
мышление и, как следствие, принятие решений, для решения реальных задач.
Мы не будем касаться глубокого обучения в нашем курсе.
Итак, общая схема, соответствующая описанному, представлена на ри-
сунке 2.
Ну и давайте раз и навсегда, достаточно грубо скажем: а что может на
сегодняшний день обученная машина? Итак, она может: строить предсказа-
ние, запоминать новое, воспроизводить имеющееся, выбирать лучшее. А чего
не может? Уж точно не может выйти за рамки задачи, создать новое и пора-
ботить весь мир.
2.4 Карта мира машинного обучения
Посмотрите внимательно на рисунок. В нем каким-то образом класси-
фицированы те или иные алгоритмы, относящиеся к машинному обучению.
Способов классификации – масса, и здесь представлен всего один из множе-
ства возможных. Основные ветки машинного обучения – это классическое
обучение, делящееся на обучения с учителем и без, обучение с подкреплени-
ем, ансамблевые методы и нейронные сети с глубоким обучением.
Давайте теперь поговорим чуть подробнее про наш курс. Мы, в основ-
ном, будем заниматься обучением с учителем. Во второй лекции мы подроб-
но разберем задачу регрессии: линейную регрессию, многомерную линейную
регрессию, полиномиальную регрессию, а также скажем пару слов про греб-
невую или ридж-регрессию и метод регрессии LASSO. В третьей лекции мы
затронем задачу классификации: обсудим классификатор на основе логисти-
ческой регрессии, а также сравним его с линейной регрессией. В четвертой
лекции мы продолжим заниматься классификацией и рассмотрим наивный
байесовский классификатор, а также метод𝑘 ближайших соседей, погово-
рим про проклятие размерностей. Пятая же лекция перенесет нас в раздел
обучения без учителя – к задаче кластеризации: мы рассмотрим алгоритмы
𝐾-средних и агломеративную (иерархическую) кластеризацию.
Для изучения алгоритмов уменьшения размерности, ансамблевых мето-
дов и обучения с подкреплением, мы приглашаем вас в курс, являющийся
продолжением данного – «Advanced Machine Learning».
Ну что же, во введении, то есть в данной лекции, мы кратко пройдемся
по каждой нарисованной ветке и расскажем про задачи, которые решаются
теми или иными методами.
8
Рис. 3: Карта мира МО
3 Классификация алгоритмов машинного обучения
3.1 Классическое обучение: обучение с учите-
лем
Все алгоритмы классического обучения основаны, в основном, на ап-
парате математической статистики, так что для понимания происходящего
придется что-то да вспомнить. Но, честно говоря, они, в большинстве своем,
настолько просты, что на пальцах объясняются даже ребенку. Но не нуж-
но недооценивать эти методы: решение доброй половины задач современного
общества основано именно на них.
Итак, классическое обучение делится на обучение с учителем и без. В
обучении с учителем, что ясно и из названия, есть некий учитель, который
говорит, как правильно. Иными словами, есть не только входные данные, но
и реальные выходные (как ответы в сборнике задач). Например, объявления
о продаже квартир содержат в себе «верные», установленные цены на жилье.
В итоге, при обучении с учителем всегда есть некий тренировочный набор
данных – набор, на котором модель обучается.
Обучение без учителя выходных данных не содержит. При обучении без
учителя на машину просто вываливают гигабайты входных данных и говорят
9
что-то типа: ну, разберись, что тут к чему? Зачастую люди и сами не знают,
что и к чему, и даже не знают есть ли что-то интересное в этих данных, а ма-
шина пытается найти скрытые закономерности и разложить все по полочкам.
Представьте, например, что несколько однотипных огромных книг рассыпа-
лись на страницы, причем страницы не пронумерованы. Как думаете, легко
собрать из этих страниц книги в их первоначальном виде? А если книги, к
тому же, на иностранном языке? А если вы не знаете изначально исходное
количество книг? Ох.
Обучение с учителем, как мы видели, и как снова же отражено на рисун-
ке, глобально можно разделить на две ветки – регрессия и классификация.
Рис. 4: Классическое обучение
10
3.1.1 Задача регрессии
Задачу регрессии мы уже не раз вспоминали. Задача регрессии – это
задача предсказания числа: цены квартиры или телефона, курса доллара
на завтра, ожидаемого объема продаж, медицинских показателей до/после
лечения и так далее. По сути своей компьютер решает задачу, известную
из детства: есть набор точек, иллюстрирующих данные, и нужно их как-
то соединить, чтобы получилось красиво и логично. И вот с этим-то самые
большие проблемы.
Вот вам пример такой задачи, взгляните на рисунок 5. Данные, кстати,
близки к реальным, хоть и сгенерированы. Рисунок показывает зависимость
добавки к стоимости квартиры (в тысячах долларов) от времени транспорт-
ной доступности до ближайшего метро (в минутах). Всего сгенерировано100
рекламных объявлений. В качестве предиктора𝑋 выступает время, а в ка-
честве отклика𝑌 – добавка к стоимости.
Рис. 5: Добавка к стоимости в зависимости от удаленности от метро
Ну и как же соединить точки, какая хоть примерно зависимость? Конеч-
но, можно сделать то, что приходит, наверное, первым в голову, но сомни-
тельно, что цена ведет себя именно так как показано на рисунке 6.
С этим «приближением» куча непоняток: чем объясняются такие скач-
ки то вверх, то вниз? Почему функция такая сложная и зубчатая (или, как
говорят математики, не гладкая)? Как получить вид функции для предска-
зания, какое у нее аналитическое задание? Все эти вопросы восходят к так
называемой проблеме интерпретации модели – ее толкованию и трактованию.
Регрессия предлагает весьма наглядные модели: линейные или полино-
миальные. Синим (рисунок 7) показано приближение линейной функцией (от
11
Рис. 6: Наивная оценка истинного распределения
Рис. 7: Регрессия
того и линейная регрессия), зеленым – квадратичной, а фиолетовым – ку-
бической. Видно, что квадратичное и кубическое приближения очень даже
неплохо повторяют форму зависимости. Конечно, ошибки обусловлены ошиб-
ками модели, но такая модель, очевидно, очень наглядна и легко объясняема.
Подробнее о том, как находить коэффициенты, мы обсудим во второй лекции.
12
3.1.2 Задача классификации
Задача классификации – задача отнесения объекта к одному из заранее
известных классов. Не хотим вас пугать, но и задачи классификации вы ре-
шаете не менее регулярно, чем задачи регрессии. Скажем, после стирки носки
нужно разделить по цвету (красные, синие, черные) и по размеру (большие
– мужа, маленькие – жены). А при разборе сумок после похода в магазин
овощи часто складываются в одну корзину, а фрукты – совсем в другую. И
уж порошок точно среди них не валяется.
В итоге – задача классификации – это та задача, которую уже и ребенок
решает с самого рождения: большое к большому, маленькое к маленькому,
машинки к машинкам, а роботы – к роботам. А что, если это трансформер?
Тут и взрослый встанет в ступор.
Рис. 8: Детский классификатор
Конечно, зачастую решаются более сложные задачи. Например, когда
человек приходит в банк, чтобы взять кредит, сотрудник банка должен вы-
яснить: кредитоспособен человек или нет. Понятно, что это делается на осно-
ве персональных данных человека: его дохода, его места работы, семейного
положения, возраста, наличия других кредитов и многого-многого другого.
Проанализироваввсюсводкуинформации,банковскийработникдолженпри-
нять решение, к какому классу отнести этого «потенциального клиента» –
«кредитоспособен» или «некредитоспособен».
13
Другой пример задачи классификации – обнаружение спама в электрон-
ной почте. Почтовый клиент должен определить, принадлежит ли письмо
нежелательной почте (спаму) или нет. Конечно, это делается на основе раз-
личных данных, как, например, количество адресатов, количество слов «ку-
пить», «продать», «заработать», «репутации» данного адреса у почтового
клиента. К вопросу фильтрации спама мы еще непременно вернемся в лекции
о байесовском классификаторе.
Задача классификации вовсе не обязана иметь лишь два исхода. Напри-
мер, задача распознавания рукописных цифр, будучи задачей классифика-
ции, включает десять исходов, а задача распознавания слов – такое коли-
чество исходов, что его сложно даже произнести (а точно посчитать – тем
более). Есть разные методы классификации. В этом курсе мы рассмотрим с
вамилогистическуюрегрессию,наивныйбайесовскийклассификаториметод
𝑘-ближайших соседей, а в курсе «Advanced Machine Learning» еще и машины
опорных векторов (SVM).
3.2 Классическое обучение: обучение без учите-
ля
Мы уже сказали несколько слов про обучение без учителя ранее. Как
утверждается, оно изобретено в90ых годах, то есть гораздо позже, чем обу-
чение с учителем. Используется обучение без учителя реже, но иногда про-
сто нет выбора: ведь данные вовсе не всегда размечены, и далеко не всегда
в них наведен должный порядок. Итак, согласно рисунку, обучение без учи-
14
теля включает в себя три основные ветки: кластеризация, поиск ассоциаций,
уменьшение размерности.
Рис. 9: Классическое обучение
3.2.1 Кластеризация
Задача кластеризации тесно связана с задачей классификации. Навер-
ное, каждому знакомы оба этих термина, но далеко не каждый может сразу
сказать: а чем они отличаются? Так вот, кластеризация – это та же класси-
фикация, но без заранее известных кластеров. Машина сама должна искать
похожие объекты и объединять их в классов. Количество кластеров заранее
тоже неизвестно и в алгоритмах либо задается заранее человеком, либо, снова
же, дается на откуп машине.
Посмотритехотябынарисунок:налевомпредставленыкартинкидевяти
котов. Ну и на сколько бы кластеров вы их разбили? Наверное, это зависит
от степени погруженности в детали. На первый взгляд – это все просто коты.
Но если начать решать задачу «найди отличия», то можно заметить, что у
некоторых котов зрачок круглый, а у некоторых – овальный. Кроме того,
15
Рис. 10: Немного о кластеризации
у части котов радужная оболочка глаза желтая, у части – коричневая, а у
части и вообще зеленая. Да и вообще, коты еще отличаются по окрасу: есть
серые, есть голубые, есть серо-полосатые, есть бело-полосатые. Да, кстати, а
это коты или кошки? Ну и сколько кластеров у нас уже получилось? А ведь
если пригласить специалиста по котам, он, наверное, назовет еще с десяток
отличий.
Тут естественным образом возникает вопрос: а сколько кластеров ре-
ально нужно? Это, естественно, зависит от задачи. Если достаточно просто
определитьтипживотного,тохватитиодного,аеслиужеипороду,то,скорее
всего, перечисленных нами не хватит. И мы еще в выигрышном положении:
мы хотя бы знаем (вроде как), что все, что на картинках – представители
кошачьих.
На второй картинке все менее радужно: тут присутствуют и пирожные,
и выпечка, и какие-то непонятные то ли значки, то ли фишки. Как тут разо-
браться, если и экспертно, с помощью человеческого интеллекта, ничего не
понять?Аужкактутработатьмашине,какиекластерывыбрать?Съедобное-
нет, сдобное-нет, по форме, по цвету, по начинке, по оформлению... Да вари-
антов просто тьма. А может просто – все круглые? Согласитесь, задача не
кажется простой.
В нашем курсе мы рассмотрим несколько алгоритмов кластеризации: это
метод 𝐾-средних и агломеративная (иерархическая) кластеризация.
3.2.2 Ассоциации
Поиск ассоциаций, наверное, одна из самых свежих и мало разработан-
ных веток машинного обучения. Сюда входят и анализ продуктовой корзины,
16
и прогноз акций и распродаж, и расстановка товаров на полках и так далее.
Например, вы покупаете чай. Стоит ли на вашем пути поставить разные
сладости: печеньки, конфеты, торты? А стоит ли тут же выставить кофе?
Как часто эти продукты берут вместе? Как часто, взяв чай, человек захочет
взять и кофе? А может тогда рядом с кофе нужно поставить молоко для
коктейлей, или, скажем, какие-то сиропы? Ведь если этого не будет, то поку-
патель может просто забыть купить тот или иной товар. А часто ему может
просто не прийти это в голову, ведь раньше он и думать не думал добавить
в кофе корицу или, например, карамельный сироп. Если вы владелец сети
гипермаркетов, то эти вопросы будут крайне актуальны!
Илиинтернет-магазины,даилюбыеинтернет-сервисы.Вызамечали,что
стоит вам только поискать, скажем, какую-то бытовую технику (в прочем,
как и любой другой товар), то в течение продолжительного времени то и
дело сбоку экрана вы будете получать предложения различных магазинов по
интересующим вас товарам и такие, и сякие, и с перламутровыми пуговицами
– только купи. Это и есть – ассоциации.
3.2.3 Уменьшение размерности
Часто так бывает, что в данных имеется настолько много предикторов,
что ничего не понять. Мы не можем данные ни визуализировать, чтобы вы-
сказать какие-то предположения о зависимостях (так как не хватает размер-
ности пространства), ни понять вообще: а что в этих данных происходит.
Тогда часто имеет смысл объединить признаки в один и получить какую-то
более общую абстракцию. Ну например. Предположим, что у нас имеются
данные о птицах в городе: среднего размера, серые, редко белые, иногда ко-
ричневые, с красными лапами. Ну что, можно выкинуть все эти данные и
схлопнуть их до одного – перед нами голубь. Конечно, мы теряем информа-
цию о конкретном голубе (да и иногда это может быть и вообще не голубь),
но, честно говоря, абстракция голубь несет в себе куда больше смысла, чем
все эти цвета, лапы и размеры. Да и модель обучается на меньших размер-
ностях куда лучше и быстрее, чем на больших.
А иногда данные просто мало отличаются. Представьте, что мы рассчи-
тываем среднюю массу продуктовой корзины человека с какими-то парамет-
рами, и, например, знаем, что печенье весит500 грамм ±5%, что является
погрешностью производителя. Ну и зачем нам это знать в нашей конкрет-
ной задаче? Ясно, что достаточно просто считать, что если человек берет
упаковку печенья, то масса увеличивается на500 грамм, и все! Вот и опять
уменьшение размерности.
С некоторыми методами снижения размерности мы познакомимся с вами
в курсе «Advanced Machine Learning».
17
3.3 Обучение с подкреплением
Обучение с тически порог искусственного интеллекта. Почему? Да пото-
му что по сути дела машина просто-напросто пытается выжить в реальной
среде. За каждое действие она получает баллы: положительные, если она сде-
лала хорошо, нейтральные, если, условно, ничего особо не поменялось, или
отрицательные, если опростоволосилась. Цель, как и у студентов, набрать
как можно больше баллов! Почему это удобно?
Рис. 11: Обучение с подкреплением
Ну смотрите, есть такая достаточно известная и весьма сложная игра
Го. И машина не так давно научилась обыгрывать человека. И что, скажете
вы? Вроде в шахматах это происходит достаточно давно. Так вот, сравни-
тельно недавно было доказано, что число комбинаций в игре Го физически
невозможно просчитать, так как этих комбинаций больше, чем атомов во все-
ленной (число комбинаций состоит из 171 одной цифры :)). А вот в шахматах
машине вполне себе реально просчитать просто ВСЕ будущие ходы, а значит
ей достаточно просто выбрать лучшую для себя ситуацию. Чувствуете разни-
цу? В Го требуется что-то кроме лобового брутфорса (перебора)! Интеллект,
что ли?
Ну и что же делает машина? А она просто старается выйти из каждой
ситуации с меньшими потерями, с наилучшими возможными баллами. Для
этого машина производит миллионы симуляций в среде, считает свои очки,
и выбирает что-то лучшее. Только вот проблема: а кто сказал, что исчерпа-
ны и изучены все ситуации? Скажем, вот стоите вы на перекрестке, горит
зеленый. И что, можно переходить? А вдруг из-за угла поворачивает води-
тель и не видит вас, а вы, в свою очередь, в наушниках, и не слышите визга
18
колес? Вы уверены, что машина просчитала такую ситуацию? А она вообще
знала о такой возможности? Пока нет окончательного ответа на то, как пре-
дупреждать такие ситуации, исследователи придумывают все новые и новые
«костыли» – локальные решения проблемы, а глобальных решений пока что
нет.
Идея описанного лежит в основе алгоритма Q-learning, обсуждаемого в
курсе «Advanced Machine Learning», и она очень условно проиллюстрирована
на рисунке. За правильные ходы машина получает плюшки в виде плюсов, а
за неправильные – минусы. В некотором смысле, обучение можно сравнить
с дрессировкой животного: за верно выполненную команду вы поощряете
собаку лакомством или игрушкой, а за неверное строго отчитываете, а иногда
даете и подзатыльник. Вот и вся схема.
Рис. 12: Q-learning
3.4 Ансамблевые методы
На данный момент ансамблевые методы являются одними из самых точ-
ных при решении различных задач. Идея достаточно проста и логична – объ-
единить разные методы по принципу дополнения друг друга: где один метод
несправилсяидопустилмногоошибок,тамегоошибкиисправитдругой.При
этом даже лучше, чтобы методы были скорее нестабильными и плавающими
на входных данных, нежели суперэффективными и устойчивыми. Давайте
коротенько пройдемся по некоторым подходам.
19
Стекинг действует по следующему принципу: обучи несколько различ-
ных алгоритмов и передай их на вход последнему, который и примет финаль-
ное решение.
Беггинг предполагает обучение одного и того же алгоритма много раз
на случайных выборках из исходных данных, а в конце усреднение ответов.
Бустинг предлагает обучать алгоритмы последовательно, причем при
каждом следующем обучении новый алгоритм уделяет особое внимание
ошибкам предыдущего. Как и в беггинге, выборки берутся из исходных дан-
ных, но не совсем случайно, что, правда, уже было сказано: к данным обяза-
тельно добавляются те, на которых ошибся предыдущий алгоритм.
Рис. 13: Ансамблевые методы
Эти методы уже не так хорошо иллюстрируются, как предыдущие, их
мы подробно обсудим в курсе «Advanced Machine Learning».
3.5 Совсем немного о нейросетях и глубоком
обучении
Давайте начнем с того, что такое нейросеть. Нейросеть – это набор ней-
ронов и связей между ними. Нет, вовсе не как в нашем организме. По сути
дела нейрон – это функция с кучей входов и одним единственным выходом.
20
В итоге, нейрон выполняет вот какую задачу: на вход он берет всякие числа с
«входов», что-то с ними делает (применяет к ним некоторое преобразование
или функцию), и выдает на выход результат проделанной работы.
Что же такое связь? Связь – это канал, через которые нейроны шлют
друг другу какие-то числовые значения. У каждого канала есть свой вес (ну
или пропускная способность, или прочность связи). Например, если через
связь с весом0.5 проходит число10, то на вход нейрону подается число0.5 ·
10 = 5.
Рис. 14: Пример нейрона
Чтобы связи не были какими попало, нейроны решили связывать по сло-
ям: внутри одного слоя нейроны не связаны между собой, а соединены с
нейронами только предыдущего и следующих слоев. В реалиях все это пред-
ставляется матрицами и вычисляется средствами линейной алгебры. Ну да
не будем об этом.
В принципе, все. Мы рассмотрели основные задачи и схемы машинного
обучения и готовы приступить к самим алгоритмам. Но для начала немного
самой базовой статистики.
4 Чуть-чуть необходимой общей статистики
Мы, в общем-то, будем обсуждать различные статистические методы
непосредственно перед задачами, для которых они нам потребуются. Но все-
таки нам кажется полезным напомнить, как определяются основные харак-
теристики (одномерной) выборки.
Все обычно начинается с генеральной совокупности. Генеральная сово-
купность 𝜉 – это и есть та случайная величина, с которой мы имеем дело,
21
которая дает нам данные. Давайте сразу привяжемся к какому-то примеру.
Пусть генеральная совокупность 𝜉 – это та сумма денег, которую человек
оставил в супермаркете – вполне себе такие данные. Ясно, что значение этой
суммы, то есть значение𝜉, меняется как от человека к человеку, так и у одно-
го и того же человека при разных походах в магазин. На практике множество
значений 𝜉 всегда конечно, ведь, например данные берутся за прошедший
месяц (да и просто за какой-то период времени), а за месяц было обслужено
конечное число покупателей.
Мы считаем, что случайная величина𝜉 подчиняется какому-то закону,
этот закон мы будем называть ее распределением. Наверное, из курса ста-
тистики вы помните популярные распределения: нормальное, равномерное,
распределение Бернулли, биномиальное распределение. На данном этапе не
очень важно понимать досконально что означает этот термин – распреде-
ление, но важно понимать одно: мы предполагаем, что какой-то закон, по
которому меняется 𝜉, есть. А законы отличаются чем? Правильно, вероят-
ностями значений. В нашем примере, ясное дело, количество потраченных
денег не может быть отрицательным. Значит, у нашей случайной величины
такое распределение, что вероятность принять отрицательные значения рав-
на нулю. В то же время, в супермаркете редко можно увидеть кого-то, кто
заплатит больше 500 долларов. Значит вероятности того, что𝜉 принимает
значения большие, чем500 долларов чрезвычайно малы. Ну а что там меж-
ду этими границами? Это, похоже, и интересно узнать. Как? По выборке.
Выборкой объема (или размера) 𝑛 из генеральной совокупности 𝜉 на-
зывается набор независимых случайных величин 𝑋1, 𝑋2, ..., 𝑋𝑛, имеющих
распределение такое же, как и𝜉. Часто еще говорят о случайном векторе
⃗𝑋 = (𝑋1, 𝑋2, ..., 𝑋𝑛). Мотивировка остается прежней: если поменять день
или час наблюдения, то поменяется и выборка, именно потому элементы вы-
борки – случайные величины. При конкретном же наблюдении выборка – это
набор из𝑛 чисел 𝑥1, 𝑥2, ..., 𝑥𝑛 или ⃗𝑋 = (𝑥1, 𝑥2, ..., 𝑥𝑛).
Оказывается, по выборке можно оценивать математическое ожидание,
дисперсию, среднее отклонение случайной величины. Давайте вспомним, как
это делается?
Что такое математическое ожидание? Если говорить на пальцах, то это
среднее вероятностное значение рассматриваемой случайной величины. Что
означают эти слова? Ну смотрите, давайте немного упростим рассматривае-
мый общий пример. Будем рассматривать только тех людей, кто ничего не
потратил, или потратил очень много. Величина чека равна нулю, если по-
купатель ничего не купил в магазине – не такое уж редкое явление. В то
же время потраченная сумма, большая500 долларов, хоть и возможное, но
очень редкое явление. Зададимся вопросом, сколько «в адекватном среднем»
тратит человек на покупку в магазине (еще раз, при рассмотрении только
22
тех, кто либо ничего не тратит, либо очень много)? Ну что, среднее арифме-
тическое? То есть
Средние траты= 0 + 500
2 = 250?
Не кажется ли это странным? Ведь, скажем (данные взяты из головы), на100
учтенных покупателей, попадающих под критерии,99 тратят ноль, и только
один тратит500. Видимо, резонно записать это так:
𝜉 0 500
P 99
100
1
100
.
Это знакомая запись – таблица распределения дискретной случайной величи-
ны. В первой строчке стоят значения, а во второй – вероятности (вычислен-
ные на основе частоты). Что разумно назвать математическим ожиданием
E𝜉? Похоже, величину
E𝜉 = 0· 99
100 + 500· 1
100 = 5.
Не кажется ли вам, чисто интуитивно, что полученная величина куда ближе
к истине?
Но тут проблема: значений вероятностей мы не знаем. Именно поэтому в
статистике математическое ожидание мы хотим оценить (характеристика-то
полезная). Легко понять, что кандидатом является так называемое выбороч-
ное среднее
𝑋 = 𝑋1 + 𝑋2 + ... + 𝑋𝑛
𝑛 ,
которое на конкретном наблюдении⃗𝑋 = (𝑥1, 𝑥2, ..., 𝑥𝑛) превращается просто
в среднее арифметическое элементов выборки
𝑋 = 𝑥1 + 𝑥2 + ... + 𝑥𝑛
𝑛 .
Рассмотрим, для примера, вот такую выборку
𝑋 = (0, 11, 2, 3, 9, 2, 8, 6, 3.4, 8, 7.5, 9, 4, 8, 6).
Тогда выборочное среднее вычисляется, как
𝑋 = 0 + 11 + 2 + 3 + 9 + 2 + 8 + 6 + 3.4 + 8 + 7.5 + 9 + 4 + 8 + 6
15 ≈5.793.
На рисунке 15 синие точки – это элементы выборки. Красная точка отвечает
выборочному среднему𝑋.
23
Рис. 15: Выборка𝑋 и ее среднее𝑋
Вотвидите,краснаяточкаиправдапримерно«посередине»нашихсиних
точек – элементов выборки. Можно доказать, что с ростом объема выборки
выборочноесреднеевселучшеилучшеприближаетистинноематематическое
ожидание генеральной совокупности𝜉.
Вторая важная характеристика – это разброс. Все на том же рисунке
отчетливо видно, что синие находятся не очень-то близко друг к другу. Есте-
ственнозадатьсявопросом:анасколькоониразлетаютсяотсреднеговразные
стороны? В какой-то степени можно провести аналогию со стрельбой по ми-
шени: хочется попасть в «яблочко», но зачастую это не удается, попадание
происходит в какую-то другую точку мишени.
Вот и в примере с покупками в гипермаркете, мы можем примерно по-
нять: насколько велик разброс от среднего? Зная разброс, можно прогнозиро-
вать выручку за какой-то период времени как в «худшем», так и в «лучшем»
случаях. Давайте что-то посчитаем. В примере с гипермаркетом мы рассмат-
ривали случайную величину𝜉, имеющую следующее распределение:
𝜉 0 500
P 99
100
1
100
и математическое ожидание
E𝜉 = 0· 99
100 + 500· 1
100 = 5.
Ичто,разбросравен max |𝜉−E𝜉|= 495?Опятьже,странно,еслиэтотак,ведь
значение500 крайнемаловероятно.Можетопять,взятьсреднийразброс?Вот
24
и появляется дисперсия, которая, как мы напомним, определяется, как
D𝜉 = E (𝜉 −E𝜉)2 .
Легко видеть, что распределение случайной величины(𝜉 −E𝜉)2 = (𝜉 −5)2
описывается следующей таблицей:
(𝜉 −5)2 25 4952
P 99
100
1
100
.
Ну а тогда
D𝜉 = E (𝜉 −5)2 = 25· 99
100 + 4952 · 1
100 = 2475.
Ого, это ведь огромное число! Намного больше, чем то, что мы получали
весьма «наивным» способом. Мы нигде не напортачили? Смотрите, мы же
вычисляем математическое ожидание квадрата и получаем на самом деле
что-то вроде среднего квадрата разброса. Сам же разброс может быть оценен
так называемым среднеквадратическим отклонением𝜎, равным
𝜎 =
√︀
D𝜉.
В нашем примере𝜎 =
√
2475 ≈49.75, что куда ближе к истине.
Снова, так как распределение вероятностей обычно неизвестно, то дис-
персию нужно оценить. Хорошей оценкой дисперсии оказывается величина
𝑆2 = 1
𝑛
𝑛∑︁
𝑖=1
(︀
𝑋𝑖 −𝑋
)︀2
.
Иногда используют и такую оценку
𝑆2
0 = 1
𝑛 −1
𝑛∑︁
𝑖=1
(︀
𝑋𝑖 −𝑋
)︀2
,
но мы не будем вдаваться в такие детали. Отсюда, конечно, получается и
оценка для отклонения𝜎:
𝜎*=
⎯⎸⎸⎷1
𝑛
𝑛∑︁
𝑖=1
(︀
𝑋𝑖 −𝑋
)︀2
, 𝜎 *
0 =
⎯⎸⎸⎷ 1
𝑛 −1
𝑛∑︁
𝑖=1
(︀
𝑋𝑖 −𝑋
)︀2
Вернемся к нашему совсем синтетическому примеру – к выборке
𝑋 = (0, 11, 2, 3, 9, 2, 8, 6, 3.4, 8, 7.5, 9, 4, 8, 6).
25
Проведя вычисления, получим, что 𝑆2 ≈9.63 и 𝜎 ≈3.1. Это значит, что
в большинстве случаев значения нашей случайной величины должны нахо-
диться в диапазоне (︀
𝑋 −𝜎*, 𝑋 + 𝜎*)︀
.
Рис. 16: Выборка𝑋 и ее параметры
На рисунке представлены выборка, изображенная синими точками, вы-
борочное среднее, красной точкой, и границы интервала𝑋 ±𝜎* зелеными
точками. Как видно, большинство элементов выборки находятся как раз-таки
между зелеными точками.
Думаем, этих предварительных замечаний достаточно, чтобы окунуться
в методы машинного обучения, а также в приложения этих методов к раз-
личным задачам. Удачи :)
26
