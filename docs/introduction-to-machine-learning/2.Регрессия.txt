Регрессия
Высшая школа цифровой культуры
Университет ИТМО
dc@itmo.ru
Содержание
1 Простейшая линейная регрессия 2
1.1 Линейная регрессия и МО . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Модель простейшей линейной регрессии и метод наименьших
квадратов . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Пример: затраты времени на покупки . . . . . . . . . . . . . . . 9
1.4 Построение доверительных интервалов для коэффициентов ре-
грессии . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.5 Немного об интерпретации доверительных интервалов . . . . . 21
1.6 Доверительные интервалы для примера . . . . . . . . . . . . . . 22
1.7 Проверка гипотез . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.8 Проверка гипотез для примера . . . . . . . . . . . . . . . . . . . 24
1.9 Оценка точности модели . . . . . . . . . . . . . . . . . . . . . . . 25
1.10 Оценка точности модели для примера . . . . . . . . . . . . . . . 26
2 Множественная линейная регрессия 27
2.1 Основные определения и матричные обозначения . . . . . . . . 27
2.2 МНК для множественной регрессии . . . . . . . . . . . . . . . . 29
2.3 Статистическая оценка параметров множественной линейной
регрессии . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4 Оценка точности модели множественной регрессии . . . . . . . 32
2.5 Гипотеза о проверке статистической значимости линейной ре-
грессионной модели . . . . . . . . . . . . . . . . . . . . . . . . . 32
3 Немного о полиномиальной регрессии 33
4 Заключение 34
Высшая школа цифровой культуры Университет ИТМО
1 Простейшая линейная регрессия
Итак, мы приступаем к решению первой задачи обучения с учителем
– задаче регрессии. Как уже отмечалось, задача регрессии – это задача
предсказания числа (или отклика) 𝑌 по значениям входных переменных
𝑋1, 𝑋2, ..., 𝑋𝑝 (или предикторов). Функцию𝑓(𝑋), отвечающую зависимости
𝑌 = 𝑓(𝑋1, 𝑋2, ..., 𝑋𝑝), мы будем предполагать линейной, а наша задача будет
заключаться в поиске коэффициентов этой линейной модели. Говоря мате-
матическим языком, мы будем решать задачу параметрического оценивания.
Начнем?
1.1 Линейная регрессия и МО
Часто требуется определить, как зависит одна случайная величина от
одной или нескольких других величин. Самый общий вид зависимости – ста-
тистическая зависимость. Например, пусть𝑋 = 𝜉 +𝜂 – это сумма случайных
величин 𝜉 и 𝜂, а 𝑌 = 𝜉 + 𝜙 – сумма случайных величин𝜉 и 𝜙. Ясно, что
величины 𝑋 и 𝑌 зависимы, но нет явной функциональной зависимости, то
есть мы не можем указать зависимость вида𝑋 = 𝑓(𝑌 ) или 𝑌 = 𝑓(𝑋).
Можно дать и более неформальный и жизненный пример. Ясно, что
стоимость квартиры зависит от площади, этажа, месторасположения и дру-
гих параметров, но не является функцией от них. Все потому, что есть куча
факторов, которые просто невозможно учесть. Например, при одинаковых
входных параметрах (хотя и это очень относительно) продавец, скорее всего,
выставит квартиру дешевле, если ему срочно нужны деньги, и не будет сни-
жать цену ни на рубль, если продажа «не горит», а может и вообще поднять
ее из-за того, что каждую весну на балконе ласточки вьют гнездо. Ну и как
тут понять ценообразование?
Что же в этом случае делать? Как получить хоть какую-то функцию,
которая может предсказать изменение интересующей нас величины по изме-
нениюпараметров?Длязависимыхслучайныхвеличинимеетсмыслрассмот-
реть математическое ожидание одной из них при фиксированном значении
другой и выяснить, как влияет на среднее значение первой величины изме-
нение значений второй. Так, в примере с квартирой, среднее значение цены
можно считать функцией от параметров, влияющих на цену.
В этой части мы познакомимся с понятием линейной регрессии, доста-
точно простым и часто используемым «инструментом» при обучении с учите-
лем. Линейная регрессия известна уже довольно давно и подробно освещена в
большомколичествекниг.Напервыйвзглядможетпоказаться,чтоонаслиш-
ком тривиальна по сравнению с более продвинутыми средствами статистики,
о которых будет рассказано позже, но на самом деле линейная регрессия до
сих пор широко применяется как непосредственно в статистике, так и в ее
2
Высшая школа цифровой культуры Университет ИТМО
приложениях к машинному обучению. Кроме того, линейная регрессия яв-
ляется хорошей отправной точкой для изучения более новых подходов, так
как многие методы статистики, как мы увидим позже, есть не что иное, как
обобщение линейной регрессии.
На какие же вопросы может ответить линейная регрессия? Для иллю-
страции приведем пример. Предположим, что мы – консультанты-аналитики,
работающие в некоторой фирме, перед которыми стоит задача анализа и
улучшенияобъемапродажопределенногопродукта.Пустьвкачествепродук-
тов выступают, например: мобильные телефоны и самолеты. Эти продукты
продаются у ста одного дистрибьютора (объем выборки –101). В качестве
входных данных выступает объем финансирования, вложенного в рекламу
конкретного продукта (в тысячах и сотнях тысячах долларов), а в качестве
выходных – объем проданного товара (в тысячах единиц). Еще раз поясним,
что на рисунках по оси абсцисс отложено количество финансирования, выде-
ленного на рекламу продукции, а на оси ординат – объем продаж продукта (в
тысячах единиц). Рисунок?? отвечает за мобильные телефоны, а рисунок??
– за самолеты. Уже на первом рисунке мы видим, что реклама, в общем и це-
лом, продуктивно влияет на объем продаж телефонов, хотя вид зависимости
не очень понятен.
Рис. 1: Зависимость объема продаж мобильных телефонов от затрат на ре-
кламу
Совершенно наивно полагать, что самая «правильная» зависимость – это
зависимость, представленная на рисунке?? (зависимость получена просто со-
единением соседних точек отрезками). Как интерпретировать такую модель,
3
Высшая школа цифровой культуры Университет ИТМО
Рис. 2: Зависимость продаж самолетов от объема финансирования
какобъяснить?Почемуприувеличенияхзатратнарекламупродажиторезко
падают, то взмывают вверх? Может быть есть какие-то неучитываемые нами
параметры, как, например, период отпусков (когда продажи падают по объ-
ективным причинам), или приближение нового года (когда они же взмывают
вверх, и снова понятно почему), а может данные просто содержат ошибки? Во
всех этих случаях предложенная «модель» только усугубит прогноз и будет
ни чем не лучше, чем просто число, сказанное наугад.
Второй рисунок трактовать сложнее. Мы видим, что небольшое финан-
сирование (до100 тысяч долларов), в общем и целом, дает примерно одина-
ковый объем продаж, хотя имеются и выбросы в сторону увеличения объема.
Дальше же ситуация противоречива. Увеличение затрат на рекламу до400,
а то и до 600 тысяч долларов в среднем увеличивает продажи в полтора-
два раза, опять же, за исключением некоторых выбросов. Кстати, на втором
рисунке видны и очевидно аномальные данные с отрицательным объемом
продаж.
Директор фирмы не может непосредственно повлиять на объем продаж,
однако он может влиять на объем бюджета, выделяемого на рекламу, косвен-
но влияя на продажи. Какие же вопросы нас могут заинтересовать?
1. Есть ли реальная зависимость между вложенным в рекламу бюджетом
и объемом продаж? Ясно, что если зависимости не наблюдается, то
зачем тратить деньги на рекламу?
2. Если зависимость все-таки есть, то насколько она сильна? Другими сло-
4
Высшая школа цифровой культуры Университет ИТМО
Рис. 3: Наивная зависимость
вами, зная объем бюджета, потраченного на рекламу, можем ли мы с
достаточной точностью предсказать объем продаж? Если да, то зави-
симость сильная, иначе – слабая.
3. Какие товары популярны и продаются? Выгодно ли тратить бюджет на
рекламу всех товаров?
4. Насколькоточномыможемоценитьизменениеобъемапродаж,изменяя
объем бюджета для рекламы?
5. Насколькоточномыможемпредсказатьобъемпродаж,знаяобъемвли-
ваемого в рекламу бюджета?
6. Линейна ли зависимость?
7. Имеется ли синергия в областях продаж? Ведь может так оказаться,
что вливание50000 долларов на рекламу мобильных телефонов и50000
долларовнарекламусамолетовлучше,тоестьприведеткболеевысоко-
му объему продаж, чем вливание100000 на рекламу только телефонов.
Оказывается, линейная регрессия может ответить на каждый из написанных
выше вопросов. Давайте приступим к детальному изучению.
5
Высшая школа цифровой культуры Университет ИТМО
1.2 Модель простейшей линейной регрессии и
метод наименьших квадратов
Предположим, что наблюдаемая (случайная) величина 𝑌 зависит от
некоторого известного и неслучайного фактора 𝑋1, а также от случайной
ошибки 𝜀, наличие которой объясняется либо погрешностью измерений, ли-
бо ошибками самой модели, либо эта ошибка просто-напросто заложена в
основе эксперимента. В качестве основной модели в этом пункте мы будем
рассматривать следующую линейную модель
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜀.
Итак, зависимость между𝑌 и 𝑋1 предполагается линейной с точностью до
некоторой ошибки.
Определение 1.2.1Модель, описываемая зависимостью
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜀,
где 𝜃0, 𝜃1 – числовые параметры,𝑋1 – неслучайный параметр, значения ко-
торого либо задаются, либо наблюдаются (иначе говоря – известны),𝜀 –
случайная ошибка, называется моделью простейшей линейной регрессии.
Часто используют и следующие два определения.
Определение 1.2.2Функция
𝑓(𝑋1) = 𝜃0 + 𝜃1𝑋1
в модели простейшей линейной регрессии называется линией регрессии𝑌
на 𝑋1.
Определение 1.2.3Уравнение
𝑌 = 𝜃0 + 𝜃1𝑋1
в модели простейшей линейной регрессии называется уравнением регрессии
𝑌 на 𝑋1.
Абстрактная модель – это хорошо. Но как ее строить и применять на практи-
ке, на конкретных наблюдаемых значениях𝑋1 и 𝑌 ? Давайте опишем схему
подробнее. Начнем же с того, что аккуратно выпишем: а что дано?
Итак,пустьпроводится 𝑛 экспериментов,вкаждомизкоторых(обратите
на это внимание!)неслучайная величина 𝑋1 приняла значения𝑥1, 𝑥2, ..., 𝑥𝑛.
6
Высшая школа цифровой культуры Университет ИТМО
Допустим также, что среди этих значений есть хотя бы два различных. В за-
висимости от значений величины𝑋1, мы наблюдаем𝑛 значений 𝑌1, 𝑌2, ..., 𝑌𝑛
нашей случайной величины 𝑌 . В итоге, в результате эксперимента у нас
есть 𝑛 пар данных(𝑥1, 𝑌1), (𝑥2, 𝑌2), ...,(𝑥𝑛, 𝑌𝑛). Ясно, что они могут быть лег-
ко изображены на плоскости. На рисунке ??, показывающем зависимость
объема продаж мобильных телефонов в зависимости от затрат на их рекла-
му, изображена101 пара данных.Неслучайная величина 𝑋1 – этоизвест-
ный объем выделенного финансирования рекламы, аслучайная величина
𝑌 – это объем продаж при известном финансировании инеизвестных, слу-
чайных других факторах (или ошибках).
Рис. 4: Зависимость объема продаж мобильных телефонов от затрат на ре-
кламу
В итоге, так как в результате измерений в эксперименте возникали слу-
чайныеошибки,овозможнойприродекоторыхмыговорилиранее,тоточного
равенства
𝑌𝑖 = 𝜃0 + 𝜃1𝑥𝑖
для каждого измерения𝑖 ∈{1, 2, ..., 𝑛}при одних и тех же (пока что неиз-
вестных!) параметрах𝜃0 и 𝜃1 получить, скорее всего, не получится, но можно
утверждать, что при каждом𝑖 справедливо соотношение
𝑌𝑖 = 𝜃0 + 𝜃1𝑥𝑖 + 𝜀𝑖
при одних и тех же параметрах𝜃0 и 𝜃1.
7
Высшая школа цифровой культуры Университет ИТМО
Замечание 1.2.1Отметим в качестве замечания, что конкретные зна-
чения 𝑌1, 𝑌2, ..., 𝑌𝑛 случайной величины 𝑌 вполне разумно обозначать ма-
ленькими буквами𝑦1, 𝑦2, ..., 𝑦𝑛. Однако во избежании путаницы с тем, что
𝑌 случайна, а𝑋1 – нет, договоримся в этой лекции и конкретные значения
случайной величины𝑌 обозначать заглавными буквами.
Итак, сама по себе модель озвучена. Но как же найти оценки параметров
𝜃0 и 𝜃1, зная набор данных(𝑥1, 𝑌1), (𝑥2, 𝑌2), ...,(𝑥𝑛, 𝑌𝑛)? Ведь не зная парамет-
ров, мы не можем решать задачу предсказания (а именно для ее решения все
и затевается).
Для поиска коэффициентов регрессии, мы будем пользоваться методом
наименьших квадратов (МНК). Этот метод позволяет найти такие оценки̂︀𝜃0
и ̂︀𝜃1 параметров 𝜃0 и 𝜃1, что сумма квадратов ошибок𝜀(𝜃0, 𝜃1) в наблюдаемых
𝑛 экспериментах минимальна. Иными словами, минимизируется функция
𝜀(𝜃0, 𝜃1) = 𝜀2
1 + 𝜀2
2 + ... + 𝜀2
𝑛 =
𝑛∑︁
𝑖=1
𝜀2
𝑖 =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2
и ищутся аргументы, ее минимизирующие. В итоге решается следующая за-
дача
arg min
𝜃0,𝜃1
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2 .
Определение 1.2.4Оценкой метода наименьших квадратов для неиз-
вестных параметров𝜃0 и 𝜃1 уравнения регрессии называется набор значений
параметров, минимизирующий выражение
𝜀(𝜃0, 𝜃1) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2 .
Ну что, идейная сторона вопроса на этом закончена. С технической же точки
зрения перед нами функция
𝜀(𝜃0, 𝜃1) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2 ,
зависящая от двух переменных𝜃0 и 𝜃1, которую нам требуется минимизиро-
вать. Решение задачи минимизации дается следующей теоремой.
Теорема 1.2.1Минимум функции
𝜀(𝜃0, 𝜃1) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2
8
Высшая школа цифровой культуры Университет ИТМО
единственен и достигается при
𝜃1 =
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)(𝑌𝑖 −𝑌 )
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
, 𝜃 0 = 𝑌 −𝜃1𝑋1,
где 𝑋1 – среднее принимаемых переменной𝑋1 значений, то есть
𝑋1 = 1
𝑛
𝑛∑︁
𝑖=1
𝑥𝑖,
а 𝑌 – среднее принимаемых переменной𝑌 значений, то есть
𝑌 = 1
𝑛
𝑛∑︁
𝑖=1
𝑌𝑖.
Доказательство. Рассматриваемая функция дифференцируемая, а значит
необходимым условием экстремума является равенство нулю частных произ-
водных этой функции:
{︃𝜕𝜀(𝜃0,𝜃1)
𝜕𝜃0
= 0
𝜕𝜀(𝜃0,𝜃1)
𝜕𝜃1
= 0 ⇔
⎧
⎪⎪⎨
⎪⎪⎩
−2
𝑛∑︀
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖) = 0
−2
𝑛∑︀
𝑖=1
𝑥𝑖 (𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖) = 0
.
Решая эту систему (а это – линейная система из двух уравнений с двумя
неизвестными 𝜃0 и 𝜃1), находим, что
𝜃1 =
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)(𝑌𝑖 −𝑌 )
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
, 𝜃 0 = 𝑌 −𝜃1𝑋1.
Конечно, назвать найденные значения𝜃0 и 𝜃1 оценками можно лишь после
того, как мы и правда убедимся, что полученная точка – точка минимума.
Это можно сделать, используя какое-нибудь достаточное условие экстремума
функции двух переменных, а можно и ограничиться следующим соображе-
нием: функция 𝜀(𝜃0, 𝜃1) выпукла вниз, а значит найденная точка, подозри-
тельная на экстремум, и правда является точкой минимума. □
Теперь можно написать, что
̂︀𝜃1 =
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)(𝑌𝑖 −𝑌 )
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
, ̂︀𝜃0 = 𝑌 −̂︀𝜃1𝑋1,
9
Высшая школа цифровой культуры Университет ИТМО
где
𝑋1 = 1
𝑛
𝑛∑︁
𝑖=1
𝑥𝑖, 𝑌 = 1
𝑛
𝑛∑︁
𝑖=1
𝑌𝑖.
Итак, на основе данных по мобильным телефонам, получаем значения̂︀𝜃0 ≈
4.88, ̂︀𝜃1 ≈0.30. В итоге, функция
𝑓(𝑋1) = 4.88 + 0.30𝑋1
иестьискомаялиниялинейнойрегрессии.Построимееграфик,онизображен
на рисунке 1 синим цветом.
Рис. 5: Зависимость объема продаж мобильных телефонов от затрат на ре-
кламу и регрессия
Как видно из графика, полученная нами модель действительно «непло-
хо» приближает изображенные данные (ну, строго говоря, что считать кри-
терием плохо или неплохо мы обсудим чуть позже, опять же вернувшись к
этим примерам). Кроме того, если провести вертикальные зеленые (парал-
лельные оси 𝑂𝑦) линии от красных точек до синей прямой, то мы получим
величины ошибок 𝜀𝑖 (сумму квадратов которых, мы минимизировали), ко-
торые показывают отклонение нашей модели от реальных данных, рисунок
2.
На основе данных по продажам самолетов мы получаем значения̂︀𝜃0 ≈
5.13 и ̂︀𝜃1 ≈1.34. Значит, функция
𝑓(𝑋1) = 5.13 + 1.34𝑋1
10
Высшая школа цифровой культуры Университет ИТМО
Рис. 6: Зависимость объема продаж мобильных телефонов от затрат на ре-
кламу, регрессия и ошибки
иестьискомаялиниялинейнойрегрессии.Построимееграфик,онизображен
на рисунке 3 синим цветом. Детальное обсуждение точности данной модели,
как и модели, полученной ранее, проведем чуть позже.
В этого пункта, еще раз подчеркнем, что, найдя оценки̂︀𝜃0 и ̂︀𝜃1, предска-
зание ищется, используя уравнение регрессии
𝑌 = ̂︀𝜃0 + ̂︀𝜃1𝑋1.
Например модель, построенная для мобильных телефонов, с уравнением
𝑌 = 4.88 + 0.30𝑋1
при затратах на рекламу в150 тысяч долларов дает предсказание по объему
продаж в
𝑌 = 4.88 + 0.30 ·150 = 49.88
тысяч единиц.
1.3 Пример: затраты времени на покупки
Чтобы формулы не казались пугающими, а все увиденное не было че-
ресчур абстрактным, покажем расчеты на конкретном не объемном примере.
Пусть, например, имеются данные о том, сколько минут человек находится в
продуктовом супермаркете в зависимости от количества приобретаемых им
товаров. Данные представим в виде таблицы.
11
Высшая школа цифровой культуры Университет ИТМО
Рис. 7: Зависимость объема продаж самолетов от затрат на рекламу и ре-
грессия
№ наблюдения Количество выбранных товаров Время в магазине (мин.)
1 10 15
2 5 12
3 12 18
4 25 30
5 1 3
6 18 20
7 11 14
8 7 10
9 19 20
10 15 13
Предположим, что вам нужно сделать некоторое количество покупок, но вы
ограничены во времени. Можно ли спрогнозировать, сколько вам понадобит-
ся времени для совершения того или иного количества покупок, опираясь
на данные предыдущих походов в магазин? Для перехода к моделированию
определим, что является известной переменной, а что откликом. В качестве
известной переменной𝑋1 выберем количество товаров, которое купил чело-
век, а в качестве отклика𝑌 – время, проведенное в магазине. Значит, полу-
чаем следующий набор(𝑥1, 𝑌1), (𝑥2, 𝑌2), . . . ,(𝑥10, 𝑌10) пар исходных данных
(для удобства приведенных в таблице):
12
Высшая школа цифровой культуры Университет ИТМО
№ наблюдения Количество выбранных товаров Время в магазине (мин.)
1 𝑥1 = 10 𝑌1 = 15
2 𝑥2 = 5 𝑌2 = 12
3 𝑥3 = 12 𝑌3 = 18
4 𝑥4 = 25 𝑌4 = 30
5 𝑥5 = 1 𝑌5 = 3
6 𝑥6 = 18 𝑌6 = 20
7 𝑥7 = 11 𝑌7 = 14
8 𝑥8 = 7 𝑌8 = 10
9 𝑥9 = 19 𝑌9 = 20
10 𝑥10 = 15 𝑌10 = 13
Для наглядной иллюстрации изобразим эти данные на рисунке 4. По
горизонтальной оси отложены иксы, а по вертикальной – игреки.
Рис. 8: Зависимость времени, проведенного в магазине, от количества вы-
бранных товаров
13
Высшая школа цифровой культуры Университет ИТМО
Так как в нашем случае𝑛 = 10, то формулы для̂︀𝜃0 и ̂︀𝜃1 примут следу-
ющий вид:
̂︀𝜃1 =
10∑︀
𝑖=1
(𝑥𝑖 −𝑋1)(𝑌𝑖 −𝑌 )
10∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
, ̂︀𝜃0 = 𝑌 −̂︀𝜃1𝑋1,
где
𝑋1 = 1
10
10∑︁
𝑖=1
𝑥𝑖, 𝑌 = 1
10
10∑︁
𝑖=1
𝑌𝑖.
Начнем с вычисления последних, итак
𝑋1 = 1
10 (10 + 5 + 12 + 25 + 1 + 18 + 11 + 7 + 19 + 15) =123
10 = 12.3,
𝑌 = 1
10 (15 + 12 + 18 + 30 + 3 + 20 + 14 + 10 + 20 + 13) =155
10 = 15.5.
Теперь мы можем вычислить̂︀𝜃1:
̂︀𝜃1 = (𝑥1 −𝑋1)(𝑌1 −𝑌 ) + (𝑥2 −𝑋1)(𝑌2 −𝑌 ) + ... + (𝑥10 −𝑋1)(𝑌10 −𝑌 )
(𝑥1 −𝑋1)2 + (𝑥2 −𝑋1)2 + ... + (𝑥10 −𝑋1)2 =
= (10 −12.3)(15 −15.5) + (5−12.3)(12 −15.5) + ... + (15 −12.3)(13 −15.5)
(10 −12.3)2 + (5 −12.3)2 + ... + (15 −12.3)2 ≈0.93.
Ну а тогда
̂︀𝜃0 ≈15.5 −0.93 ·12.3 ≈4.06.
В реальных подсчетах значения лучше не округлять, и подставлять для рас-
чета ̂︀𝜃0 значение ̂︀𝜃1 с как можно большим числом знаков после запятой. Мы
округлили ̂︀𝜃1 и нашли приближенное значение̂︀𝜃0 для наглядности.
Итак, уравнение линейной регрессии имеет следующий вид:
𝑌 = 4.06 + 0.93𝑋1.
Построим получившуюся прямую, результат можно видеть на рисунке??.
Вернемся к задаче предсказания. Ответим на вопрос: сколько времени зай-
мет поход в магазин, если мы хотим приобрести, например,27 товаров? Для
прогноза достаточно вычислить значение функции𝑌 = 4 .06 + 0.93𝑋1 при
𝑋1 = 27, то есть
4.06 + 0.93 ·27 = 29.17,
а значит потребуется чуть больше, чем29 минут. Иллюстрация прогноза при-
ведена на рисунке??.
14
Высшая школа цифровой культуры Университет ИТМО
Рис. 9: Зависимость времени, проведенного в магазине, от количества вы-
бранных товаров и регрессия
2 Некоторые статистические характеристики
параметров простейшей линейной регрессии
В предыдущем пункте мы решили задачу построения регрессии больше
на эвристическом уровне, ведь мы не объяснили, во-первых, почему описан-
ная схема и правда дает хорошую модель, и, в частности, почему приме-
няется метод наименьших квадратов. Во-вторых, за кадром остались рамки
применимости модели, в частности вопрос: а что это за случайные ошибки
𝜀, неужели они могут быть любыми? В этом пункте мы подробно обсудим
математические детали построенной модели.
2.1 Параметры 𝜃0 и 𝜃1 как случайные величины
Напомним предпосылки задачи. Проведя 𝑛 экспериментов, в которых
величина 𝑋1 приняла значения𝑥1, 𝑥2, ..., 𝑥𝑛 (среди которых хотя бы два раз-
личны),мынаблюдаем 𝑛 значений𝑌1, 𝑌2, ..., 𝑌𝑛 нашейслучайнойвеличины 𝑌 .
Так как в результате измерений в эксперименте возникали случайные ошиб-
ки, то мы предполагаем, что при каждом𝑖 справедливо соотношение
𝑌𝑖 = 𝜃0 + 𝜃1𝑥𝑖 + 𝜀𝑖
при одних и тех же параметрах𝜃0 и 𝜃1.
В этом разделе мы будем исходить из следующих важных предположе-
15
Высшая школа цифровой культуры Университет ИТМО
Рис. 10: Зависимость времени, проведенного в магазине, от количества вы-
бранных товаров, регрессия и предсказание
ний (первые три из которых часто называют условиями Гаусса-Маркова):
1. Случайные величины (ошибки) 𝜀1, 𝜀2, ..., 𝜀𝑛 независимы и одинаково
распределены;
2. Ошибки не носят систематического характера, то есть E𝜀𝑖 = 0 , 𝑖 ∈
{1, 2, ..., 𝑛};
3. Дисперсии рошибок одинаковы, то естьD𝜀𝑖 = 𝜎2 > 0, 𝑖 ∈{1, 2, ..., 𝑛}
(гомоскедастичность);
4. 𝜀𝑖 ∼N0,𝜎2 .
Отметим, что даже при таких «жестких» предположениях о распределении
случайных ошибок, набор значений 𝑌1, 𝑌2, ..., 𝑌𝑛 случайной величины 𝑌 не
является выборкой в принятом ранее в статистике смысле. Ведь случайные
величины 𝑌𝑖 не являются одинаково распределенными, так как, например, их
математические ожидания различны, ведь
E𝑌𝑖 = E (𝜃0 + 𝜃1𝑥𝑖 + 𝜀𝑖) = 𝜃0 + 𝜃1𝑥𝑖,
где последние значения, вообще говоря, не одинаковы.
16
Высшая школа цифровой культуры Университет ИТМО
Теперь мы готовы изучить основные свойства оценок, полученных мето-
дом наименьших квадратов. Напомним аналитические выражения для них:
̂︀𝜃1 =
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)(𝑌𝑖 −𝑌 )
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
, ̂︀𝜃0 = 𝑌 −̂︀𝜃1𝑋1,
𝑋1 = 1
𝑛
𝑛∑︁
𝑖=1
𝑥𝑖, 𝑌 = 1
𝑛
𝑛∑︁
𝑖=1
𝑌𝑖.
Для начала оказывается, что оценки, полученные методом наименьших квад-
ратов, в случае выполнения написанных выше четырех условий совпадают с
оценками метода максимального правдоподобия.
Теорема 2.1.1 (О совпадении МНК и ММП оценок)В предположе-
ниях 1-4, сформулированных выше, оценки̂︀𝜃0 и ̂︀𝜃1 являются оценками мак-
симального правдоподобия параметров𝜃0 и 𝜃1.
Доказательство. Применим метод максимального правдоподобия. Несмот-
ря на то, что𝑌1, 𝑌2, ..., 𝑌𝑛 – не выборка в привычном смысле, в силу незави-
симости 𝑌𝑖 сам метод, как и его реализация, остаются прежними. Так как
𝑌𝑖 ∼N𝜃0+𝜃1𝑥𝑖,𝜎2 ,
то функция правдоподобия имеет следующий вид
𝑓𝜃(⃗𝑌 ) = 𝑓𝜃(𝑌1, 𝑌2, ..., 𝑌𝑛) = 1
(
√
2𝜋𝜎)𝑛 𝑒−
𝑛∑︀
𝑖=1
(𝑌𝑖−𝜃0−𝜃1𝑥𝑖)2
2𝜎2 = (2𝜋𝜎2)−𝑛/2𝑒−𝜀(𝜃0,𝜃1)
2𝜎2 ,
где
𝜀(𝜃0, 𝜃1) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2.
Ясно, что максимум функции правдоподобия достигается в случае, когда до-
стигается минимум функции𝜀(𝜃0, 𝜃1), то есть мы приходим к задаче
arg min
𝜃0,𝜃1
𝜀(𝜃0, 𝜃1) = arg min
𝜃0,𝜃1
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2,
что вторит решаемой задаче в методе наименьших квадратов. □
Замечание 2.1.1Заметим, что параметр𝜎2 обычно является неизвест-
ным. Так как выборка𝑌1, 𝑌2, ..., 𝑌𝑛 – не выборка в классическом смысле, то в
качестве его оценки использовать оценку𝑆2(𝑌 ) или 𝑆2
0 (𝑌 ), вообще говоря,
нельзя.
17
Высшая школа цифровой культуры Университет ИТМО
Следствие 2.1.2Оценкой метода максимального правдоподобия неизвест-
ного параметра𝜎2 является следующая случайная величина:
̂︀𝜎2 = 1
𝑛
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2 = 1
𝑛
𝑛∑︁
𝑖=1
𝜀2
𝑖 .
Доказательство. Как уже было получено ранее,
𝑓𝜃,𝜎2 (⃗𝑌 ) = 𝑓𝜃,𝜎2 (𝑌1, 𝑌2, ..., 𝑌𝑛) = 1
(
√
2𝜋𝜎)𝑛 𝑒−
𝑛∑︀
𝑖=1
(𝑌𝑖−𝜃0−𝜃1𝑥𝑖)2
2𝜎2 .
Тогда логарифмическая функция правдоподобия перепишется, как
𝐿𝜃,𝜎2 (⃗𝑌 ) = −𝑛
2
(︀
ln 2𝜋 + ln𝜎2)︀
−
𝑛∑︀
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2
2𝜎2 .
𝜕𝐿𝜃,𝜎2 (⃗𝑌 )
𝜕𝜎2 = −𝑛
2 · 1
𝜎2 +
𝑛∑︀
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2
2𝜎4 .
Приравняв последнее выражение к нулю и решив полученное уравнение, по-
лучим, что
𝜎2 =
𝑛∑︀
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2
𝑛 .
С помощью достаточного условия экстремума можно установить, что полу-
ченная точка является точкой максимума, а значит
̂︀𝜎2 =
𝑛∑︀
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖)2
𝑛 .
□
Замечание 2.1.2Ясно, что в случае, когда𝜃0 и 𝜃1 неизвестны, оценка пе-
реписывается, как
̂︀𝜎2 = 1
𝑛
𝑛∑︁
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
Теперь рассмотрим свойства оценок̂︀𝜃0 и ̂︀𝜃1. Полученные ниже факты
помогут нам в следующем пункте при построении доверительных интервалов
и при проверке гипотез относительно параметров модели. Начнем с̂︀𝜃0.
18
Высшая школа цифровой культуры Университет ИТМО
Лемма 2.1.1 (О свойствах оценки̂︀𝜃1) В предположениях условий
Гаусса-Маркова, оценка̂︀𝜃1 обладает следующими свойствами:
1. Она является несмещенной оценкой параметра𝜃1;
2. Она является эффективной в классе линейных несмещенных оценок;
3. Ее дисперсия равна
D̂︀𝜃1 = 𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
.
4. В предположении, что справедливо и четвертое условие, то есть что
𝜀𝑖 ∼N0,𝜎2 , она имеет нормальное распределение с параметрами𝜃1 и
D̂︀𝜃1, то есть
̂︀𝜃1 ∼N𝜃1,D̂︀𝜃1
.
Отдельно поясним второе свойство. Оно означает, что ошибкаMSE в классе
несмещенных линейных оценок минимальна именно на оценках, полученных
с помощью метода наименьших квадратов. Так как
MSE = E||𝜃 −̂︀𝜃||2 = D̂︀𝜃 + (Ê︀𝜃 −𝜃)2,
а последнее слагаемое для̂︀𝜃1, в силу несмещенности, равно нулю, то мини-
мальность MSE – суть минимальность дисперсии D̂︀𝜃 оценки ̂︀𝜃. Минималь-
ность дисперсии обеспечивает и минимальность «разброса» оценок от истин-
ного значения параметра.
Доказательство. 1.Пользуясьсвойствомлинейностиматематическогоожи-
дания, получим
Ê︀𝜃1 = 1
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)(E𝑌𝑖 −E𝑌 ).
Так какE𝜀𝑖 = 0, то
E𝑌𝑖 = E (𝜃0 + 𝜃1𝑥𝑖 + 𝜀𝑖) = 𝜃0 + 𝜃1𝑥𝑖,
а значит
E𝑌 = 1
𝑛
𝑛∑︁
𝑖=1
E𝑌𝑖 = 1
𝑛
𝑛∑︁
𝑖=1
(𝜃0 + 𝜃1𝑥𝑖) = 𝜃0 + 𝜃1𝑋1.
Тогда
E𝑌𝑖 −E𝑌 = 𝜃1(𝑥𝑖 −𝑋1)
19
Высшая школа цифровой культуры Университет ИТМО
и, подставляя это в выражение дляÊ︀𝜃1, получим
Ê︀𝜃1 = 1
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
𝑛∑︁
𝑖=1
𝜃1(𝑥𝑖 −𝑋1)2 = 𝜃1.
2. Этот пункт в общем случае мы доказывать не будем. Его частный случай
следует из свойств оценок метода максимального правдоподобия и предыду-
щей теоремы.
3. Так как выражение для̂︀𝜃1 может быть переписано, как
̂︀𝜃1 = 1
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
(︃ 𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)𝑌𝑖 −
𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)𝑌
)︃
,
и так как
𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)𝑌 = 𝑌
(︃ 𝑛∑︁
𝑖=1
𝑥𝑖 −𝑛𝑋
)︃
= 0,
то достаточно исследовать
̂︀𝜃1 = 1
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)𝑌𝑖.
Пользуясь независимостью ошибок и свойствами дисперсии, получим
D̂︀𝜃1 = 1(︂ 𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
)︂2
𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)2D𝑌𝑖.
Так как
D𝑌𝑖 = D (𝜃0 + 𝜃1𝑥𝑖 + 𝜀𝑖) = D𝜀𝑖 = 𝜎2,
то
D̂︀𝜃1 = 𝜎2
(︂ 𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
)︂2
𝑛∑︁
𝑖=1
(𝑥𝑖 −𝑋1)2 = 𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
.
4. Это свойство следует из того, что сумма независимых случайных величин,
имеющих нормальное распределение, имеет нормальное распределение, па-
раметры которого вычислены в пунктах 1 и 3.
□
20
Высшая школа цифровой культуры Университет ИТМО
Лемма 2.1.2 (О свойствах оценки̂︀𝜃0) В предположениях условий
Гаусса-Маркова, оценка̂︀𝜃0 обладает следующими свойствами:
1. Она является несмещенной оценкой параметра𝜃0;
2. Она является эффективной в классе линейных несмещенных оценок;
3. Ее дисперсия равна
D̂︀𝜃0 = 𝜎2
𝑛 .
4. В предположении, что справедливо и четвертое условие, то есть что
𝜀𝑖 ∼N0,𝜎2 , она имеет нормальное распределение с параметрами𝜃0 и
D̂︀𝜃0, то есть
̂︀𝜃0 ∼N𝜃0,D̂︀𝜃0
.
5. В предположении, что справедливо и четвертое условие, то есть что
𝜀𝑖 ∼N0,𝜎2 , она является оценкой максимального правдоподобия пара-
метра 𝜃0.
Доказательство. 1. Из свойства линейнойсти математического ожидания,
получим
Ê︀𝜃0 = E
(︀
𝑌 −𝜃1𝑋1
)︀
= E𝑌 −𝜃1𝑋1 = 1
𝑛
𝑛∑︁
𝑖=1
(𝜃0 + 𝜃1𝑥𝑖) −𝜃1𝑋1 =
= 𝜃0 + 𝜃1𝑋1 −𝜃1𝑋1 = 𝜃0.
2. Этот пункт в общем случае мы доказывать не будем. Его частный случай
следует из свойств оценок метода максимального правдоподобия и теоремы
о связи оценок МНК и ММП.
3. Используя свойства дисперсии, получим
D̂︀𝜃0 = D(𝑌 −𝜃1𝑋1) = D𝑌 = 1
𝑛2
𝑛∑︁
𝑖=1
D𝑌𝑖.
Так как
D𝑌𝑖 = D(𝜃0 + 𝜃1𝑥𝑖 + 𝜀𝑖) = D𝜀𝑖 = 𝜎2,
то
D̂︀𝜃0 = 𝑛𝜎2
𝑛2 = 𝜎2
𝑛 .
4. Это свойство следует из того, что сумма независимых нормально распре-
деленных случайных величин имеет нормальное распределение, и из вычис-
лений в пунктах 1 и 3. □
21
Высшая школа цифровой культуры Университет ИТМО
Замечание 2.1.3В последней теореме вывод дисперсии оценки ̂︀𝜃0 суще-
ственным образом опирается на то, что𝜃1 – известное число. Если же𝜃1
оценивается при помощи̂︀𝜃1, то вычисления становятся немного сложнее
и приводят к следующему выражению для дисперсии
D̂︀𝜃0 = D𝑌 + D̂︀𝜃1 ·𝑋1
2
,
откуда
D̂︀𝜃0 = 𝜎2
𝑛 + 𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
𝑋1
2
= 𝜎2
⎛
⎜⎜⎝
1
𝑛 + 𝑋1
2
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
⎞
⎟⎟⎠
2.2 Построение доверительных интервалов для
коэффициентов регрессии
Итак,перед тем как сформулировать основную теорему одоверительных
интервалах, сначала вернемся к оценке неизвестного параметра𝜎2. Напом-
ним, что согласно ММП, она имеет вид
̂︀𝜎2 = 1
𝑛
𝑛∑︁
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2.
Замечание 2.2.1Можно показать, что в предположении условий 1-4, об-
сужденных ранее, 𝑛
𝜎2
̂︀𝜎2 ∼𝜒2
𝑛−2.
По сути дела,2 степени свободы «забираются» из-за того, что мы нее зна-
ем ни параметр𝜃0, ни параметр𝜃1, а лишь оцениваем их. Пользуясь этим,
согласно свойству линейности математического ожидания и согласно то-
му, что математическое ожидание случайной величины, имеющей распре-
деление хи-квадрат с(𝑛 −2) степенями свободы, равно(𝑛 −2), получаем
E
(︁ 𝑛
𝜎2
̂︀𝜎2
)︁
= 𝑛
𝜎2 E ̂︀𝜎2 = (𝑛 −2).
А тогда
E ̂︀𝜎2 = 𝑛 −2
𝑛 𝜎2,
и полученная нами оценка ̂︀𝜎2 является смещенной, но асимптотически
несмещенной, ведь.
lim
𝑛→+∞
𝑛 −2
𝑛 = 1.
22
Высшая школа цифровой культуры Университет ИТМО
Несмещенная же оценка получается так:
̂︀𝜎2
0 = 𝑛
𝑛 −2
̂︀𝜎2 = 𝑛
𝑛 −2 ·
𝑛∑︀
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
𝑛 = 1
𝑛 −2
𝑛∑︁
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2.
Так как зачастую истинное значение параметра𝜎2 неизвестно, то его прихо-
дится оценивать, а значит приходится оценивать и дисперсии оценок̂︀𝜃0 и ̂︀𝜃1,
полученных ранее. Итак, оценим «разброс» среди возможных оценок̂︀𝜃0 и ̂︀𝜃1.
Определение 2.2.1Величины
SE( ̂︀𝜃0) =
⎯⎸⎸⎸⎷
𝑛∑︀
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
𝑛 −2 ·
⎯⎸⎸⎸⎷
1
𝑛 + 𝑋1
2
𝑛∑︀
𝑖=1
(︀
𝑥𝑖 −𝑋1
)︀2
,
SE( ̂︀𝜃1) =
⎯⎸⎸⎸⎷
𝑛∑︀
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
𝑛 −2 ·
⎯⎸⎸⎸⎷
1
𝑛∑︀
𝑖=1
(︀
𝑥𝑖 −𝑋1
)︀2
называются стандартными ошибками (standard error) оценок̂︀𝜃0 и ̂︀𝜃1, со-
ответственно.
Замечание 2.2.2Полезно понимать, откуда получаются и что обознача-
ют только что введенные стандартные ошибки. Так как
D̂︀𝜃0 = 𝜎2
⎛
⎜⎜⎝
1
𝑛 + 𝑋1
2
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
⎞
⎟⎟⎠,
и
D̂︀𝜃1 = 𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖 −𝑋1)2
,
то введенные стандартные ошибки есть не что иное, как оценки средне-
квадратических отклонений̂︀𝜃0 и ̂︀𝜃1 при замене𝜎2 на ее несмещенную оцен-
ку, полученную ранее, то есть на оценку
̂︀𝜎2
0 = 1
𝑛 −2
𝑛∑︁
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2.
23
Высшая школа цифровой культуры Университет ИТМО
На основе стандартных ошибок можно построить так называемый довери-
тельный интервал. Напомним определение.
Определение 2.2.2Пусть 0 < 𝜀 <1. Доверительный интервал(𝜃−, 𝜃+)
уровня доверия 1 −𝜀 – это интервал, в который с вероятностью 1 −𝜀
попадет реальное значение параметра.
На практике часто рассматривают значения𝜀 = 0.1, 𝜀 = 0.05 или 𝜀 = 0.01.
Теорема 2.2.1 (Доверительные интервалы для𝜃0 и 𝜃1) В предполо-
жении условий 1 - 4, доверительный интервал уровня доверия(1 −𝜀) для
параметра𝜃0 – это интервал
(︁
̂︀𝜃0 −𝑡1−𝜀/2 ·SE( ̂︀𝜃0), ̂︀𝜃0 + 𝑡1−𝜀/2 ·SE( ̂︀𝜃0)
)︁
,
где 𝑡1−𝜀/2 – это (1 −𝜀/2) квантиль распределения Стьюдента с (𝑛 −2)
степенями свободы.
Аналогично, доверительный интервал уровня доверия(1 −𝜀) для пара-
метра 𝜃1 – это интервал
(︁
̂︀𝜃1 −𝑡1−𝜀/2 ·SE( ̂︀𝜃1), ̂︀𝜃1 + 𝑡1−𝜀/2 ·SE( ̂︀𝜃1)
)︁
,
где 𝑡1−𝜀/2 – (1−𝜀/2) квантиль распределения Стьюдента с(𝑛−2) степенями
свободы.
Доказательство. Докажем, например, второе соотношение. Первое полу-
чается аналогичным образом. Как мы знаем (из леммы о свойствах оценки
̂︀𝜃1),
̂︀𝜃1 ∼N𝜃1, 𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖−𝑋1)2
.
Тогда, используя свойства линейных преобразований,
𝜃1 −̂︀𝜃1√︃
𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖−𝑋1)2
∼N0,1.
Кроме того, согласно замечанию в начале данного пункта,
𝑛 ̂︀𝜎2
𝜎2 ∼𝜒2
𝑛−2.
24
Высшая школа цифровой культуры Университет ИТМО
Тогда, в силу независимости, случайная величина
𝑡𝑛−2 =
⎛
⎜⎜⎜⎜⎜⎝
𝜃1 −̂︀𝜃1√︃
𝜎2
𝑛∑︀
𝑖=1
(𝑥𝑖−𝑋1)2
⎞
⎟⎟⎟⎟⎟⎠
:
⎛
⎝
√︃
𝑛 ̂︀𝜎2
𝜎2(𝑛 −2)
⎞
⎠ ∼T𝑛−2
имеет распределение Стьюдента с(𝑛 −2)-мя степенями свободы. Последняя
же эквивалентным образом переписывается в виде
𝑡𝑛−2 = 𝜃1 −̂︀𝜃1
SE( ̂︀𝜃1)
.
Дальнейшее построение доверительного интервала стандартно. □
2.3 Немного об интерпретации доверительных
интервалов
Возвратимся теперь к примерам, касающимся продаж телефонов и са-
молетов, с которых мы начали данную лекцию.
Впримересмобильнымителефонами,доверительныйинтервал
(︀
𝜃−
0 , 𝜃+
0
)︀
для 𝜃0 при 𝜀 = 0.1 имеет вид
(︀
𝜃−
0 , 𝜃+
0
)︀
= (3.88, 5.87) ,
а для𝜃1 имеет вид (︀
𝜃−
1 , 𝜃+
1
)︀
= (0.28, 0.31) .
Подробный пример расчета мы увидим чуть позже, а сейчас давайте вду-
маемся в смысл полученных интервалов. Напомним для наглядности, что
уравнение регрессии таково:
𝑌 = 𝜃0 + 𝜃1𝑋1.
Итак, исходя из расчетов, можно сделать вывод, что при отсутствии расходов
на рекламу (то есть при 𝑋1 = 0 ) продажи, в среднем, упадут до 3.88 −
5.87 тысяч единиц (так как прогнозируемое значение𝑌 = 𝜃0). При этом, за
каждую потраченную на рекламу тысячу долларов объем продаж в среднем
увеличится на0.28 −0.31 тысяч единиц.
В примере с самолетами, доверительный интервал уровня доверия0.9
для 𝜃0 имеет вид (︀
𝜃−
0 , 𝜃+
0
)︀
= (4.73, 5.52)
25
Высшая школа цифровой культуры Университет ИТМО
а для𝜃1 имеет вид (︀
𝜃−
1 , 𝜃+
1
)︀
= (1.12, 1.57).
Проанализировав эти результаты, можно сделать вывод, что при отсутствии
рекламы продажи, в среднем, упадут до4.73 −5.52 тысяч единиц. При этом
за каждую потраченную на рекламу сотню тысяч долларов объем продаж в
среднем увеличится на1.12 −1.57 тысячу единиц.
2.4 Доверительные интервалы для примера
Вернемся к нашему примеру со временем, проведенном в магазине, и
вычислим доверительные интервалы для параметров модели. Запишем фор-
мулы в случае десяти исходных данных:
SE( ̂︀𝜃0) =
⎯⎸⎸⎸⎷
10∑︀
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
10 −2 ·
⎯⎸⎸⎸⎸⎷
1
10 + 𝑋1
2
10∑︀
𝑖=1
(︀
𝑥𝑖 −𝑋1
)︀2
,
SE( ̂︀𝜃1) =
⎯⎸⎸⎸⎷
10∑︀
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
10 −2 ·
⎯⎸⎸⎸⎷
1
10∑︀
𝑖=1
(︀
𝑥𝑖 −𝑋1
)︀2
,
𝑋1 = 1
10
10∑︁
𝑖=1
𝑥𝑖.
Опишем, как находить 𝑡1−𝜀/2 из формулы для доверительного интервала.
Итак, у нас𝑛 = 10, то есть десять степеней свободы. Пусть𝜀 = 0.1, тогда
1 −𝜀/2 = 0 .95, 𝑛 −2 = 8 , значит в таблице, которую можно найти в до-
полнительных материалах, находим значение на пересечении восьмой строки
и столбца, соответствующего вероятности 0.95. В нашем случае получаем
𝑡0.95 ≈1.86. Так какSE( ̂︀𝜃0) = 0.91 и SE( ̂︀𝜃1) = 0.13, то
(︀
𝜃−
0 , 𝜃+
0
)︀
= (2.37, 5.75)
и (︀
𝜃−
1 , 𝜃+
1
)︀
= (0.69, 1.17) .
Какой же можно сделать вывод из проделанных расчетов? Рассмотрим сна-
чала второй интервал. В среднем, увеличение количества товаров на один,
в 90% случаев (ведь мы взяли𝜀 = 0 .1, а значит вероятность1 −𝜀 = 0 .9)
26
Высшая школа цифровой культуры Университет ИТМО
увеличивает время пребывания в магазине от0.69 минут до1.17 минут. Пер-
вый же интервал показывает, что, проведя в магазине ноль минут, можно в
среднем купить 3 −4 товара. Такая аномалия обусловлена как ошибкой в
модели (зависимость не абсолютно линейная), так и маленьким количеством
реальных данных. Если предположить, что мы проводим в магазине хотя бы
одну минуту, то аномалия пропадает.
2.5 Проверка гипотез
Стандартные ошибки также используются в так называемой задаче про-
верки гипотез. Одна из самых часто проверяемых гипотез – так называемая
гипотеза статистической значимости параметра̂︀𝜃1, формулируется следую-
щим образом:
H0 : Между 𝑋1 и 𝑌 нет зависимости.
Альтернативная ей гипотеза такова
H𝑎 : Между 𝑋1 и 𝑌 есть зависимость.
С точки зрения математики, нулевая и альтернативная гипотезы говорят не
что иное, как
H0 : 𝜃1 = 0,
H𝑎 : 𝜃1 ̸= 0.
Действительно, в случае 𝜃1 = 0 модель переписывается в виде 𝑌 = 𝜃0 и
значения 𝑋 не учитываются вовсе.
Для проверки гипотезы необходимо определить, насколько значение на-
шей оценки ̂︀𝜃1 истинного параметра𝜃1 далеко от нуля. Ясно, что это зависит
от стандартной ошибки SE( ̂︀𝜃1). Если последняя мала, то даже достаточно
малые значения̂︀𝜃1 могут доказывать, что𝜃1 ̸= 0. Если же ошибка велика, то
и значение|̂︀𝜃1|должно быть велико, чтобы отвергнуть нулевую гипотезу. На
практике обычно используют𝑡-критерий Стьюдента. Для этого вычисляют
статистику
𝑡 = |̂︀𝜃1 −0|
SE( ̂︀𝜃1)
= |̂︀𝜃1|
SE( ̂︀𝜃1)
.
Сравнивая фактическое и табличное значение𝑡1−𝜀/2 на уровне доверия1 −𝜀
с числом степеней свободы(𝑛 −2) принимается решение:
1. Если 𝑡1−𝜀/2 < 𝑡 , то гипотеза H0 отклоняется и оценка ̂︀𝜃1 признается
статистически значимой на уровне значимости𝜀.
2. Если 𝑡1−𝜀/2 ≥𝑡, то гипотеза H0 принимается и оценка ̂︀𝜃1 признается
статистически незначимой на уровне значимости𝜀.
27
Высшая школа цифровой культуры Университет ИТМО
Конечно, нас интересует, чтобы выполнялся первый пункт, иначе наша мо-
дель, с точки зрения статистики, не отражает реальной зависимости между
переменными.
В случае с мобильными телефонами при𝜀 = 0.1 мы получаем значение
𝑡 = 28 .49, что больше значения𝑡1−𝜀/2 ≈1.66 из таблицы, значит гипотеза
H0 отклоняется и принимается альтернативная гипотезаH𝑎. В случае с са-
молетами мы получаем значение𝑡 = 9.81, что снова больше значения𝑡1−𝜀/2
из таблицы, значит гипотезаH0 отклоняется и принимается альтернативная
гипотеза H𝑎.
Замечание 2.5.1Конечно, можно проверять и гипотезу о равенстве𝜃1
какому-то конкретному значению. Для этого имеет смысл использовать
статистику
𝑡 = |̂︀𝜃1 −𝜃1|
SE( ̂︀𝜃1)
.
Дальнейшие действия абсолютно такие же, как в приведенном алгоритме.
Замечание 2.5.2Аналогичным образом можно проверять гипотезы и от-
носительно значений параметра𝜃0. Подробнее об этом мы поговорим в раз-
деле, посвященном множественной регрессии.
2.6 Проверка гипотез для примера
Все необходимые значения для подсчета уже вычислены. Пусть, опять
же, 𝜀 = 0.1. В нашем случае
𝑡 = |̂︀𝜃1|
SE( ̂︀𝜃1)
= 0.93
0.13 ≈7.15.
что больше, чем1.86, значит гипотезаH0 отклоняется и принимается альтер-
нативнаягипотеза H𝑎.Темсамымустановленненулевойоткликнапредиктор,
зависимость имеется.
2.7 Оценка точности модели
Если нулевая гипотеза отвергнута в пользу альтернативной гипотезы,
довольно естественно задаться целью определить степень того, насколько мо-
дель подходит под данные. Обычно такую «оценку» линейной регрессии да-
ют две величины: среднеквадратическое отклонение остатков (RSE – residual
standard error) иR2 статистика.
В модели мы четко видим, что каждый опыт наделен некоторой ошиб-
кой 𝜀. Из-за этой ошибки, даже зная реальные значения коэффициентов𝜃0
и 𝜃1, мы не сможем точно предсказать значение𝑌 , зная значение𝑋1. RSE –
28
Высшая школа цифровой культуры Университет ИТМО
оценка среднего квадратичного отклонения𝜎2 ошибки 𝜀. Грубо говоря, она
показывает насколько построенная модель отличается от «настоящей» моде-
ли и оценивает «усредненный» корень из суммы квадратов ошибок модели.
RSE, как мы уже знаем, может быть вычислена по формуле
̂︀𝜎0 = RSE =
⎯⎸⎸⎷ 1
𝑛 −2
𝑛∑︁
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2.
Так как RSE измеряется в тех же единицах, что и 𝑌 , не всегда понятно,
хороший ли получается показатель.R2 статистика, в отличие отRSE, вели-
чина безразмерная и лежит между нулем и единицей. Для вычисленияR2,
используют формулу
R2 = 1 −
𝑛∑︀
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖)2
𝑛∑︀
𝑖=1
(︀
𝑌𝑖 −𝑌
)︀2
.
Ясно, что если построенная идеально соответствует исходным данным, то все
слагаемые в последней дроби числителя равны нулю и, тогда и только тогда,
R2 = 1 – модель идеальна. Наоборот, если̂︀𝜃1 = 0, то есть модель не зависит
от 𝑋1, тоR2 = 0 (так как в случае̂︀𝜃1 = 0 имеем ̂︀𝜃0 = 𝑌 ), и модель совершенно
несостоятельна, так как не отражает никакой зависимости между откликом
и предиктором.
Несмотря на то, что значенияR2 статистики лежат между нулем и еди-
ницей, мы все равно не можем сказать, какое значениеR2 является хорошим.
Например, в некоторых задачах физики мы точно знаем, что зависимость
линейна с незначительной ошибкой, и будем ожидать коэффициент очень
близким к единице, а маленькое значение коэффициента будет свидетель-
ствовать о серьезной проблеме в эксперименте, из которого брались данные.
Во многих же других областях, как биология, маркетинг и проч., линейная
модель является довольно грубой аппроксимацией данных, и ошибки часто
велики.
Напомним, что выборочная корреляция𝑋1 и 𝑌 определяется, как
𝑟 (𝑋1, 𝑌) =
𝑛∑︀
𝑖=1
(︀
𝑥𝑖 −𝑋1
)︀(︀
𝑌𝑖 −𝑌
)︀
√︃
𝑛∑︀
𝑖=1
(︀
𝑥𝑖 −𝑋1
)︀2
√︃
𝑛∑︀
𝑖=1
(︀
𝑌𝑖 −𝑌
)︀2
Можно показать, что, как оказывается,R2 = 𝑟2(𝑌, ̂︀𝜃0 + ̂︀𝜃1𝑋1).
29
Высшая школа цифровой культуры Университет ИТМО
В случае примера с мобильными телефонами мы получаемRSE = 3.04 и
R2 = 0.89, из чего, согласно вышесказанному, мы можем сделать вывод, что
с точки зрения статистики наша модель работает неплохо и действительно
может описывать рассматриваемую зависимость.
В случае примера с самолетами мы получаемRSE = 1.73 и R2 = 0.49.
Характеристики данной модели намного хуже, чем предыдущей.
2.8 Оценка точности модели для примера
Для нашего примера формулы переписываются в следующем виде:
RSE =
⎯⎸⎸⎷ 1
10 −2
10∑︁
𝑖=1
(𝑌𝑖 −4.06 −0.93 ·𝑥𝑖)2,
R2 = 1 −
10∑︀
𝑖=1
(𝑌𝑖 −4.06 −0.93 ·𝑥𝑖)2
10∑︀
𝑖=1
(︀
𝑌𝑖 −𝑌
)︀2
.
Проведя вычисления, получаем
RSE = 2.77, R2 = 0.87,
что свидетельствует о том, что модель, с точки зрения статистики, хорошая.
30
Высшая школа цифровой культуры Университет ИТМО
3 Множественная линейная регрессия
Простейшая линейная регрессия позволяет решить задачу предсказания
значения одной переменной, зная другую. Но на практике интересующее нас
значение часто зависит более, чем от одной переменной. Скажем объемы про-
даж компании зависят как от того, сколько потрачено на рекламу телефонов,
так и от того, сколько потрачено на рекламу самолетов. Как нам расширить
наш анализ на большее количество переменных?
3.1 Основные определения и матричные обозна-
чения
Достаточно подробно изучив одномерную регрессию, по аналогии мы мо-
жем записать модель множественной линейной регрессии в следующем виде
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝 + 𝜀,
где 𝑋1, 𝑋2, ..., 𝑋𝑝 – (неслучайные) входные данные, по которым мы пытаемся
определить (случайную) переменную𝑌 (отклик), а𝜀 – некоторая случайная
ошибка. Итак, зависимость между𝑌 и 𝑋1, 𝑋2, ...,𝑋𝑝 предполагается линей-
ной с точностью до некоторой ошибки𝜀.
Определение 3.1.1Модель, описываемая зависимостью
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝 + 𝜀,
где 𝜃0, 𝜃1, ..., 𝜃𝑝 – числовые параметры, 𝑋1, 𝑋2, ..., 𝑋𝑝 – неслучайные пара-
метры, значения которых либо заданы, либо наблюдаются (иначе говоря
– известны), 𝜀 – случайная ошибка, называется моделью множественной
линейной регрессии.
Часто используют и следующие два определения.
Определение 3.1.2Функция
𝑓(𝑋1, 𝑋2, ..., 𝑋𝑝) = 𝜃0 + 𝜃1𝑋1 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝
в модели множественной линейной регрессии называется линией регрессии
𝑌 на 𝑋1, 𝑋2, ...,𝑋𝑝.
Определение 3.1.3Уравнение
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝
в модели множественной линейной регрессии называется уравнением ре-
грессии 𝑌 на 𝑋1, 𝑋2, ...,𝑋𝑝.
31
Высшая школа цифровой культуры Университет ИТМО
Как же определить коэффициенты модели на конкретных наблюдаемых зна-
чениях 𝑋1, 𝑋2, ..., 𝑋𝑝 и 𝑌 ? Давайте опишем схему подробнее. Начнем же
снова с того, что аккуратно выпишем: а что дано?
Ясно, что на каждом𝑖-ом наблюдении мы получаем значение𝑌𝑖 по значе-
ниям (𝑥𝑖1, 𝑥𝑖2, ..., 𝑥𝑖𝑝) предикторов 𝑋1, 𝑋2, ..., 𝑋𝑝, соответственно, а в каждом
наблюдении выполняется равенство
𝑌𝑖 = 𝜃0 + 𝜃1𝑥𝑖1 + 𝜃2𝑥𝑖2 + ... + 𝜃𝑝𝑥𝑖𝑝 + 𝜀𝑖, 𝑖 ∈{1, 2, ..., 𝑛}
Далее нам будет удобно работать в матричных обозначениях. Введем допол-
нительные обозначения𝑥10 = 𝑥20 = ... = 𝑥𝑛0 = 1 и
𝑋 =
⎛
⎜⎜⎝
𝑥10 𝑥11 . . . 𝑥1𝑝
𝑥20 𝑥21 . . . 𝑥2𝑝
... ... ... ...
𝑥𝑛0 𝑥𝑛1 . . . 𝑥𝑛𝑝
⎞
⎟⎟⎠, 𝑌 =
⎛
⎜⎜⎝
𝑦1
𝑦2
...
𝑦𝑛
⎞
⎟⎟⎠, Σ =
⎛
⎜⎜⎝
𝜀1
𝜀2
...
𝜀𝑛
⎞
⎟⎟⎠, Θ =
⎛
⎜⎜⎝
𝜃0
𝜃1
...
𝜃𝑝
⎞
⎟⎟⎠.
Тогда нашу модель в матричном виде можно переписать, как
𝑌 = 𝑋 ·Θ + Σ.
Замечание 3.1.1По сути дела, введение дополнительных обозначений –
суть введение еще одного предиктора𝑋0, который в каждом наблюдении
принимает одно и то же значение, равное1. При таком соглашении ясно,
что рассматриваемая нами модель
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝 + 𝜀
эквивалентно переписывается в виде
𝑌 = 𝜃0𝑋0 + 𝜃1𝑋1 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝 + 𝜀.
Отметим также следующее часто встречающееся определение.
Определение 3.1.4Матрица 𝑋, введенная выше, часто называется ре-
грессором.
Теперь перейдем к поиску коэффициентов множественной регрессии.
32
Высшая школа цифровой культуры Университет ИТМО
3.2 МНК для множественной регрессии
Для нахождения оценок неизвестных параметров аналогично тому, что
было сделано в одномерном случае, применим МНК. Тем самым оценки
̂︀𝜃0, ̂︀𝜃1, ...,̂︀𝜃𝑝 коэффициентов 𝜃0, 𝜃1, ..., 𝜃𝑝 находятся из решения задачи мини-
мизации функции𝜀(𝜃0, 𝜃1, ..., 𝜃𝑝), зависящей уже от(𝑝 + 1)-ой переменной
𝜀(𝜃0, 𝜃1, ..., 𝜃𝑝) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖1 −𝜃2𝑥𝑖2 −... −𝜃𝑝𝑥𝑖𝑝)2 .
Так как на самом деле нам нужно не наименьшее значение функции, а коэф-
фициенты, ее минимизирующие, то решается задача
arg min
𝜃0,...,𝜃𝑝
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖1 −𝜃2𝑥𝑖2 −... −𝜃𝑝𝑥𝑖𝑝)2 .
Определение 3.2.1Оценкой метода наименьших квадратов для неиз-
вестных параметров 𝜃0, 𝜃1, ...,𝜃𝑝 модели множественной регрессии назы-
вается набор значений параметров, минимизирующий выражение
𝜀(𝜃0, 𝜃1, ..., 𝜃𝑝) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖1 −𝜃2𝑥𝑖2 −... −𝜃𝑝𝑥𝑖𝑝)2 .
Поставленную задачу можно было бы решить ровно так, как мы делали ра-
нее, но вычислений становится больше, и они становятся более громоздкими.
Сформулируемокончательнуютеоремуивоспользуемсягеометрическимисо-
ображениями для ее пояснения.
Теорема 3.2.1Пусть столбцы регрессора 𝑋 линейно независимы, а 𝑛 >
(𝑝 + 1) (количество наблюдений больше, чем количество неизвестных пара-
метров модели). Минимум функции
𝜀(𝜃0, 𝜃1, ..., 𝜃𝑝) =
𝑛∑︁
𝑖=1
(𝑌𝑖 −𝜃0 −𝜃1𝑥𝑖1 −𝜃2𝑥𝑖2 −... −𝜃𝑝𝑥𝑖𝑝)2
единственен и достигается при
Θ =
(︀
𝑋𝑇 𝑋
)︀−1
𝑋𝑇 𝑌.
Доказательство. Проведем обоснование для случая𝑝 = 1, то есть при двух
неизвестных параметрах 𝜃0 и 𝜃1. В общем случае обоснование такое же, но
становится менее геометричным.
33
Высшая школа цифровой культуры Университет ИТМО
Пусть𝑋0,𝑋1 – столбцы регрессора𝑋, они линейно независимы, а значит
порождают двумерное пространство𝐿 = R2. Кроме того,
𝑋Θ = 𝑋0𝜃0 + 𝑋1𝜃1 ∈𝐿.
Из геометрических соображений (рисунок??) ясно, что, согласно методу наи-
меньших квадратов, мы минимизируем квадрат длины𝑌 −𝑋Θ (последний
вектор обозначен пунктиром). Понятно, что длина (а значит и квадрат дли-
ны) минимален, когда вектор𝑌 −𝑋Θ ортогонален 𝐿, то есть ортогонален
каждому вектору𝑋0 и 𝑋1:
(𝑌 −𝑋Θ) ⊥𝑋𝑖, 𝑖= 0, 1.
Иными словами,
𝑋𝑇 (𝑌 −𝑋Θ) = 0 ⇔𝑋𝑇 𝑌 −𝑋𝑇 𝑋Θ = 0.
Так какdet(𝑋𝑋 𝑇 ) ̸= 0, то
Θ = (𝑋𝑇 𝑋)−1𝑋𝑇 𝑌.
□
Рис. 11: Иллюстрация к доказательству
Итого,
̂︀Θ = (𝑋𝑇 𝑋)−1𝑋𝑇 𝑌.
34
Высшая школа цифровой культуры Университет ИТМО
Замечание 3.2.1Конечно, при решении практических задач полученные
формулы в явном виде применять приходится редко – соответствующие
функции вшиты в большинство математических пакетов.
Зная оценки коэффициентов модели, предсказание может быть сделано в
соответствии с формулой
𝑌 = ̂︀𝜃0 + ̂︀𝜃1𝑋1 + ̂︀𝜃2𝑋2 + ... + ̂︀𝜃𝑝𝑋𝑝.
Обратимся к примеру построения многомерной регрессии на основе ранее
рассмотренных примеров. Рассмотрим вот какой вопрос: как зависит сум-
марный объем продаж самолетов и телефонов от вклада в рекламу каждого
товара по отдельности? Распределение исходных данных можно увидеть на
рисунке ??. Ясно, что в нашем случае количество предикторов равно двум,
Рис. 12: Зависимость суммарного объема продаж от затрат на рекламу теле-
фонов и на рекламу самолетов
а значит модель имеет следующий вид:
𝑌 = 𝜃0 + 𝜃1𝑋1 + 𝜃2𝑋2 + 𝜀.
Найдем оценки ̂︀𝜃0, ̂︀𝜃1 и ̂︀𝜃2 неизвестных коэффициентов𝜃0, 𝜃1 и 𝜃2. Используя
вышеприведенные формулы, имеем
̂︀𝜃0 = 27.20, ̂︀𝜃1 = 1.08, ̂︀𝜃2 = 0.86,
и предсказание осуществляется из соотношения
𝑌 = 27.20 + 1.08 ·𝑋1 + 0.86 ·𝑋2.
35
Высшая школа цифровой культуры Университет ИТМО
Попыткиизобразитьлиниюрегрессии(плоскость)иразбросданныхсразных
ракурсов приведены на рисунках??, ?? и ??.
Рис. 13: Зависимость суммарного объема продаж от затрат на рекламу теле-
фонов и на рекламу самолетов и регрессия
3.3 Статистическая оценка параметров множе-
ственной линейной регрессии
В данном пункте аналогично тому, как было сделано в случае простей-
шей линейной регрессии, можно было бы подробно обсудить всевозможные
статические характеристики параметров модели множественной регрессии.
Мы не будем этого делать подробно, так как, с одной стороны, вся схема
очень похожа, а с другой – доставляет немало технических трудностей. Так
что мы ограничимся лишь сводкой важных для практики результатов.
Итак, мы снова будем исходить из следующих важных предположений
(первые три из которых часто называют условиями Гаусса-Маркова):
1. Случайные величины (ошибки) 𝜀1, 𝜀2, ..., 𝜀𝑛 независимы и одинаково
распределены;
2. Ошибки не носят систематического характера, то есть E𝜀𝑖 = 0 , 𝑖 ∈
{1, 2, ..., 𝑛};
3. Дисперсии рошибок одинаковы, то естьD𝜀𝑖 = 𝜎2 > 0, 𝑖 ∈{1, 2, ..., 𝑛}
(гомоскедастичность);
4. 𝜀𝑖 ∼N0,𝜎2 .
36
Высшая школа цифровой культуры Университет ИТМО
Рис. 14: Зависимость суммарного объема продаж от затрат на рекламу теле-
фонов и на рекламу самолетов и регрессия
Не вдаваясь в детали свойств оценок̂︀Θ (предлагаем их сформулировать ана-
логичным простейшему случаю образом самостоятельно), перейдем сразу к
практически значимым вопросам – формулам для доверительных интерва-
лов. Для начала получим несмещенную оценку дисперсий𝜎2 ошибок 𝜀𝑖.
Лемма 3.3.1В предположении условий 1-4, оценка
̂︀𝜎2
0 = 1
𝑛 −𝑝 −1
𝑛∑︁
𝑖=1
(𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖1 −̂︀𝜃2𝑥𝑖2 −... −̂︀𝜃𝑝𝑥𝑖𝑝)2 = 1
𝑛 −𝑝 −1|𝑌 −𝑋 ̂︀Θ|2
является несмещенной оценкой параметра𝜎2.
Теперь можно дать явное выражение и для доверительного интервала.
Теорема 3.3.1 (Доверительный интервал для𝜃𝑖) В предположении
условий 1 - 4, доверительный интервал уровня доверия(1−𝜀) для параметра
𝜃𝑖 – это интервал
(︁
̂︀𝜃𝑖 −𝑡1−𝜀/2 ·̂︀𝜎0
√︁
(𝑋𝑇 𝑋)−1
(𝑖+1)(𝑖+1), ̂︀𝜃𝑖 + 𝑡1−𝜀/2 ·̂︀𝜎0
√︁
(𝑋𝑇 𝑋)−1
(𝑖+1)(𝑖+1)
)︁
,
где 𝑡1−𝜀/2 – это(1 −𝜀/2) квантиль распределения Стьюдента с(𝑛 −𝑝 −1)
степенями свободы, а(𝑋𝑇 𝑋)−1
(𝑖+1)(𝑖+1) – элемент матрицы(𝑋𝑇 𝑋)−1, стоя-
щий на пересечении(𝑖 + 1)-ой строки и(𝑖 + 1)-ого столбца.
37
Высшая школа цифровой культуры Университет ИТМО
Рис. 15: Зависимость суммарного объема продаж от затрат на рекламу теле-
фонов и на рекламу самолетов и регрессия
Умея строить доверительные интервалы, ясно, как проверять гипотезы от-
носительно значений коэффициентов регрессии. В частности, как решать
задачу о проверке статистической значимости найденных оценок ̂︀𝜃𝑖, 𝑖 ∈
{0, 1, ..., 𝑝}.
Следствие 3.3.2 (О проверке гипотез относительно параметров𝜃𝑖)
Пусть проверяется гипотеза𝜃𝑖 = 𝜃0
𝑖 ∈R против альтернативы 𝜃𝑖 ̸= 𝜃0
𝑖 ,
то есть
H0 : 𝜃𝑖 = 𝜃0
𝑖
H𝑎 : 𝜃𝑖 ̸= 𝜃0
𝑖 .
Пусть
𝑡 = |̂︀𝜃𝑖 −𝜃0
𝑖 |
̂︀𝜎0
√︁
(𝑋𝑇 𝑋)−1
(𝑖+1)(𝑖+1)
,
где (𝑋𝑇 𝑋)−1
(𝑖+1)(𝑖+1) – элемент матрицы(𝑋𝑇 𝑋)−1, стоящий на пересечении
(𝑖 + 1)-ой строки и(𝑖 + 1)-ого столбца и𝑡1−𝜀/2 – квантиль уровня(1 −𝜀/2)
распределения Стьюдента с(𝑛 −𝑝 −1) степенями свободы, тогда
1. Если 𝑡 > 𝑡1−𝜀/2, то гипотезаH0 отклоняется на уровне значимости
𝜀.
2. Если 𝑡 ≤𝑡1−𝜀/2, то гипотезаH0 принимается на уровне значимости
𝜀.
38
Высшая школа цифровой культуры Университет ИТМО
Примеры применения и интерпретации всех обсужденных методов мы по-
дробно провели в простейшем случае, так что в этом пункте подробно на них
не останавливаемся.
3.4 Оценка точности модели множественной ре-
грессии
Как и в случае простейшей регрессии, будем рассматриватьRSE и R2
оценки для нашей модели. Как и ранее, ошибкаRSE задается соотношением
RSE = ̂︀𝜎0 =
⎯⎸⎸⎷ 1
𝑛 −𝑝 −1
𝑛∑︁
𝑖=1
(︁
𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖1 −... −̂︀𝜃𝑝𝑥𝑖𝑝
)︁2
=
=
√︃
|𝑌 −𝑋 ̂︀Θ|2
𝑛 −𝑝 −1 ,
и показывает «усредненную» сумму квадратов ошибок модели после того,
как произведена оценка параметров𝜃0, 𝜃1, ..., 𝜃𝑝. Аналогично простейшему
случаю, вводится в рассмотрение иR2 статистика:
R2 = 1 −
𝑛∑︀
𝑖=1
(︁
𝑌𝑖 −̂︀𝜃0 −̂︀𝜃1𝑥𝑖1 −... −̂︀𝜃𝑝𝑥𝑖𝑝
)︁2
𝑛∑︀
𝑖=1
(𝑌𝑖 −𝑌 )2
Снова, аналогично простейшему случаю совершенно понятно, что если по-
строенная идеально соответствует исходным данным, то все слагаемые в чис-
лителе последней дроби равны нулю и, тогда и только тогда,R2 = 1, то есть
модель идеальна. Наоборот, если̂︀𝜃1 = ̂︀𝜃2 = ... = ̂︀𝜃𝑝 = 0, то есть модель не
зависит от 𝑋1, 𝑋2, ..., 𝑋𝑝, то R2 = 0, и модель совершенно несостоятельна,
так как не отражает никакой зависимости между откликом и предиктором.
Как оказывается, значение R2 лишь увеличивается при добавлении новых
предикторов к модели, даже если они очень слабо влияют на отклик.
Можнопоказать,чтовзаимосвязьмежду R2 икорреляцией 𝑟2 дляслучая
многомерной регрессии такова:
R2 = 𝑟2
(︁
𝑌, ̂︀𝜃0 + ̂︀𝜃1𝑋1 + .. + ̂︀𝜃𝑝𝑋𝑝
)︁
.
В примере множественной регрессии с объемом продаж телефонов и са-
молетов в зависимости от средств, «вливаемых» в рекламу, коэффициенты
таковы:R2 = 0.89, аRSE = 11.04. Такие значения свидетельствуют о высоком
качестве модели.
39
Высшая школа цифровой культуры Университет ИТМО
3.5 Гипотеза о проверке статистической значи-
мости линейной регрессионной модели
Одним из важнейших применений оценкиR2 является проверка стати-
стической значимости построенной модели «в целом», то есть проверка сле-
дующего предложения: а есть ли зависимость отклика𝑌 от предикторов𝑋1,
𝑋2, ...,𝑋𝑝?
В качестве нулевой гипотезы расморим гипотезу
H0 : Все параметры𝜃𝑖 модели равны нулю при𝑖 ∈{1, 2, ..., 𝑝},
а в качестве альтернативной гипотезы
H𝑎 : Хотя бы один из параметров𝜃𝑖 не равен нулю при𝑖 ∈{1, 2, ..., 𝑝}.
Короче это можно записать так:
H0 : 𝜃1 = 𝜃2 = ... = 𝜃𝑝 = 0,
H𝑎 : 𝜃2
1 + 𝜃2
2 + ... + 𝜃2
𝑝 ̸= 0.
Тест проверки гипотез осуществим с помощью𝐹-статистики
𝐹 = R2
1 −R2
𝑛 −𝑝 −1
𝑝 .
Оказывается, что в условиях 1-4 введенная статистика имеет распределение
Фишера с параметрами(𝑝, 𝑛−𝑝 −1) (со степенями свободы𝑝 и 𝑛 −𝑝 −1).
Проверка гипотез осуществляется следующим образом.
Пусть 𝜀 > 0 и 𝑓1−𝜀 – квантиль распределения Фишера с праметрами
(𝑝, 𝑛−𝑝 −1) уровня (1 −𝜀).
1. Если 𝐹 > 𝑓1−𝜀, то гипотезаH0 отклоняется на уровне значимости𝛼, и,
как следствие, построенная модель является статистически значимой.
2. Если 𝐹 ≤𝑓1−𝜀, то гипотезаH0 принимается на уровне значимости𝛼, и,
какследствие,построеннаямодельявляетсястатистическинезначимой.
4 Немного о полиномиальной регрессии
Как мы уже неоднократно отмечали, линейная регрессия предполагает
линейную зависимость между откликом и предикторами. В реальных зада-
чах зависимость может не быть линейной. Оказывается, модель линейной
регрессии без существенных усложнений может быть расширена до модели
40
Высшая школа цифровой культуры Университет ИТМО
так называемой полиномиальной регрессии. Модель простейшей полиноми-
альной регрессии имеет вид
𝑌 = 𝜃0 + 𝜃1𝑋 + 𝜃2𝑋2 + ... + 𝜃𝑝𝑋𝑝 + 𝜀.
Важно отметить, что коэффициенты модели могут быть найдены с помо-
щью МНК, описанного выше, ведь перед нами не что иное, как многомерная
линейная регрессия, только вместо предикторов взяты степени𝑋:
𝑋1 = 𝑋, 𝑋2 = 𝑋2, ..., 𝑋𝑝 = 𝑋𝑝.
Значит, для обсчета такой модели (точнее, для нахождения оценок̂︀𝜃0, ̂︀𝜃1, ...,
̂︀𝜃𝑝), мы можем использовать аппарат линейной регрессии. Обычно степени
𝑋 выше четвертой не встречаются, так как полиномиальные кривые могут
получаться мало предсказуемой формы.
Значения отклика𝑌𝑖 по значениям предиктора𝑥𝑖 могут быть получены
по правилу
𝑌𝑖 = ̂︀𝜃0 + ̂︀𝜃1𝑥𝑖 + ̂︀𝜃2𝑥2
𝑖 + ... + ̂︀𝜃𝑝𝑥𝑝
𝑖 , 𝑖 ∈{1, 2, ..., 𝑛}.
Предположим, что нам даны данные, как на рисунке??. Результат полино-
Рис. 16: Набор данных для полиномиальной регрессии
миальной регрессии при различных степенях𝑝 можно увидеть на рисунке
??. Синим обозначена классическая линейная регрессия, зеленым – полино-
41
Высшая школа цифровой культуры Университет ИТМО
Рис. 17: Полиномиальная регрессии
миальная с полиномом второй степени, фиолетовым – полиномиальная с по-
линомом третьей степени. Видно, что полином третьей степени приближает
исходные данные лучше, чем все остальные.
Понятно, что полиномиальная регрессия – лишь частный случай модели,
обобщающей модель линейной регрессии. Можно взять какие-то функции𝜙1,
𝜙2, ...,𝜙𝑝 и построить более общую модель
𝑌 = 𝜃0 + 𝜃1𝜙1(𝑋) + 𝜃2𝜙2(𝑋) + ... + 𝜃𝑝𝜙𝑝(𝑋) + 𝜀,
обсчет которой производится, конечно, точно так же, как и в полиномиаль-
ном случае. Кроме того, все оценки коэффициентов регрессии проводятся
изложенным выше способам.
5 Заключение
В данной лекции мы рассмотрели подходы к решению задачи регрессии.
Сам аппарат решения этой задачи достаточно хорошо изучен и подкреплен
различными статистическими оценками, однако, как обычно, выбор конкрет-
ного набора функций𝜙𝑖 при построении обобщенной регрессии – задача, ко-
торая отдается на откуп исследователю. Удачи!
42
