Классификаторы
k-NN и наивный Байес
Высшая школа цифровой культуры
Университет ИТМО
dc@itmo.ru
Содержание
1 Классификатор k-NN 2
1.1 Небольшое введение . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Интуитивное представление об алгоритме k-NN . . . . . . . . . 3
1.3 Метрики . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 Классификация методом k-NN . . . . . . . . . . . . . . . . . . . 10
1.5 Пример классификации . . . . . . . . . . . . . . . . . . . . . . . 12
1.6 Взвешенный k-NN . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.7 Немного про число k в алгоритме k-NN . . . . . . . . . . . . . . 18
2 Алгоритмы, их оценка и разделение набора данных 19
2.1 Алгоритмы и эмпирический риск . . . . . . . . . . . . . . . . . . 20
2.2 Разделение тренировочного набора данных . . . . . . . . . . . . 24
2.3 k-блочная кросс-валидация (k-fold cross-validation) . . . . . . . . 26
2.4 Пример: взвешенный k-NN и k-блочная кросс-валидация . . . . 27
2.5 Проклятие размерности . . . . . . . . . . . . . . . . . . . . . . . 31
3 Наивный байесовский классификатор 35
3.1 Некоторые вводные замечания . . . . . . . . . . . . . . . . . . . 35
3.2 Небольшая вероятностная подоплека . . . . . . . . . . . . . . . . 35
3.3 Построение модели и вывод формул . . . . . . . . . . . . . . . . 38
3.3.1 Алгоритм построения классификатора на конкретной
выборке . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4 Пример построения классификатора . . . . . . . . . . . . . . . . 42
3.5 Классификация писем. Сглаживание по Лапласу . . . . . . . . . 44
4 Заключение 47
Высшая школа цифровой культуры Университет ИТМО
1 Классификатор k-NN
1.1 Небольшое введение
Добрый день, уважаемые слушатели! В прошлой лекции мы познако-
мились с первой из двух веток обучения с учителем – задачей регрессии –
задачей предсказания числа (точнее – непрерывной переменной) по извест-
ному набору входных данных. В данной лекции мы рассмотрим несколько
подходов к решению задачи второй ветки обучения с учителем – задачи клас-
сификации.
Как уже отмечалось ранее, с задачей классификации мы встречаемся
чуть ли не ежедневно: дети, собирая пирамидку, классифицируют ее элемен-
ты по размеру или по форме; взрослые перед стиркой раскладывают одежду
по цвету и типу ткани; в магазине овощи можно найти в отделе овощей, мо-
лочные продукты – в холодильнике, а химию – на отдельных полках, распо-
ложенных, как правило, вдалеке от съестных товаров; нас классифицируют
в банках при рассмотрении заявки на кредит (кредитоспособен или нет), при
трудоустройстве на новое место работы (компетентен или нет), во время пан-
демии (в зоне риска или нет) и много-много где еще. Приведенные примеры
– малая толика, скажем так, классификаций, которые каждый день с нами
(или над нами) производят(ся).
Шутки шутками, но задача классификации – одна из важнейших задач
и в производственной сфере, которая, конечно, активно влияет и на нашу
жизнь. Все мы пользуемся электронной почтой, и наверняка нас периодиче-
ски раздражает, что важное письмо теряется среди какой-то рекламы, ко-
торую мы считаем спамом. С другой стороны, папка со спамом тоже редко
пустует. Как туда попадают письма? Почему иногда туда попадают и важ-
ные письма? Дело заключается, конечно, в алгоритмах работы и настройке
так называемого спам-фильтра, который относительного каждого входящего
письма принимает решение: пропустить письмо или отправить в спам – вот
вам и двухклассовая классификация. Но спам – это хотя бы вполне безобид-
но. А что по поводу подозрительных финансовых транзакций?
Предположим, вы потеряли банковскую карту, но до сих пор этого не за-
метили. И с этой карты стали снимать через банкомат серии крупных сумм
денег. Бьемся об заклад – банк позвонит вам чуть ли не сразу, чтобы уточ-
нить: а вы ли совершаете эти операции. Если нет – карта будет моменталь-
но заблокирована. В частности из-за этого, для обеспечения безопасности,
многие банки убедительно просят сообщать им информацию о примерных
датах поездок за границу, глобальных тратах и многом другом. Иначе все
нестандартные действия сразу становятся подозрительными, а подозритель-
ные действия могут повлечь моментальную блокировку доступа к счету, что
2
Высшая школа цифровой культуры Университет ИТМО
за границей, согласитесь, весьма не к стати.
Итак, обосновав мотивировку решения задачи классификации, давайте
приступим к изучению, наверное, самого простого и естественного метода
метрической классификации – метода ближайших соседей.
1.2 Интуитивное представление об алгоритме k-
NN
Прежде чем строго сформулировать алгоритм, давайте поговорим про
саму идею и суть метода. Начнем же с формальной постановки задачи: раз-
беремся с тем, что нам дано, и что мы хотим получить.
Итак, тренировочный набор данных в разделе обучения с учителем у нас
стандартен: каждый объект набора обладает𝑝 предикторами 𝑋1, 𝑋2, ..., 𝑋𝑝
и некоторым откликом𝑌 . При решении задачи классификации множество
откликов 𝑌 состоит из так называемых меток классов – заранее известного,
конечного набора каких-то элементов. На практике классы обычно удобно
нумеровать числами.
Замечание 1.2.1В случае решения задачи двухклассовой классификации,
множество 𝑌 часто полагается, состоящим из двух элементов: «+1» и
«−1» (условно – положительный и отрицательный классы), т.е. 𝑌 =
{−1, 1}.
При решении задачи многоклассовой, скажем, 𝑀-классовой класси-
фикации (при 𝑀 > 2), множество 𝑌 обычно полагается таким: 𝑌 =
{1, 2, ..., 𝑀}, где каждый элемент𝑌 взаимно однозначно (биективно) от-
вечает некоторому классу предметной области.
Пример 1.2.1Например, решая задачу 4-ех классовой классификации с
возможными метками классов: «банан», «яблоко», «груша», «апельсин»,
взаимно однозначное соответствие может быть таким:
1 ↔«банан», 2 ↔«яблоко», 3 ↔«груша», 4 ↔«апельсин».
Нумерация, конечно, может быть и другой. Иногда она и вовсе не обяза-
тельна, все зависит от используемого алгоритма.
Ну что, с исходными данными разобрались. Теперь давайте обсудим логику
работы модели, которая могла бы на основе имеющихся тренировочных дан-
ных строить какие-то предсказания. Давайте попробуем придумать алгоритм
самостоятельно, а для этого обратимся к рисунку 1.
Будем считать, что рассматриваемые объекты описываются двумя чис-
ловыми предикторами𝑋1, 𝑋2 и одним из двух возможных откликов, условно,
3
Высшая школа цифровой культуры Университет ИТМО
Рис. 1: Пример интуитивного подхода
«крестики» или «нолики» (двухклассовая классификация). Так как в каче-
стве тренировочных данных используются объекты, принадлежность кото-
рых к классам заранее известна, мы сразу, для наглядности, изображаем их
крестиками и ноликами (в зависимости от отклика).
Задача, конечно, состоит не в том, чтобы красиво изобразить трениро-
вочные объекты, а в том, чтобы новому, тестовому наблюдению, обладаю-
щему двумя предикторами, назначить один из двух возможных классов. По-
смотрите на появившийся красный объект. Как вы считаете, к какому классу
его стоит отнести: крестики или нолики? Интуиция подсказывает, что новый
объект – это, скорее всего, крестик. Почему мы так решили? Ну как... Мы
видим, что рядом с новым объектом находятся в основном крестики, поэтому
разумно считать, что он тоже крестик. Такое рассуждение находит отраже-
ние и в поговорке: «скажи мне кто твой друг, и я скажу кто ты». Впрочем,
такая определенность бывает не всегда.
Рассмотрим менее очевидную ситуацию (рисунок 2 а)). К какому теперь
классу отнести тестовый красный объект? Задумайтесь, какой бы вы сделали
выбор, а затем как бы вы его обосновали? Правильного ответа на поставлен-
ный вопрос (да, собственно, как и на предыдущий), конечно, нет, но неко-
торой логики придерживаться все-таки можно. Скорее всего вы догадались,
что самый простой и естественный путь – найтиближайшего представите-
ля тренировочных данных и посмотреть на метку его класса. На глаз, или
путем несложных расчетов мы видим, что этот ближайший представитель от-
носится к классу «нолики», значит возникает идея и тестовому наблюдению
4
Высшая школа цифровой культуры Университет ИТМО
назначить класс «нолики». Все? Или не все?
Немного подумав, скорее всего возникает весьма резонный вопрос: а по-
чему мы принимаем решение, основываясь лишь на метке класса одного тре-
нировочного объекта? А вдруг этот представитель – выброс? Тогда мы точно
ошибемся. И вообще, опираясь на нашу логику получается, что мы совершен-
но слепы: видим только то, что перед самым носом, и вообще не обращаем
внимание на перспективу. Такой подход работает далеко не всегда и, конечно,
не может считаться удовлетворительным. Что же делать? Решение, по боль-
шому счету, лежит на поверхности: нужно немного «открыть глаза» нашему
алгоритму – рассмотреть побольше ближайших тренировочных данных. Да-
вайте в нашем примере рассмотрим отклики не одного, а, например, трех
«ближайших соседей». Что будет в этом случае (рисунок 2 б))?
а)
 б)
Рис. 2: Выбор количества соседей.
Мы видим, что теперь среди трех ближайших представителей трениро-
вочных данных два крестика и один нолик. Какой класс назначать тестовому
наблюдению? Видимо, решение принимается большинством, а значит назна-
чаемый класс (если доверять трем ближайшим соседям) – «крестики».
Грубо говоря, по описанному нами принципу и работает алгоритм k-
ближайших соседей (k-nearest neighbors), или, сокращенно, k-NN. Даже на
таком простом примере мы видим, что чуть ли не основная задача, возни-
кающая при использовании данного алгоритма, – правильный подбор числа
k. О том, как это делать, мы поговорим еще не раз. Сейчас же мы обратим
внимание вот на что: оказывается, что проблема выбора k – это вовсе не
единственная проблема.
В нашем примере мы то и дело использовали слово «ближайших», и вся
наша классификация опиралась на понятие расстояния. В рассмотренном
примере расстояние измерялось по прямой, но всегда ли это нужно делать
именно так? И вообще, а как еще можно измерять расстояние, и почему ино-
гда это настолько критично? Давайте разбираться.
5
Высшая школа цифровой культуры Университет ИТМО
1.3 Метрики
Задаваясь вопросом о вычислении расстояния между какими-либо дву-
мя объектами, первое, что приходит голову – это взять линейку и произвести
замер по прямой (или, что то же самое, по кратчайшему пути), получив за-
ветное число. Будут ли это метры, футы – не важно. В то же время ясно,
что такой подход в условиях задачи возможен далеко не всегда. Представь-
те себе, условно, глобус, и пингвина на южном полюсе. Пусть глобус имеет
формушара,арадиусшараравен,скажем,тремметрам.Пингвинхочеткрат-
чайшим образом попасть на северный полюс, какое при этом расстояние он
пройдет? Просто соединить две точки и получить6 метров не выйдет – пинг-
вин же не пройдет сквозь глобус, верно? А как тогда? Смекалистые, конечно,
догадались, что пингвину нужно пройти половину длины окружности цен-
трального сечения шара. Так как это сечение – круг радиуса3, то половина
длины окружности, его ограничивающей – это3𝜋 ≈9.4 метра. Согласитесь,
намного больше, чем6.
Можно привести и другой пример. Представьте, что вы приехали на
экскурсию в Санкт-Петербург и планируете свой маршрут по осмотру до-
стопримечательностей. Расстояние от Зимнего дворца до Петропавловской
крепости очень маленькое – рукой подать, да только Нева мешает. А чтобы
дойти или доехать, придется сделать неплохой крюк: пройти по Дворцовому
мосту, обогнуть здание биржи, пересечь Биржевой мост, пройти еще чуть-
чуть по набережной – и мы у цели. Но опять, расстояние, которое пришлось
преодолеть, оказалось в несколько раз больше, чем расстояние по прямой!
К чему это мы ведем? К тому, что выбор способа измерения расстоя-
ния очень важен. Даже больше, зачастую этот выбор и вовсе оказывается
ключевым вопросом при решении той или иной «метрической» задачи.
Определение 1.3.1Пусть 𝑋 – какое-то множество. Метрикой (рассто-
янием) на множестве𝑋 называют функцию𝑑(𝑥, 𝑦) :𝑋 ×𝑋 →R, которая
для любых𝑥, 𝑦, 𝑧∈𝑋 удовлетворяет трем свойствам:
1. Она неотрицательна, то есть𝑑(𝑥, 𝑦) ≥0, причем
𝑑(𝑥, 𝑦) = 0⇔𝑥 = 𝑦.
2. Она симметрична, то есть
𝑑(𝑥, 𝑦) =𝑑(𝑦, 𝑥).
3. Выполняется неравенство треугольника, то есть
𝑑(𝑥, 𝑦) ≤𝑑(𝑥, 𝑧) +𝑑(𝑦, 𝑧).
6
Высшая школа цифровой культуры Университет ИТМО
Пояснимкаждыйпунктопределенияболеедетально.Первоетребованиевесь-
маестественно:расстояниедолжнобытьнеотрицательным,причемоноравно
нулю между объектами тогда и только тогда, когда объекты совпадают. Вто-
рое требование говорит, что нет разницы: измерять ли расстояние от точки𝑥
до точки𝑦, или от точки𝑦 до 𝑥, – результат будет один и тот же. Третье же
требование есть не что иное, как неравенство треугольника, утверждающее,
что если вершины треугольника находятся в точках𝑥, 𝑦, 𝑧, то длина любой
стороны не больше суммы длин двух других.
Рассмотрим теперь основные метрики, которые встретятся нам далее.
Предположим, что мы работаем в пространствеR𝑝, 𝑥, 𝑥′∈R𝑝 и
𝑥 = (𝑥1, 𝑥2, ..., 𝑥𝑝), 𝑥 ′= (𝑥′
1, 𝑥′
2, ..., 𝑥′
𝑝).
1. Начнем с известной всем еще со школы евклидовой метрики (евкли-
дова расстояния), являющейся, по сути, обобщением теоремы Пифаго-
ра. Расстояние𝑑𝐸(𝑥, 𝑥′) между точками 𝑥 и 𝑥′ полагается равным, по
определению, корню из суммы квадратов разностей соответствующих
координат:
𝑑𝐸 (𝑥, 𝑥′) =
√︁
(𝑥1 −𝑥′
1)2 + (𝑥2 −𝑥′
2)2 + . . .+
(︀
𝑥𝑝 −𝑥′𝑝
)︀2
,
𝑑𝐸 (𝑥, 𝑥′) =
⎯⎸⎸⎷
𝑝∑︁
𝑖=1
(𝑥𝑖 −𝑥′
𝑖)2.
Написанное расстояние выдает, наверное, наиболее привычное для нас
расстояние – расстояние по прямой. Причем не важно, на прямой (в
R1), на плоскости (вR2) или в пространстве (вR3) мы находимся.
2. Насамомделе, упомянутое толькочто евклидово расстояние– это лишь
частный случай вот какого расстояния – расстояния Минковского:
𝑑𝑞 (𝑥, 𝑥′) =
𝑞
√︁
|𝑥1 −𝑥′
1|𝑞 + |𝑥2 −𝑥′
2|𝑞 + . . .+
⃒⃒𝑥𝑝 −𝑥′𝑝
⃒⃒𝑞
, 𝑞 ≥1,
𝑑𝑞 (𝑥, 𝑥′) = 𝑞
⎯⎸⎸⎷
𝑝∑︁
𝑖=1
|𝑥𝑖 −𝑥′
𝑖|𝑞, 𝑝 ≥1.
Понятно, что евклидово расстояние получается из расстояния Минков-
ского в случае, когда𝑞 = 2.
3. Если взять𝑞 = 1, то получим расстояние, которое называется манхэт-
тенским или расстоянием городских кварталов:
𝑑1 (𝑥, 𝑥′) =
𝑝∑︁
𝑖=1
|𝑥𝑖 −𝑥′
𝑖|.
7
Высшая школа цифровой культуры Университет ИТМО
Это расстояние, по сути, решает вопрос расчета длины поездки по горо-
ду, если все улицы города строго перпендикулярны. В этом случае про-
извольный «адекватный» маршрут, соединяющий две выбранные точ-
ки, имеет одинаковую длину, совпадающую с нашими о ней представ-
лениями (рисунок 3). На рисунке красным, синим и желтым построены
«адекватные» маршруты, соединяющие две точки. Как уже говорилось,
с точки зрения манхэттенского расстояния все эти маршруты имеют
одинаковую длину. Зеленым же построено кратчайшее расстояние – по
прямой. Такой маршрут, конечно, невозможен.
Рис. 3: Сравнение евклидова и манхэттенского расстояний
4. Если же 𝑞 устремить к +∞, то получим так называемое расстояние
Чебышёва:
𝑑∞(𝑥, 𝑥′) = max
𝑖∈{1,...,𝑝}
|𝑥𝑖 −𝑥′
𝑖|.
Расстояние Чебышёва показывает максимальное покоординатное от-
клонение одного объекта от другого и может быть полезно, например,
вот в какой ситуации. Представьте, что вы совершаете замеры воды
в водохранилищах 1, 2, ..., 𝑛, причем норма уровня воды в каждом из
них соответственно равна𝑥′
1, 𝑥′
2, ..., 𝑥′
𝑝. Тогда расстояние Чебышёва по-
казывает максимальное отклонение уровня воды от нормы среди всех
8
Высшая школа цифровой культуры Университет ИТМО
водохранилищ.
Можнопроверить,чтовсефункции,которыемытолькочтообсудили,иправ-
да являются метриками, то есть удовлетворяют определению, сформулиро-
ванному ранее. Так как мы будем использовать метрики для сравнения «бли-
зости» объектов, немаловажно понимать их геометрическую интерпретацию.
Заодно увидим, в рамках какой метрики мыслите конкретно Вы :)
Представьте себе множество точек плоскости, удаленных от центра ко-
ординат 𝑂(0, 0) на расстояние не большее, чем1. Что вы себе представили?
Вероятнее всего, перед глазами всплывает круг единичного радиуса с цен-
тром в начале координат. В чем подвох, спросите вы? Подвох в том, что в
изначальном вопросе использовано слово расстояние, и, в зависимости от то-
го, какое расстояние берется, геометрический образ будет сильно отличаться.
Чтобы показать это наглядно, переведем заданный ранее вопрос на язык ма-
тематики: требуется изобразить множество точек𝑥 ∈R2 таких, что
𝑑𝑞(𝑂, 𝑥) ≤1, 𝑂 (0, 0),
при разных значениях𝑝.
В случае с евклидовым расстоянием, при𝑝 = 2, заявленное неравенство
записывается, как:
𝑑2(𝑂, 𝑥) =
√︁
𝑥2
1 + 𝑥2
2 ≤1.
Возводя его в квадрат, приходим к эквивалентному неравенству:
𝑥2
1 + 𝑥2
2 ≤1,
которое задает круг радиуса1 c центром в начале координат. Большинство
людей думает именно в терминах евклидовой метрики.
В случае манхэттенского расстояния, при𝑞 = 1, получаем внутренность
ромба (на самом деле – повернутого на𝜋/4 квадрата):
|𝑥1|+ |𝑥2|≤ 1.
В случае расстояния Чебышёва, при𝑞 = +∞, получаем внутренность
квадрата со стороной два:
max
𝑖∈{1,2}
|𝑥𝑖|≤ 1 ⇔|𝑥1|≤ 1, |𝑥2|≤ 1.
Все множества можно увидеть на рисунке 4.
9
Высшая школа цифровой культуры Университет ИТМО
Рис. 4: Расстояния слева направо: евклидово, манхэттенское, Чебышёва.
1.4 Классификация методом k-NN
Разобравшись с понятием метрики, мы теперь смело можем перейти к
построению алгоритма классификации методом k-NN. Пусть имеется трени-
ровочный набор данных𝑋 = (𝑥1, . . . , 𝑥𝑛) объема 𝑛,
𝑥𝑖 = (𝑥𝑖1, 𝑥𝑖2, ..., 𝑥𝑖𝑝), 𝑖 ∈{1, 2, ..., 𝑛},
причем каждому объекту 𝑥𝑖 соответствует отклик 𝑦𝑖 ∈ 𝑌 , а на 𝑝-мерном
множестве объектов задана метрика𝑑. Тогда алгоритм классификации таков:
1. Дляновогообъекта 𝑧 вычислитьрасстояния 𝑑(𝑧, 𝑥𝑖) докаждогообъекта
𝑥𝑖, 𝑖 ∈{1, 2, . . . , 𝑛}.
2. Элементы тренировочного набора данных расположить в порядке
неубывания расстояний до𝑧:
𝑑(𝑧, 𝑥(𝑧)
1 ) ≤𝑑(𝑧, 𝑥(𝑧)
2 ) ≤... ≤𝑑(𝑧, 𝑥(𝑧)
𝑛 ),
где𝑥(𝑧)
1 = 𝑥𝑡1 , а𝑡1 – любое из решений задачиArg min
𝑖∈{1,2,...,𝑛}
𝑑(𝑧, 𝑥𝑖),𝑥(𝑧)
2 = 𝑥𝑡2 ,
а 𝑡2 – любое из решений задачи Arg min
𝑖∈{1,2,...,𝑛}∖{𝑡1}
𝑑(𝑧, 𝑥𝑖), и так далее.
3. Отклики перенумеровать согласно пункту 2:
𝑦(𝑧)
𝑖 = 𝑦𝑡𝑖, 𝑖 ∈{1, 2, ..., 𝑛}.
4. Среди ближайших𝑘 соседей найти такой класс𝑦 ∈𝑌 , который встре-
чается чаще всего (в случае, если классов несколько – выбрать любой):
𝑎 (𝑧, 𝑘) = Arg max
𝑦∈𝑌
𝑘∑︁
𝑖=1
I
(︁
𝑦(𝑧)
𝑖 = 𝑦
)︁
,
10
Высшая школа цифровой культуры Университет ИТМО
где функция I(𝐴) – индикатор события𝐴, равна единице, если собы-
тие 𝐴 произошло (истинно), и нулю иначе. В рассматриваемом случае
индикатор дает единицу, если метка класса𝑦(𝑧)
𝑖 равна метке класса𝑦.
Итак, в первом пункте мы вычисляем расстояние от тестового объекта до
каждого элемента тренировочного набора данных. Во втором пункте мы упо-
рядочиваем эти расстояния по неубыванию и перенумеровываем элементы
тренировочного набора данных (первый элемент – самый близкий к тесто-
вому объекту, второй – второй по близости, и так далее). В третьем пункте
мы перенумеровываем отклики согласно перенумерованным объектам, а в
четвертом ищем тот класс (или те классы), объекты которых встретились
среди 𝑘 ближайших элементов чаще всего. Описанный алгоритм полностью
повторяет все те эвристические рассуждения, которые были даны в самом
начале.
Конечно, логично считать, что выбранная в алгоритме метрика𝑑 доста-
точно качественно отображает сходство объектов, то есть чем больше значе-
ние 𝑑(𝑥, 𝑥′), тем менее схожими являются объекты𝑥 и 𝑥′, и наоборот. Опи-
санный алгоритм классификации как раз-таки и предполагает, что схожие
объекты гораздо чаще лежат в одном классе, нежели в разных. Говоря более
формально, мы работаем в рамках так называемойгипотезы компактно-
сти: возможные классы образуют компактно локализованные подмножества
в пространстве объектов.
Определение 1.4.1Алгоритмы, основанные на анализе сходства объек-
тов при помощи метрики𝑑, часто называют метрическими.
Из данного определения понятно, что описанный алгоритм k-NN является
метрическим. Впрочем, иногда алгоритмы называют метрическими и когда
«функция сходства»𝑑 метрикой, согласно введенному нами ранее определе-
нию, не является (особенно часто в этом случае нарушено неравенство тре-
угольника).
Отметим также, что алгоритм k-NN часто называютленивым, так как
само по себе обучение происходит лишь в момент предсказания. В итоге,
грубо говоря, обучение состоит просто в хранении тренировочного набора
данных.
Замечание 1.4.1На практике признаки могут иметь разные единицы из-
мерения, что может искажать реальное расстояние между объектами.
Один из способов решения проблемы – нормализация данных. Напомним,
что один из самых простых способов – переход к относительным значени-
ям с помощью линейной нормировки:
𝑋′
𝑖 = 𝑋𝑖 −𝑋min
𝑋max −𝑋min
,
11
Высшая школа цифровой культуры Университет ИТМО
где 𝑋min – минимальное значение рассматриваемого предиктора, 𝑋max –
максимальное значение, 𝑋𝑖 – нормируемое значение, 𝑋′
𝑖 – нормированное
значение.
1.5 Пример классификации
Давайте рассмотрим работу описанного алгоритма вот на каком приме-
ре. Данные вы можете видеть в таблице. Каждый объект характеризуется
двумя атрибутами (предикторами): сладость и хруст, а также относится к
одному из трех классов: фрукт, овощ или протеин.
Продукт Сладость Хруст Класс
банан 10 1 фрукт
апельсин 7 4 фрукт
виноград 8 3 фрукт
креветка 2 2 протеин
бекон 1 5 протеин
орехи 3 3 протеин
сыр 2 1 протеин
рыба 3 2 протеин
огурец 2 8 овощ
яблоко 9 8 фрукт
морковь 4 10 овощ
сельдерей 2 9 овощ
салат айсберг 3 7 овощ
груша 8 7 фрукт
Итак, начнем с того, что разместим все эти данные, для больше наглядности,
на плоскости (рисунок 5). По горизонтали отметим сладость, по вертикали
хруст. Таким образом, каждому фрукту, овощу или протеину соответствует
точка на плоскости своего цвета: овощам – желтые точки, фруктам – темно-
зеленые, а протеинам – светло-зеленые. Мы видим, что данные очень хорошо
разделились на группы (рисунок 6).
Теперь предположим, что нам необходимо классифицировать новый объ-
ект, зная только значения сладости и хруста. Допустим, это перец со значе-
ниями сладость — 6, хруст — 9. Согласно описанному алгоритму, нам необ-
ходимо рассчитать расстояние от тестового объекта до всех тренировочных
объектов, предварительно выбрав метрику 𝑑. Выберем сначала евклидову
метрику 𝑑2.
В нашем примере все переменные будем считать нормированными по
шкалеот 0 до10.Произведемрасчетрасстояниямеждуобъектами«морковь»
12
Высшая школа цифровой культуры Университет ИТМО
Рис. 5: Визуализация тренировочных данных.
Рис. 6: Объединение в группы.
13
Высшая школа цифровой культуры Университет ИТМО
с атрибутами(4, 10) и «перец» с атрибутами(6, 9):
𝑑2(морковь, перец) =
√︀
(6 −4)2 + (9−10)2 ≈2.24.
Аналогичный расчет произведем для всех остальных тренировочных объек-
тов, результаты можно видеть в таблице:
Продукт Расстояние
банан 8.94
апельсин 5.10
виноград 6.32
креветка 8.06
бекон 6.4
орехи 6.71
сыр 8.94
рыба 7.62
огурец 4.12
яблоко 3.16
морковь 2.24
сельдерей 4
салат айсберг 3.61
груша 2.83
Отсортируем наши данные по неубыванию расстояния, результат представ-
лен в таблице:
Продукт Расстояние
морковь 2.24
груша 2.83
яблоко 3.16
салат айсберг 3.61
сельдерей 4
огурец 4.12
апельсин 5.10
виноград 6.32
бекон 6.4
орехи 6.71
рыба 7.62
креветка 8.06
банан 8.94
сыр 8.94
14
Высшая школа цифровой культуры Университет ИТМО
Итак, «ближайший» объект к тестовому – это морковь, затем идет груша,
яблоко и так далее. Теперь выберем число𝑘. Пусть 𝑘 = 1, тогда, согласно
описанному алгоритму, тестовому объекту присваивается метка класса само-
гоближайшегообъектатренировочныхданных.Таккакближайшийобъект–
это морковь, принадлежащая классу овощей, то и перцу присваивается класс
овощ. При𝑘 = 3самыми близкими окажутся яблоко, груша и морковь, тогда
перец будет отнесен к классу фруктов, так как представителей этого класса
среди ближайших соседей большинство. При𝑘 = 4 к ближайшим соседям
добавится салат, и тестовый объект оказывается возможным отнести как к
классу фрукт, так и к классу овощ. При𝑘 = 5, 6, 7, перец будет уверенно
классифицирован, как овощ.
Продукт Класс Расстояние
морковь овощ 2.24
груша фрукт 2.83
яблоко фрукт 3.16
салат айсберг овощ 3.61
сельдерей овощ 4
огурец овощ 4.12
апельсин фрукт 5.10
виноград фрукт 6.32
бекон протеин 6.4
орехи протеин 6.71
рыба протеин 7.62
креветка протеин 8.06
банан фрукт 8.94
сыр протеин 8.94
Теперь давайте посмотрим, изменится ли наша классификация, если
взять другую метрику? Возьмем манхеттенское расстояние𝑑1:
𝑑1(𝑥, 𝑥′) =
𝑝∑︁
𝑖=1
|𝑥𝑖 −𝑥′
𝑖|.
Будем классифицировать все тот же перец со значениями предикторов: сла-
дость – 6, хруст – 9. Найдем расстояние между объектами «морковь» и «пе-
рец»:
𝑑1(морковь, перец) =|6 −4|+ |9 −10|= 3.
Различия в способе вычисления расстояния отдельно показаны на рисунке 7.
Так, если евклидово расстояние𝑑2 – это кратчайший путь по прямой (по ги-
потенузе),томанхэттенскоерасстояние 𝑑1 –этосуммадлинсоответствующих
15
Высшая школа цифровой культуры Университет ИТМО
Рис. 7: Отличие в расстояниях.
катетов. Аналогичный расчет произведем для всех точек и сразу отсортиру-
ем тренировочные объекты по неубыванию расстояний от тестового объекта,
результаты представлены в таблице:
Продукт Класс Расстояние
морковь овощ 3
яблоко фрукт 4
сельдерей овощ 4
груша фрукт 4
огурец овощ 5
салат айсберг овощ 5
апельсин фрукт 6
виноград фрукт 8
бекон протеин 9
орехи протеин 9
рыба протеин 10
креветка протеин 11
банан фрукт 12
сыр протеин 12
Теперь мы не сможем уверенно классифицировать перец при𝑘 = 2, 3, 4, но
сможем отнести его к его классу овощей при𝑘 = 1, 5, 6, 7. Также можно об-
ратить внимание, что манхэттенское расстояние дает нам целые значения в
рамках рассматриваемого примера. Кроме того, имеется очень много одина-
16
Высшая школа цифровой культуры Университет ИТМО
ковых расстояний, а сортировка равноудаленных объектов сильно влияет на
результат.
1.6 Взвешенный k-NN
Итак, на данном этапе мы познакомились с простейшим вариантом алго-
ритма k-NN. В то же время, при рассмотрении последнего примера мы столк-
нулись со следующей проблемой: при некоторых значениях k новый, тесто-
вый объект непонятно как классифицировать – ему подходит сразу несколько
классов. Конечно, в алгоритме сказано, что в качестве нового класса можно
взять любой из подходящих, но иногда это кажется абсурдным. Представьте,
скажем, что вы решаете задачу двухклассовой классификации. Тогда невоз-
можность классифицировать новый объект, по своей сути означает, что алго-
ритм отвечает что-то вроде «решай сам, я не знаю». Такое, конечно, вполне
возможно, но нежелательно.
В случае двухклассовой классификации вы, наверное, догадались, что
простейшее решение таково: в качестве значений параметра k достаточно
брать только нечетные числа. В таком случае количество ближайших соседей
не будет делиться (нацело) на2, а значит и равенства в количестве голосов
за тот или иной класс не будет. Таким образом, классификация обязательно
будет проведена. А что, если классов больше, чем два? В данной лекции мы
изложим лишь один подход к решению этой проблемы – это так называемый
взвешенный метод k-NN. Идея метода такова: каждому объекту тренировоч-
ных данных𝑥𝑖 приписывается некоторый вес𝜔𝑖, зависящий, вообще говоря,
от тестового объекта𝑧, т.е.
𝜔𝑖 = 𝜔𝑖(𝑥𝑖, 𝑧), 𝑖 ∈{1, 2, ..., 𝑛}.
По итогу, тестовому наблюдению приписывается тот класс, суммарный вес
представителей которого среди k ближайших соседей наибольший (или один
из таких классов, если их несколько).
Алгоритм взвешенного k-NN может быть формально записан, например,
так. Пусть имеется тренировочный набор данных𝑋 = (𝑥1, . . . , 𝑥𝑛) объема 𝑛,
𝑥𝑖 = (𝑥𝑖1, 𝑥𝑖2, ..., 𝑥𝑖𝑝), 𝑖 ∈{1, 2, ..., 𝑛},
причем каждому объекту 𝑥𝑖 соответствует отклик 𝑦𝑖 ∈ 𝑌 , а на 𝑝-мерном
множестве объектов задана метрика𝑑.
1. Дляновогообъекта 𝑧 вычислитьрасстояния 𝑑(𝑧, 𝑥𝑖) докаждогообъекта
𝑥𝑖, а также веса
𝜔𝑖 = 𝜔𝑖(𝑥𝑖, 𝑧), 𝑖 ∈{1, 2, . . . , 𝑛}
17
Высшая школа цифровой культуры Университет ИТМО
2. Элементы тренировочного набора данных расположить в порядке
неубывания расстояний до𝑧:
𝑑(𝑧, 𝑥(𝑧)
1 ) ≤𝑑(𝑧, 𝑥(𝑧)
2 ) ≤... ≤𝑑(𝑧, 𝑥(𝑧)
𝑛 ),
где𝑥(𝑧)
1 = 𝑥𝑡1 , а𝑡1 – любое из решений задачиArg min
𝑖∈{1,2,...,𝑛}
𝑑(𝑧, 𝑥𝑖),𝑥(𝑧)
2 = 𝑥𝑡2 ,
а 𝑡2 – любое из решений задачи Arg min
𝑖∈{1,2,...,𝑛}∖{𝑡1}
𝑑(𝑧, 𝑥𝑖), и так далее.
3. Отклики и веса перенумеровать согласно пункту 2:
𝑦(𝑧)
𝑖 = 𝑦𝑡𝑖, 𝜔 (𝑧)
𝑖 = 𝜔𝑡𝑖, 𝑖 ∈{1, 2, ..., 𝑛}.
4. Среди ближайших𝑘 соседей найти такой класс𝑦 ∈𝑌 , который имеет
наибольший вес (в случае, если классов несколько – выбрать любой):
𝑎 (𝑧, 𝑘) = Arg max
𝑦∈𝑌
𝑘∑︁
𝑖=1
I
(︁
𝑦(𝑧)
𝑖 = 𝑦
)︁
𝜔(𝑧)
𝑖 .
Один из способов задания весов основан на близости «проверяемого» объекта
по отношению к другим. Идея способа проста: чем меньше расстояние от тре-
нировочного объекта до тестового, тем более значимым при классификации
является тренировочный объект. Даже точнее, тем более значим голос этого
тренировочного объекта, или тем более значима метка его класса. Напри-
мер, в качестве веса𝑤𝑖 можно взять величину, обратно пропорциональную
квадрату расстояния между объектами𝑥𝑖 и 𝑧, то есть
𝜔𝑖 = 𝜔𝑖(𝑥𝑖, 𝑧) = 1
𝑑2(𝑥𝑖, 𝑧).
Таким образом, выбранные веса увеличивают значимость ближайших объ-
ектов, и уменьшают значимость более дальних объектов. Чуть позже мы
подробно разберем числовой пример, но сейчас еще немного поговорим про
проблему выбора числа k.
1.7 Немного про число k в алгоритме k-NN
Дадим несколько комментариев по поводу того, как в общем и целом
изменение параметра k влияет на классификацию при использовании метода
k-NN (конечно, лишь на конкретных данных). Посмотрите на рисунок 8.
Совершенно понятно, что при малых значениях параметра k мы суще-
ственно ограничиваем «зону видимости» для нашего классификатора. На-
пример, при 𝑘 = 1 он просто «запоминает» тренировочный набор данных,
18
Высшая школа цифровой культуры Университет ИТМО
Рис. 8: Влияние числа𝑘.
отсюда и неровности в границах классов, какие-то островки причудливой
формы. Поянтно также, что при небольших значениях k классификатор ока-
зывается чувствительным к выбросам, хотя и практически идеально работает
с теми данными, на которых учился.
С увеличением k граница разделения становится весьма гладкой и пре-
вращаетсяпрактическивпрямую,разделениенаклассывыглядитболееесте-
ственным, появляется все больше ошибок на тренировочных данных. Модель
оказывается легкой и удобной в интерпретации.
Так какое значение k лучше? Это весьма сложный вопрос. Разумным
подходом является оценка частоты ошибок путем разделения исходного на-
бора данных на тестовую и обучающую выборки. Давайте об этом подробно
и поговорим.
2 Алгоритмы, их оценка и разделение набора данных
Итак, вопрос, который до сих пор остается нерешенным – это вопрос
выбора параметра алгоритма k-NN, то есть параметра k. Оказывается, ответ
19
Высшая школа цифровой культуры Университет ИТМО
на этот вопрос не лежит на поверхности. Чтобы в нем разобраться, придется
копнуть несколько глубже и обратиться к идеологии. Точнее, начать с ответа
на вопрос: а чего мы пытаемся добиться (глобально), решая задачу класси-
фикации, и какие при этом возникают проблемы?
Добиться мы пытаемся следующего: мы хотим произвольному объекту
из пространства признаков присвоить какой-то класс (желательно, конечно,
правильный) из имеющегося набора. Какие возникают проблемы? Пробле-
мы вполне себе понятны: мы ограничены в знаниях, ведь у нас в руках есть
только тренировочный набор данных; мы же должны опыт, полученный на
этом конечном наборе данных, распространить, вообще говоря, на больший
объем данных (возможно, бесконечный). Итого оказывается, что наша ос-
новная цель – научить алгоритм обобщать полученные знания на новые, ра-
нее не встречавшиеся прецеденты. Но сделав это используя, например, k-NN,
правильнее скорее задуматься вот о чем: сделали-то сделали, но как теперь
проверить, что получилось хорошо, и что вообще значит это «хорошо»? Для
того, чтобы осветить эти вопросы, для начала введем базовые определения,
которые в дальнейшем будут использоваться чуть ли не всюду.
2.1 Алгоритмы и эмпирический риск
Мыпостоянноиспользуемследующиевыражения:алгоритм,обучитьал-
горитм, оценить алгоритм. Но что такое этот «алгоритм»? Оказывается, что
все весьма просто.
Определение 2.1.1Пусть 𝑋 – множество всевозможных объектов, а𝑌
– множество откликов. Отображение
𝑎 : 𝑋 →𝑌
называется алгоритмом.
Итак, алгоритм – это функция, сопоставляющая произвольному объекту
некоторый отклик.
Замечание 2.1.1Поясним введенные обозначения несколько детальнее.
Рассматривая модель простейшей линейной регрессии, в качестве множе-
ства 𝑋 мы рассматривали множество вещественных чиселR или какое-
то его непрерывное подмножество, а алгоритм задавался следующим обра-
зом:
𝑎(𝑥) =𝜃0 + 𝜃1𝑥.
В только что рассмотренном примере классификации, пространство𝑋 –
это пространство
𝑋 = [0, 10] ×[0, 10],
20
Высшая школа цифровой культуры Университет ИТМО
так как имеется всего два предиктора (сладость и хруст), которые могут
принимать произвольные значения в диапазоне от0 до 10 (включительно),
а 𝑌 – это трехэлементное множество
𝑌 = {овощ, фрукт, протеин}.
Алгоритм k-NN (и взвешенный алгоритм k-NN), описанные ранее, как раз-
таки являются алгоритмами в смысле введенного определения.
Теперь давайте разбираться, как мы будем оценивать наш алгоритм.
Определение 2.1.2Пусть 𝑎 : 𝑋 →𝑌 – алгоритм. Функция потерь (loss
function) 𝐿(𝑎, 𝑥) – это произвольная неотрицательная функция, характе-
ризующая величину ошибки алгоритма𝑎 на объекте𝑥.
Понятно,чтоеслиошибкиалгоритманаобъекте 𝑥 нет,то,видимо, 𝐿(𝑎, 𝑥) = 0
и оказывается разумным принять следующее определение.
Определение 2.1.3Пусть 𝑎 : 𝑋 →𝑌 – алгоритм, а 𝐿(𝑎, 𝑥) – функция
потерь. Если𝐿(𝑎, 𝑥) = 0, то ответ𝑎(𝑥) называют корректным.
Конечно, при оценке того или иного алгоритма, разумно рассматривать при-
сущиеемупотериненакаком-тоодномобъекте(вдругон,например,выброс),
а на большем количестве объектов и, что привычно в статистике, как-то эти
потери усреднять (то есть смотреть на качество работы алгоритма «в сред-
нем»). Тут и возникает так называемый эмпирический риск.
Определение 2.1.4Пусть 𝑥1, 𝑥2, ..., 𝑥𝑛 ∈ 𝑋 – некоторый набор данных,
𝑎(𝑥) : 𝑋 →𝑌 – алгоритм, 𝐿(𝑎, 𝑥) – функция потерь. Функционалом по-
терь (эмпирическим риском (empirical risk), функционалом средних потерь)
называется функционал
𝑄(𝑎, 𝐿, 𝑥1, ..., 𝑥𝑛) = 1
𝑛
𝑛∑︁
𝑖=1
𝐿(𝑎, 𝑥𝑖).
Замечание 2.1.2Отметим, что в качестве функции потерь при реше-
нии задачи классификации часто используют такую функцию:
𝐿(𝑎, 𝑥) =I(𝑎(𝑥) ̸= 𝑦(𝑥)) =
=
{︃
1, если отклик𝑦(𝑥) объекта 𝑥 не совпал с𝑎(𝑥)
0, иначе ,
21
Высшая школа цифровой культуры Университет ИТМО
где 𝑦(𝑥) – это известный отклик объекта𝑥. Смысл этой функции доста-
точно прост: она выдает1, если ответ алгоритма не совпадает с истин-
ным откликом, и0 иначе. В этом случае функционал потерь принимает
следующий вид
𝑄(𝑎, 𝐿, 𝑥1, ..., 𝑥𝑛) = 1
𝑛
𝑛∑︁
𝑖=1
I(𝑎(𝑥𝑖) ̸= 𝑦(𝑥𝑖))
и выдает не что иное, как долю неправильных ответов построенного клас-
сификатора на наборе данных𝑥1, 𝑥2, ..., 𝑥𝑛. Ясно, что мы хотим миними-
зировать значение написанного функционала. Запомним это.
При решении задачи регрессии, мы уже встречались со следующей
функцией потерь:
𝐿(𝑎, 𝑥) = (𝑎(𝑥) −𝑦(𝑥))2,
где снова 𝑦(𝑥) – это известный отклик объекта 𝑥. Функционал потерь,
построенный по данной функции потерь, таков
𝑄(𝑎, 𝐿, 𝑥1, ..., 𝑥𝑛) = 1
𝑛
𝑛∑︁
𝑖=1
(𝑎(𝑥𝑖) −𝑦(𝑥𝑖))2.
Напомним, что решая задачу простейшей линейной регрессии, алгоритм
мы искали в виде𝑎(𝑥) = 𝜃0 + 𝜃1𝑥. Функционал потерь в этом случае при-
нимает уже знакомый вид
𝑄(𝑎, 𝐿, 𝑥1, ..., 𝑥𝑛) = 1
𝑛
𝑛∑︁
𝑖=1
(𝜃0 + 𝜃1𝑥𝑖 −𝑦(𝑥𝑖))2,
а для поиска коэффициентов𝜃0 и 𝜃1 мы его, опять-таки, минимизировали,
то есть решали задачу
Arg min
𝜃0,𝜃1
1
𝑛
𝑛∑︁
𝑖=1
(𝜃0 + 𝜃1𝑥𝑖 −𝑦(𝑥𝑖))2 = Arg min
𝜃0,𝜃1
𝑛∑︁
𝑖=1
(𝜃0 + 𝜃1𝑥𝑖 −𝑦(𝑥𝑖))2.
Как легко заметить, это в точности метод наименьших квадратов! Функ-
ции потерь бывают и другого вида, пока что мы не будем касаться этого
вопроса более детально.
Написанные наблюдения как, видимо, и здравый смысл подсказывают нам,
что алгоритм тем лучше, чем меньше значение соответствующего ему функ-
ционала потерь. Тем самым, мы приходим к следующему выводу: имеет
22
Высшая школа цифровой культуры Университет ИТМО
смысл искать такой алгоритм или такие алгоритмы𝑎(𝑥), которые миними-
зируют функционал потерь, то есть решать следующую задачу:
Arg min
𝑎∈𝒜
𝑄(𝑎, 𝐿, 𝑥1, ..., 𝑥𝑛).
В написанной задаче 𝒜– это некоторое рассматриваемое нами множество
алгоритмов. В случае задачи простейшей линейной регрессии, множество ал-
горитмов 𝒜описывалось, например, так:
𝒜= {𝜃0 + 𝜃1𝑥, 𝜃0, 𝜃1 ∈R}.
При рассмотрении метода k-NN под множеством алгоритмов𝒜можно пони-
мать такое множество:
𝒜= {алгоритмы k-NN, 𝑘 ∈N}.
Замечание 2.1.3Все приведенные примеры множеств алгоритмов пока
что, по сути, являются ни чем иным, как множеством возможных пара-
метров (или гиперпараметров) некоторых «фиксированных» алгоритмов.
На практике часто возникают, конечно, и более «хитрые» множества.
Обучение алгоритма – это, по сути дела, выбор «наилучшего» алго-
ритма из набора𝒜.
Итак, нам удалось разобраться с общей постановкой задачи оценивания
алгоритма. В то же время, вопрос выбора (или построения) множества
𝑥1, 𝑥2, ..., 𝑥𝑛, на котором проводится оценка, все еще остается открытым. Этот
вопрос оказывается очень важным, и его никак нельзя недооценивать.
Замечание 2.1.4Подчеркнем отдельно, что поднятый вопрос, как и за-
тронутый ранее вопрос выбора множества𝒜– это не просто какая-то
теоретическая трудность; это, в некотором смысле, одна из важнейших
отправных точек при построении хорошей модели.
Для того, чтобы не быть голословными, приведем такой «абсурдный»
пример, показывающий, как важно правильно и четко ставить задачу в
машинном обучении (да и не только в нем). Скажем, пусть мы решили
оценивать алгоритм навсем тренировочном наборе данных𝑥1, 𝑥2, ..., 𝑥𝑛 с
откликами 𝑦1, 𝑦2, ..., 𝑦𝑛, и никак не ограничили множество𝒜. Какой тогда
алгоритм будет лучше всего справляться с поставленной задачей? Навер-
ное, вы догадались, вот такой:
𝑎(𝑥𝑖) =𝑦𝑖.
23
Высшая школа цифровой культуры Университет ИТМО
Конечно, внимательный слушатель скажет, что такой алгоритм не удо-
влетворяет самому определению алгоритма, ведь он выдает хоть какой-то
ответ только на элементах тренировочного набора данных. Не беда, при
𝑥 /∈{𝑥1, 𝑥2, ..., 𝑥𝑛}значение 𝑎(𝑥) можно положить любым, при этом всегда
будет выполняться:
𝑄(𝑎, 𝐿, 𝑥1, ..., 𝑥𝑛) = 0,
а лучше и быть не может!
Итак, мы составили алгоритм, который идеален с точки зрения функциона-
ла потерь, но совершенно бесполезен на практике. Причиной этому служит
даже не то, что мы не ограничили как-то множество𝒜. Причиной этому
служит то, что алгоритмобучался на выборке𝑥1, 𝑥2, ..., 𝑥𝑛, для элементов
которой он ужевидел правильные ответы. Нам же интересно, как он будет
себя вести на данных, которых он раньшене видел. Но чтобы это оценить,
мы сами должны знать «правильные ответы». Давайте посмотрим, как это
реализуется на практике.
2.2 Разделение тренировочного набора данных
Как мы поняли в предыдущем пункте, нас на самом-то деле интересу-
ет нахождение такого алгоритма𝑎(𝑥), для которого значение функционала
потерь было бы маленьким на новом, ранее невиданном алгоритмом наборе
данных при условии, что данные «подчиняются закономерностям трениро-
вочного набора данных». Последняя, возможно несколько странная фраза
станет понятной чуть позже, когда мы коснемся вероятностной подоплеки
кухни задачи классификации. Сейчас же ее понимать можно и нужно имен-
но в смысле озвученного ранеепринципа компактности.
Замечание 2.2.1Подчеркнем отдельно, что данный пункт лекции явля-
ется, субъективно, очень важным, потому что он пытается пояснить
идеологию обучения и тестирования алгоритмов в машинном обучении.
Алгоритмы могут быть очень хорошими, но при неправильном обучении
ничего «хорошего», скорее всего, не получится.
Итак, вернемся к вопросу: откуда же нам взять данные, которые алгоритм
не видел ранее, если у нас самих есть только тренировочный набор данных
𝐷 = {(𝑥1, 𝑦1), (𝑥2, 𝑦2), ...,(𝑥𝑛, 𝑦𝑛)}?
Первая идея – разделить этот набор на две части𝐷𝑡𝑟𝑎𝑖𝑛 и 𝐷𝑡𝑒𝑠𝑡 и обучить
модель на суженном тренировочном наборе 𝐷𝑡𝑟𝑎𝑖𝑛, после чего оценить ре-
зультат на наборе𝐷𝑡𝑒𝑠𝑡. Еще раз подчеркнем, что мы искусственным образом
сужаем исходный тренировочный набор данных𝐷 до набора𝐷𝑡𝑟𝑎𝑖𝑛 лишь для
24
Высшая школа цифровой культуры Университет ИТМО
того, чтобы проверить, а как обученный нами алгоритм𝑎(𝑥) работает на тех
данных, которых он не видел ранее, вычислив функционал потерь
𝑄(𝑎, 𝐿, 𝐷𝑡𝑒𝑠𝑡)
на тестовом наборе𝐷𝑡𝑒𝑠𝑡.
Замечание 2.2.2Конечно, возникает вопрос, в каком соотношении и как
делить тренировочный набор данных𝐷. Обычно отношение 𝐷𝑡𝑟𝑎𝑖𝑛 : 𝐷𝑡𝑒𝑠𝑡
составляет примерно 80 : 20, но такое деление очень условно. Куда более
деликатен следующий вопрос: а как делить?
Если нет никакой информации про время, когда собирались те или иные
данные из набора𝐷, а набор данных𝐷 репрезентативен, то вряд ли можно
придумать что-то лучшее, чем случайное разделение.
Если же есть данные о времени сбора, то лучше производить деле-
ние по какой-то временной засечке. Например, если данные собираются в
течение недели (и есть надежда, что они однородны), то все данные, со-
бранные (условно) до четверга включительно идут в тренировочный набор,
а данные, собранные после четверга, в тестовый. Проведя такую засечку,
мы полностью выкидываем при обучении алгоритма информацию о данных,
полученных в будущем, то есть полностью от него, от будущего, абстра-
гируемся. Это, кстати, ровно то, чего мы и хотим: чтобы то, что обу-
чилось сегодня, работало завтра. Конечно, все это – эмпирические подходы
к оценке алгоритма.
Оказывается, что на практике специалисты часто поступают иначе – более
хитро. Они делят набор данных не на две части, а на три, откалывая от𝐷𝑡𝑒𝑠𝑡
так называемую validation-part𝐷𝑣𝑎𝑙. И это совсем нетривиальный, но очень
важный момент. Попробуйте догадаться, зачем так делают специалисты. Мы,
конечно, это сейчас же обсудим.
Представьте, что у вас есть несколько моделей, скажем, несколько клас-
сификаторов k-NN с разными значениями k, причем каждый из этих клас-
сификаторов вы обучили на тренировочном наборе данных𝐷𝑡𝑟𝑎𝑖𝑛, а также
оценили на тестовом 𝐷𝑡𝑒𝑠𝑡. Какой вы выберите? Конечно тот, функционал
потерь которого на тестовом наборе данных выдаст меньшее значение, не так
ли? Если так, то тут и кроется (не идеологическая, а концептуальная) ошиб-
ка: финальное решение опять же принято с использованием информации из
будущего, а это ну никак не годится! То, чего мы так старались избежать –
использования неизвестного при обучении – опять играет злую шутку. Такое
переиспользование тестового набора данных часто ведет к так называемому
переобучению модели, и ничего хорошего не сулит.
В итоге, важным оказывается следующее наблюдение: использование те-
стового набора данных𝐷𝑡𝑒𝑠𝑡 слишком часто, по своей сути ведет к тому, что
25
Высшая школа цифровой культуры Университет ИТМО
алгоритм начинает обучаться и на наборе данных𝐷𝑡𝑒𝑠𝑡, что противоречит
всему тому, что было сказано ранее.
Теперь к практике. Имея набор данных𝐷𝑣𝑎𝑙, имеет смысл выбрать тот
алгоритм, который допускает меньшее число ошибок на этом наборе данных,
и тогда именно этот алгоритм и нужно оценить на𝐷𝑡𝑒𝑠𝑡, предварительно обу-
чивнавсемнаборе 𝐷𝑡𝑟𝑎𝑖𝑛 (включаяискуственноотколотыйнабор 𝐷𝑣𝑎𝑙).Полу-
ченное значение функционала потерь на𝐷𝑡𝑒𝑠𝑡 как раз-таки разумно считать
«средней» ошибкой обученного алгоритма. В следующем пункте мы расска-
жемпротакназываемуюk-блочнуюкросс-валидацию(k-foldcross-validation),
которая обобщает описанную ранее идею.
2.3 k-блочная кросс-валидация (k-fold cross-
validation)
Итак, предположим, что у нас есть некоторый набор алгоритмов. Как
же из них выбрать лучший, используя уже озвученную k-блочную кросс-
валидацию?Насамомделемеханизмвыбораоченьпрост,онсразустановится
понятным, если посмотреть на следующий рисунок (рисунок 9):
Рис. 9: k-fold cross-validation
Вместе с рисунком проведем и формальное описание алгоритма.
1. Пусть исходный набор данных разделен на𝐷𝑡𝑟𝑎𝑖𝑛 и 𝐷𝑡𝑒𝑠𝑡.
2. Набор данных𝐷𝑡𝑟𝑎𝑖𝑛 разбивается на𝑘 непересекающихся примерно оди-
наковых по размеру блоков (множеств):
𝐷𝑡𝑟𝑎𝑖𝑛 = 𝐷1 ∪𝐷2 ∪. . .∪𝐷𝑘.
26
Высшая школа цифровой культуры Университет ИТМО
3. Пусть 𝑖 меняется в диапазоне{1, 2, ..., 𝑘}:
(a) Производится обучение выбранного алгоритма𝑎(𝑥) на множестве
𝐷𝑖
𝑡𝑟𝑎𝑖𝑛 = 𝐷𝑡𝑟𝑎𝑖𝑛 ∖𝐷𝑖.
(b) Производится тестирование выбранного алгоритма𝑎(𝑥) на множе-
стве 𝐷𝑖
𝑡𝑒𝑠𝑡 = 𝐷𝑖 и вычисляется значение
Loss𝑖(𝑎) =𝑄(𝑎, 𝐿, 𝐷𝑖
𝑡𝑒𝑠𝑡).
4. Финальная оценка ошибки алгоритма 𝑎(𝑥) в результате k-блочной
кросс-валидации такова:
Loss(𝑎) =1
𝑘
𝑘∑︁
𝑖=1
Loss𝑖(𝑎).
5. Шаги 3 −4 повторяются для каждого исследуемого (сравниваемого)
алгоритма.
6. В качестве лучшего алгоритма выбирается тот алгоритм𝑎, значение
Loss(𝑎) которого наименьшее.
7. Выбранный лучший алгоритм обучается на наборе данных𝐷𝑡𝑟𝑎𝑖𝑛 и те-
стируется на наборе𝐷𝑡𝑒𝑠𝑡. Полученная ошибка𝑄(𝑎, 𝐿, 𝐷𝑡𝑒𝑠𝑡) считается
«средней» ошибкой лучшего алгоритма.
Итак, по сути дела тренировочный набор данных делится на несколько бо-
лее маленьких кусочков, каждый из которых на своей итерации выступа-
ет тестовым набором данных – набором данных для оценивания алгоритма.
Финальная оценка каждого алгоритма – это усреднение оценок на каждой
итерации.
2.4 Пример: взвешенный k-NN и k-блочная
кросс-валидация
Вернемсякнашемупримеруспродуктамии,используякросс-валидацию
при 𝑘 = 3выясним, какой из алгоритмов (взвешенный k-NN при k =3, 4 или
обычный k-NN при k =4) справляется лучше. Предположим, что деление на
𝐷𝑡𝑟𝑎𝑖𝑛 и 𝐷𝑡𝑒𝑠𝑡 уже произведено, а в качестве𝐷𝑡𝑟𝑎𝑖𝑛 выступает уже знакомый
нам набор данных
27
Высшая школа цифровой культуры Университет ИТМО
Продукт Сладость Хруст Класс
банан 10 1 фрукт
апельсин 7 4 фрукт
виноград 8 3 фрукт
креветка 2 2 протеин
бекон 1 5 протеин
орехи 3 3 протеин
сыр 2 1 протеин
рыба 3 2 протеин
огурец 2 8 овощ
яблоко 9 8 фрукт
морковь 4 10 овощ
сельдерей 2 9 овощ
салат айсберг 3 7 овощ
груша 8 7 фрукт
Видно, что объем тренировочного набора данных𝐷𝑡𝑟𝑎𝑖𝑛 равен 14, а значит
блоки могут быть выбраны, например, таких размеров:5, 5, 4.
На первом шаге будем использовать первый блок в качестве тестового
(таблица 1), а второй и третий – в качестве тренировочных (таблица 2).
Продукт Сладость Хруст Класс
банан 10 1 фрукт
апельсин 7 4 фрукт
виноград 8 3 фрукт
креветка 2 2 протеин
бекон 1 5 протеин
Таблица 1: Итерация 1. Тестовые данные.
Отобразим на плоскости данные из тренировочного набора (рисунок 10) и
проведем классификацию продуктов из тестового набора взвешенным мето-
дом k-NN, когда параметр k равен4. В качестве метрики используем евкли-
дово расстояние, а в качестве веса𝜔𝑖 – величину, обратно пропорциональную
квадрату расстояния между объектами𝑥𝑖 и классифицируемым объектом𝑧:
𝜔𝑖 = 1
𝑑2(𝑥𝑖, 𝑧).
Найдем расстояние от продукта «банан» до всех элементов тренировочного
набораданных,атакжеприсущиеимвеса.Например,«расстояние»отбанана
до сыра равно:
𝑑2 (банан, сыр) =
√︀
(10 −2)2 + (1−1)2 = 8.
28
Высшая школа цифровой культуры Университет ИТМО
Продукт Сладость Хруст Класс
орехи 3 3 протеин
сыр 2 1 протеин
рыба 3 2 протеин
огурец 2 8 овощ
яблоко 9 8 фрукт
морковь 4 10 овощ
сельдерей 2 9 овощ
салат айсберг 3 7 овощ
груша 8 7 фрукт
Таблица 2: Итерация 1. Тренировочные данные.
Вес тренировочного объекта «сыр» в этом случае равен:
𝑤банан, сыр= 1
𝑑2
2 (банан, сыр) = 1
64 ≈0.016.
Аналогичным образом вычислим расстояния и веса до остальных объектов,
данные представлены в таблице 3. Назначим класс на основе четырех бли-
Продукт Класс Расстояние Вес
орехи протеин 7.28 0.019
сыр протеин 8 0.016
рыба протеин 7.071 0.020
огурец овощ 10.63 0.009
яблоко фрукт 7.071 0.020
морковь овощ 10.817 0.009
сельдерей овощ 11.314 0.008
салат айсберг овощ 9.22 0.012
груша фрукт 6.325 0.025
Таблица 3: Расстояние и веса для продукта «банан»
жайших соседей: груша, яблоко, рыба и орехи. Если бы мы использовали
обычный k-NN, то мы бы столкнулись с проблемой конкуренции двух клас-
сов: фрукты и протеины. При использовании взвешенного k-NN такой про-
блемы удается избежать. Действительно, сумма весов, отвечающих классу
фрукты, равна 0.045, а сумма весов, отвечающих классу протеины, равна
0.039. Таким образом, банан будет классифицирован как фрукт.
Аналогичные вычисления выполняются для каждого продукта из тесто-
вого набора (таблица 4). Вы можете проверить себя и выполнить расчеты,
мы же представим сразу итоговые результаты классификации.
29
Высшая школа цифровой культуры Университет ИТМО
Рис. 10: Итерация 1. Тренировочные данные.
Как видим из таблицы, имеет место1 ошибка (подвел бекон), а значит
при использовании стандартной для задачи классификации функции потерь
𝐿(𝑎, 𝑥) =I(𝑎(𝑥) ̸= 𝑦(𝑥)),
значение функционала потерь будет равно0.2:
Loss1(взв. 4-NN) =𝑄(𝑎, 𝐿, 𝐷1
𝑡𝑟𝑎𝑖𝑛) =1
5 = 0.2.
Далее вы можете ознакомиться с результатами итераций 2 и 3, для которых
повторяются аналогичные шаги (таблицы 5, 6). В этих итерациях назначен-
ные классы полностью совпадают с исходными. Таким образом частота оши-
бок будет нулевой, а усреднение всех ошибок по трем итерациям составит
около 0.07:
Loss(взв. 4-NN) =1
3
3∑︁
𝑖=1
Loss𝑖 = 1
15 ≈0.07.
Если в качестве k во взвешенном k-NN использовать число3, то для нового
алгоритма
Loss(взв. 3-NN) =1
3
3∑︁
𝑖=1
Loss𝑖 = 1
5 = 0.2.
30
Высшая школа цифровой культуры Университет ИТМО
Продукт Сладость Хруст Исходный
класс
Назначенный
класс
банан 10 1 фрукт фрукт
апельсин 7 4 фрукт фрукт
виноград 8 3 фрукт фрукт
креветка 2 2 протеин протеин
бекон 1 5 протеин овощ
Таблица 4: Итерация 1. Тестовые данные.
Если же оставить𝑘 = 4, но при этом отказаться от взвешивания соседей, то
для такого алгоритма
Loss(4-NN) =1
3
3∑︁
𝑖=1
Loss𝑖 = 1
4 = 0.25.
Итак, согласно описанному алгоритму, в качестве финального классифика-
тора из рассматриваемого набора имеет смысл выбирать тот, оценка сред-
ней ошибки Loss для которого является наименьшей. Среди рассмотренных
алгоритмов наименьшая ошибка – это ошибкаLoss(взв. 4-NN). Значит, сре-
ди исследуемых алгоритмов мы выбираем алгоритм взвешенных 4-NN, и для
дальнейшихпрогнозов(дляобучения)вкачестветренировочногонаборадан-
ных используем все множество𝐷𝑡𝑟𝑎𝑖𝑛. Для оценки истинной средней ошибки
выбранного алгоритма, имеет смысл протестировать его на заранее заготов-
ленном множестве𝐷𝑡𝑒𝑠𝑡.
Продукт Сладость Хруст Исходный
класс
Назначенный
класс
орехи 3 3 протеин протеин
сыр 2 1 протеин протеин
рыба 3 2 протеин протеин
огурец 2 8 овощ овощ
яблоко 9 8 фрукт фрукт
Таблица 5: Итерация 2. Тестовые данные.
2.5 Проклятие размерности
Раз уж мы в предыдущем пункте снова затронули алгоритм k-NN, обра-
тимся к еще одной, теперь уже «метрической» проблеме машинного обучения
– проклятию размерности.
31
Высшая школа цифровой культуры Университет ИТМО
Продукт Сладость Хруст Исходный
класс
Назначенный
класс
морковь 4 10 овощ овощ
сельдерей 2 9 овощ овощ
салат айсберг 3 7 овощ овощ
груша 8 7 фрукт фрукт
Таблица 6: Итерация 3. Тестовые данные.
Давайте подумаем над такой задачей. Предположим, что наши трениро-
вочные данные (𝑛 штук) равномерно распределены в некотором единичном
гиперкубе в пространствеR𝑑 при достаточно большом𝑑. Достаточно боль-
шое значение𝑑 означает, что каждый объект обладает достаточно большим
количеством признаков (𝑑 штуками).
Поместим теперь в центр нашего гиперкуба новый тестовый объект и
ответим на такой вопрос: гиперкуб с ребром какой длины𝑙 должен быть по-
строен вокруг тестового объекта, чтобы внутри этого гиперкуба содержалось
k ближайших соседей (рисунок 11), то есть k точек тренировочного набора
данных?
Рис. 11: Многокоробочность
Математика в данном случае очень проста. Объем рассматриваемого ги-
перкуба равен, конечно, 𝑙𝑑 и, в силу того, что тренировочные данные рас-
32
Высшая школа цифровой культуры Университет ИТМО
пределены равномерно в гиперкубе с единичным ребром, а внутрь искомого
гиперкуба должно попасть k ближайших соседей,
𝑙𝑑 ≈𝑘
𝑛 ⇒ 𝑙 ≈
(︂𝑘
𝑛
)︂1/𝑑
.
На первый взгляд ничего удивительного, но давайте посчитаем. Для этого
зададим какие-то «адекватные» параметры. Пусть, например,𝑘 = 10, 𝑛 =
1000, тогда исследуем длину ребра при разных𝑑, учитывая, что
𝑙 ≈
(︂ 1
100
)︂1/𝑑
.
При 𝑑 = 2 получим 0.1 – неплохо. При 𝑑 = 10 получим около 0.631, при
𝑑 = 100 получим около0.955, а при𝑑 = 1000 около 0.995. Попытка проил-
люстрировать написанные соотношения приведена на рисунке 12.
Рис. 12: Заполнение пространства
Что мы видим? А видим мы то, что чем больше значение𝑑, тем дальше
все объекты находятся друг от друга и, грубо говоря, тем ближе они распо-
ложены к границам гиперкуба. Внимание: они вовсе не внутри этого куба,
ведь иначе длину ребра гиперкуба, содержащего𝑘 соседей, можно бы было
взять меньшей, и не было бы такой заполняемости. И вот это наблюдение
– огромная проблема, ведь из него следует, что метод k-NN вовсе несосто-
ятелен! При увеличении 𝑑 ближайшие соседи, по сути, тоже находятся на
границах гиперкуба вместе со всеми остальными тренировочными данными.
Выбрав наудачу одного соседа, тут же рядом, на расстоянии пол шага на-
ходится куча других тренировочных данных лишь случайно не вошедших в
число ближайших соседей: какой тут принцип компактности?!
Все эти рассуждения очень естественны с точки зрения математики, но
очень неестественны для нашего понимания реальности, и это абсолютно нор-
33
Высшая школа цифровой культуры Университет ИТМО
мально: мы привыкли к трехмерному пространству. Попробуем дать еще од-
но объяснение тому, что происходит, уже на вероятностном языке. Давайте
найдем вероятность того, что случайно взятая точка внутри гиперкуба нахо-
дится не на его границе, а внутри. Для этого, конечно, нужно, чтобы каждая
одномерная координата точки лежала внутри отрезка, например,[0, 1], а не
на его границе. Отступим на𝜀 >0 от граничных точек, тогда вероятность
оказаться во внутренней части отрезка равна(1 −2𝜀) (рисунок 13). Понятно,
Рис. 13: Вероятностное объяснение
что для𝑑-мерного пространства (в силу независимости признаков) получаем,
что вероятность оказаться внутри гиперкуба равна
(1 −2𝜀)𝑑,
и последнее выражение очень быстро стремится к нулю с ростом𝑑. Итак, при
больших 𝑑 вероятность оказаться внутри гиперкуба почти ноль.
На самом деле после всех этих рассуждений может показаться, что мет-
рические подходы и вовсе несостоятельны. Это, конечно, не так, ведь прове-
денные рассуждения касаются только равномерного распределения данных,
что бывает не часто. Куда чаще оказывается, что на самом деле данные за-
полняют какое-то подпространство пространства R𝑑 (гиперплокость, более
хитрое многообразие), и тогда метод k-NN вполне хорошо справляется со
своей задачей. Все сказанное еще раз указывает на важность предваритель-
ной обработки данных, снижения размерности и прочим изыскам, ведь на
практике достаточно редко складывается ситуация, что все предикторы ис-
ходного набора данных важны.
Итак,поговоривдостаточнопрометрическиеклассификаторы,перейдем
теперь к некоторой альтернативе – вероятностным классификаторам.
34
Высшая школа цифровой культуры Университет ИТМО
3 Наивный байесовский классификатор
3.1 Некоторые вводные замечания
В предыдущем пункте мы рассмотрели один из способов решения зада-
чи классификации – с использованием метрического классификатора k-NN.
Классифицируя новый объект, мы пользовались лишь имеющимися у нас
тренировочными данными, не используя (и, в общем-то, не предполагая) ни-
какой вероятностной зависимости между предикторами и откликами, хотя
о, в некотором смысле, наличии таковой говорилось еще во введении. Чем
пользовались мы, так это гипотезой компактности. В этой части лекции мы
рассмотрим метод классификации, имеющий откровенно вероятностную при-
роду, а основным инструментом будет выступать известная теорема Байеса.
Необходимость (как и адекватность) вероятностных моделей подкрепля-
ется следующими наблюдениями. В задачах обучения с учителем элементы
тренировочного набора данных – это, в общем-то, не реальные объекты, а
лишь доступные нам данные о реальных объектах, и это наблюдение очень
существенно. Например, данные могут бытьнеточными: измерение значе-
ний признаков (атрибутов) тренировочных объектов, а также измерение от-
кликов зачастую производится с погрешностью (хотя бы из-за округления) –
достаточно вспомнить предположения модели регрессии, обсуждаемой ранее.
Кроме того, данные могут бытьнеполными, так как зачастую измеряют-
ся не все мыслимые признаки, а лишь те, которые доступны для измерения.
Вспомните, иногда и вовсе непонятно, какие признаки присущи объекту, а
какие – нет. В результате, одному и тому же описанию объекта𝑥 могут в ре-
альности соответствовать как разные объекты, так и разные отклики. Имен-
но поэтому предположение о существовании явной зависимости, дающей по
описанию объекта единственно верный ответ (отклик), кажется достаточно
наивным. Вероятностные модели позволяют, в некотором смысле, устранить
описанную некорректность. Кроме того, явные зависимости являются част-
ным случаем вероятностных, но мы подробно на этом останавливаться не
будем.
3.2 Небольшая вероятностная подоплека
В отличие от случая k-NN, где мы начали рассмотрение алгоритма с на-
глядных геометрических соображений, здесь мы сразу приведем вероятност-
ную подоплеку, и, собственно, сам алгоритм, а затем посмотрим на примеры
его применения в реальных задачах.
Для начала напомним некоторые факты из теории вероятностей. Начнем
с определения условной вероятности.
Определение 3.2.1Пусть событие 𝐵 таково, что P(𝐵) > 0. Условной
35
Высшая школа цифровой культуры Университет ИТМО
вероятностью события𝐴 при условии, что произошло событие𝐵, называ-
ется число
P(𝐴|𝐵) =P (𝐴 ∩𝐵)
P(𝐵) .
Иными словами, условная вероятность – это отношение вероятности события
𝐴 ∩𝐵, то есть события «произошло и𝐴, и 𝐵», к вероятности события𝐵.
Иначе – ищется отношение вероятности доли события 𝐴, попавшего в 𝐵,
к вероятности события𝐵 – не что иное, как перенормировка, или, просто-
напросто, изменение (сужение) вероятностного пространства.
Это определение моментально может быть переписано в виде
P (𝐴 ∩𝐵) =P(𝐴|𝐵)P(𝐵).
Если, к тому же,P(𝐴) > 0, то мы в праве рассмотреть и такую условную
вероятность:
P(𝐵|𝐴) =P(𝐵 ∩𝐴)
P(𝐴) = P(𝐴 ∩𝐵)
P(𝐴) ,
откуда
P(𝐴 ∩𝐵) =P(𝐵|𝐴)P(𝐴).
Объединяя написанные соотношения, получаем так называемые правила
умножения:
P (𝐴 ∩𝐵) =P(𝐴|𝐵)P(𝐵) =P(𝐵|𝐴)P(𝐴).
Заменив теперь в определении условной вероятности числитель на второе со-
отношение, приходим к так называемойформуле Байеса, широко исполь-
зуемой не только в теории вероятностей, но и в машинном обучении:
P(𝐴|𝐵) =P(𝐵|𝐴)P(𝐴)
P(𝐵) .
Обратите внимание, в этой формуле условные вероятности «поменялись ме-
стами». Отметим в определении часто встречающиеся названия объектов,
входящих в написанную формулу.
Определение 3.2.2Вероятность P(𝐵|𝐴) называют правдоподобием, ве-
роятности P(𝐴) и P(𝐵) – априорными вероятностями, а вероятность
P(𝐴|𝐵) – апостериорной вероятностью.
Естественно пока что остается открытым вопрос: чем нам помогают введен-
ные понятия, и при чем тут вообще машинное обучение? Рассмотрим уже
упоминавшуюся ранее жизненную и хорошо известную задачу – задачу клас-
сификации входящих писем на «спам» и «не спам» и посмотрим, что мы на
36
Высшая школа цифровой культуры Университет ИТМО
данный момент можем получить. Пример тренировочных данных представ-
лен в таблице. Упростим себе задачу, считая, что предиктор всего один и
принимает всего два значения:0 и 1, причем значение, равное1 означает,
что текст письма содержит слово «приз». Столбец «спам» – это отклик, ко-
торый тоже принимает всего два значения0 и 1, причем значение, равное1
означает, что письмо отнесено к спаму.
Текст письма «приз» «спам»
Вы выиграли приз! 1 1
Оплатите первый месяц услуг и получите приз. 1 1
Получите приз за рекомендацию банковских услуг. 1 0
Вам достался дом в наследство. 0 1
. . . . . . . . . . . .
Предположим, что всего анализировалось80 писем, а слово «приз» встре-
тилось среди пяти писем, помеченных как спам, и среди трех, помеченных
как не спам. В оставшихся72 письмах слова «приз» не было, при этом27 из
них были помечены как «спам», а остальные45 – как не «спам». Поместим
данные в такую таблицу:
«спам» не «спам»
всего писем 32 48
есть слово «приз» 5 3
нет слова «приз» 27 45
.
Итак, у нас всего32 письма, помеченных как «спам», среди которых5 со-
держат слово «приз», и48 писем, помеченных как не «спам», среди которых
3 содержат слово «приз». Интересен вопрос: как оценить вероятность, что
письмо относится к классу «спам», если оно содержит слово «приз»?
Вспомним, как на основе приведенных данных рассчитываются вероят-
ности различных событий. Конечно, в нашем случае истинные вероятности
неизвестны, но мы их можем оценить, используя частоту. Например, вероят-
ность того, что случайное письмо, попавшее в спам, содержит слово «приз»,
оценивается при помощи частоты числом5/32, ведь на основе тренировоч-
ных данных всего32 письма отнесены к категории «спам», и из них ровно5
содержат слово «приз». Таким образом, согласно собранным данным, резонно
считать, что
P(«приз» |«спам») = 5
32.
В свою очередь, вероятность того, что присланное письмо относится к ка-
тегории «спам», оценивается числом32/80, а вероятность того, что письмо
37
Высшая школа цифровой культуры Университет ИТМО
содержит слово «приз» – числом8/80, то есть
P(«спам») =32
80, P(«приз») = 8
80.
Теперь, используя формулу Байеса, мы можем найти вероятность того, что
письмо является спамом, если оно содержит слово «приз»:
P(«спам» |«приз») =P(«приз» |«спам») ·P(«спам»)
P(«приз») =
5
32 ·32
80
8
80
= 5
8 = 0.625.
Как мы видим, письмо, содержащее слово «приз», скорее стоит отнести к ка-
тегории «спам», нежели к категории не «спам». Аналогичным образом можно
рассчитать вероятности и других событий. Итак, мы вспомнили, как на осно-
вечастотрассчитыватьвероятности.Теперьможноприступитькпостроению
классификатора.
3.3 Построение модели и вывод формул
Давайте для начала обратимся непосредственно к вероятностной поста-
новке задачи. Пусть𝑋 – это множество объектов, каждый из которых описы-
вается 𝑝 признаками – случайными величинами𝑋1, 𝑋2, ..., 𝑋𝑝, и откликом𝑌
(снова отметим, что отклик принимает конечное множество значений). Будем
трактовать 𝑋 ×𝑌 как вероятностное пространство с некоторым совместным
распределением. Кроме того, для простоты изложения будем предполагать,
что все случайные величины𝑋1, 𝑋2, ..., 𝑋𝑝 имеют дискретное распреде-
ление с конечным множеством значений. Наша задача – назначить но-
вому, тестовому объекту с предикторами𝑋1, 𝑋2, ..., 𝑋𝑝, одну из меток клас-
сов из множества𝑌 . Как нам это сделать? Наверное понятно, что сначала
оказывается логичным вычислить вероятности отнесения нового объекта к
каждому из имеющихся классов, то есть набор вероятностей:
P(Класс = 𝑦|𝑋1, 𝑋2, ..., 𝑋𝑝), 𝑦 ∈𝑌.
Замечание 3.3.1Важно понять, что показывает написанное (не совсем
строго) выражение при фиксированном 𝑦 ∈ 𝑌 . Оно показывает вероят-
ность отнесения объекта с заранее неизвестным классом к классу𝑦, учи-
тывая то, что объект обладает атрибутами𝑋1, 𝑋2, ..., 𝑋𝑝 (строже – это
соотношение задает соответствующее вероятностное распределение).
Итак, набор вероятностей у нас есть. Как теперь понять к какому классу𝑦*∈
𝑌 логичнее всего отнести тестовый объект? Конечно, к наиболее вероятному
38
Высшая школа цифровой культуры Университет ИТМО
(или к одному из наиболее вероятных, если их несколько), то есть выбрать
такой 𝑦*∈𝑌 , что
𝑦*= Arg max
𝑦∈𝑌
P(Класс = 𝑦|𝑋1, 𝑋2, ..., 𝑋𝑝)
Пользуясь формулой Байеса, последнее соотношение может быть переписано
в виде
𝑦*= Arg max
𝑦∈𝑌
P(Класс = 𝑦|𝑋1, 𝑋2, ..., 𝑋𝑝) =
= Arg max
𝑦∈𝑌
P(𝑋1, 𝑋2, . . . , 𝑋𝑝|Класс = 𝑦)P(Класс = 𝑦)
P(𝑋1, 𝑋2, . . . , 𝑋𝑝) .
Так как вероятность, стоящая в знаменателе, не зависит от𝑦, то она одина-
кова для всех𝑦 и не влияет на максимизацию, а значит
𝑦*= Arg max
𝑦∈𝑌
(P(𝑋1, 𝑋2, . . . , 𝑋𝑝|Класс = 𝑦)P(Класс = 𝑦)) .
С точки зрения теории все хорошо, задача вроде бы решена, но что на прак-
тике? Проблема заключается в том, что на практике мы не знаем совместного
распределения. Понятно, что оценить вероятностиP(Класс = 𝑦) достаточно
легко – будем оценивать частоту встречаемости классов в тренировочном на-
боре данных, а вот оценить условные вероятностиP(𝑋1, 𝑋2, . . . , 𝑋𝑝|Класс =
𝑦) не получится – их слишком много. Для того, чтобы оценить их достаточно
«хорошо», нужно каждый случай пронаблюдать несколько раз – это, конеч-
но же, невозможно. Здесь и помогает заявленная в названии классификатора
«наивность».
Итак, наивный байесовский классификатор работает в предположении
условной независимости предикторов при условии данной метки
класса. Математически это означает следующее:
P(𝑋1, 𝑋2, . . . , 𝑋𝑝|Класс = 𝑦) =P (𝑋1|Класс = 𝑦) ·P (𝑋2|Класс = 𝑦) ·. . .·
P (𝑋𝑝|Класс = 𝑦) =
𝑝∏︁
𝑖=1
P (𝑋𝑖|Класс = 𝑦) .
Несмотря на такое, казалось бы, существенное ограничение, оказывается, что
байесовскийклассификатордостаточнохорошосправляетсясосвоейзадачей.
Даже больше, именно наивный байесовский классификатор до недавнего вре-
мени был основным алгоритмом классификации писем на «спам» и не «спам»
в большинстве современных почтовых сервисов.
Замечание 3.3.2Отметим, что в контексте классификации, скажем,
писем или текстов, «наивное» ограничение означает следующее: разные
39
Высшая школа цифровой культуры Университет ИТМО
слова в тексте на одну и ту же тему появляются независимо друг от
друга. Еще раз вдумайтесь, насколько это, честно говоря, далеко от дей-
ствительности.
Где же нам помогает предположение о независимости? Конечно, в оценке
условных вероятностейP(𝑋𝑖|Класс = 𝑦). Оцениваются эти вероятности как
обычно – частотой, подробнее об этом мы скажем чуть позже. В итоге, в
предположении наивности, классификатор принимает следующий вид:
𝑦*= Arg max
𝑦∈𝑌
(︃
P(Класс = 𝑦)
𝑝∏︁
𝑖=1
P (𝑋𝑖|Класс = 𝑦)
)︃
.
Это еще не все. В случае, если объем тренировочных данных велик, а веро-
ятности (или их оценки) малы, то результат перемножения компьютерными
средствами,имеющимиограниченнуюточность,приведетнаскчему-товроде
нуля. Для решения этой проблемы воспользуемся переходом к натурально-
му логарифму. Так как натуральный логарифм – возрастающая функция, то
подеедействиеммаксимумыпереходятвмаксимумы,азначиткласификатор
можно представить в следующем виде:
𝑦*= Arg max
𝑦∈𝑌
(︃
ln
(︃
P(Класс = 𝑦)
𝑝∏︁
𝑖=1
P (𝑋𝑖|Класс = 𝑦)
)︃)︃
,
или, пользуясь свойствами логарифма, в виде
𝑦*= Arg max
𝑦∈𝑌
(︃
ln P(Класс = 𝑦) +
𝑝∑︁
𝑖=1
ln P (𝑋𝑖|Класс = 𝑦)
)︃
.
Почти все, остался последний вопрос: а как же вернуться к вероятностям
отнесения к классам? Достаточно просто: нужно избавиться обратно от ло-
гарифмов и нормировать результаты. Для каждого класса𝑦 ∈𝑌 вычислим
значение
𝐹(𝑦) = lnP(Класс = 𝑦) +
𝑝∑︁
𝑖=1
ln P (𝑋𝑖|Класс = 𝑦) .
Тогда, чтобы получить вероятность отнесения объекта𝑋 к какому-то классу
𝑦*∈𝑌 , остается составить отношение следующего вида:
P(Класс = 𝑦*|𝑋1, 𝑋2, ..., 𝑋𝑝) = 𝑒𝐹(𝑦*)
∑︀
𝑦∈𝑌
𝑒𝐹(𝑦) .
40
Высшая школа цифровой культуры Университет ИТМО
Замечание 3.3.3Для сокращения числа операций с экспонентой, часто
бывает полезным следующее преобразование:
P(Класс = 𝑦*|𝑋1, 𝑋2, ..., 𝑋𝑝) = 1
𝑒−𝐹(𝑦*) ∑︀
𝑦∈𝑌
𝑒𝐹(𝑦) = 1
1 + ∑︀
𝑦∈𝑌, 𝑦̸=𝑦*
𝑒𝐹(𝑦)−𝐹(𝑦*) ,
где в последней сумме индекс пробегает все метки класса, кроме𝑦*.
Теперь мы готовы сформулировать алгоритм построения классификатора на
конкретной выборке.
3.3.1 Алгоритм построения классификатора на конкретной вы-
борке
Пусть имеется тренировочный набор данных𝑋 = (𝑥1, 𝑥2, ..., 𝑥𝑛) объема
𝑛,
𝑥𝑖 = (𝑥𝑖1, 𝑥𝑖2, ..., 𝑥𝑖𝑝), 𝑖 ∈{1, 2, ..., 𝑛},
причем каждому объекту 𝑥𝑖 соответсвует отклик 𝑦𝑖 ∈𝑌 . Пусть требуется
классифицировать новый объект𝑧 со значениями предикторов(𝑧1, 𝑧2, ..., 𝑧𝑝).
Тогда объекту𝑧 назначается любой класс из набора
Arg max
𝑦∈𝑌
(︃
ln P(Класс = 𝑦) +
𝑝∑︁
𝑖=1
ln P (𝑋𝑖|Класс = 𝑦)
)︃
,
где в качестве оценки вероятностиP(Класс = 𝑦) берется частота встречаемо-
сти соответствующего класса в тренировочном наборе данных, то есть
P(Класс = 𝑦) =
𝑛∑︀
𝑖=1
I(𝑦𝑖 = 𝑦)
𝑛 =
= количество тренировочных элементов с меткой класса𝑦
общее количество тренировочных элементов ,
а в качестве оценки вероятностиP (𝑋𝑖|Класс = 𝑦) – частота встречи значения
предиктора 𝑧𝑖 среди тренировочных данных класса𝑦, то есть
P(𝑋𝑖|Класс = 𝑦) =
𝑛∑︀
𝑘=1
I(𝑥𝑘𝑖 = 𝑧𝑖, 𝑦𝑘 = 𝑦)
𝑛∑︀
𝑘=1
I(𝑦𝑘 = 𝑦)
= количество тренировочных элементов класса𝑦, у которых𝑋𝑖 = 𝑧𝑖
количество тренировочных элементов класса𝑦 .
41
Высшая школа цифровой культуры Университет ИТМО
Замечание 3.3.4Отметим отдельно, что введенные выше оценки веро-
ятностей являются, по своей сути, оценками максимального правдоподо-
бия. Доказательство этого достаточно технично, оно может быть най-
дено в дополнительных материалах.
3.4 Пример построения классификатора
Чтобы детально разобрать описанный алгоритм, рассмотрим пример по-
строения наивного байесовского классификатора.
Предположим, что вы собрали данные о футбольных матчах, проводи-
мыхнаспортивнойплощадкерядомсвашимдомом.Вденьпроведенияматча
вы фиксировали погоду (дождливо, пасмурно или солнечно), среднюю тем-
пературу (холодно, прохладно или жарко), влажность (влажно или сухо) и
наличие ветра (либо ветер есть, либо безветрие). Данные представлены в сле-
дующей таблице:
Погода Ср. температура Влажность Наличие ветра Игра
солнечно жарко влажно нет нет
солнечно жарко влажно да нет
пасмурно жарко влажно нет да
дождливо прохладно сухо нет да
дождливо холодно сухо нет да
дождливо холодно сухо да нет
пасмурно прохладно влажно да да
пасмурно прохладно влажно нет нет
пасмурно прохладно влажно да да
.
Как, используя наивный байесовский классификатор, отнести новое наблю-
дение 𝑧 с предикторами (пасмурно, холодно, влажно, да) к одному из двух
классов?
Итак, у нас четыре предиктора:𝑋1 – погода, принимающий3 различных
значения, 𝑋2 – средняя температура, тоже с тремя различными значениями,
𝑋3 – влажность, принимающий2 различных значения и𝑋4 – наличие ветра,
тоже два различных значения, а также бинарный отклик𝑌 – игра: либо со-
стоялась, либо нет. Новому объекту𝑧, согласно алгоритму, нужно присвоить
любой класс из набора
Arg max
𝑦∈𝑌
(︃
ln P(Класс = 𝑦) +
4∑︁
𝑖=1
ln P (𝑋𝑖|Класс = 𝑦)
)︃
.
Рассмотрим подробно вычисления для случая, когда игра состоялась, то есть
для класса «да». Вычислим для начала
P(Класс = да) =количество тренировочных элементов с меткой класса «да»
общее количество тренировочных элементов .
42
Высшая школа цифровой культуры Университет ИТМО
Легко понять, что у нас всего9 тренировочных данных, среди которых лишь
5 имеют отклик «да», значит
P(Класс = да) =5
9.
Теперь вычислим
P(𝑋1|Класс = да) =P(Погода|Класс = да).
Так как у тестового объекта значение предиктора𝑋1 – погода – это пасмур-
но, то нам, согласно алгоритму, нужно разделить количество тренировочных
объектов, имеющих отклик «да» и значение предиктора погода – пасмур-
но, на общее количество тренировочных объектов класса «да». Тем самым,
получаем
P(𝑋1|Класс = да) =P(Погода = пасмурно|Класс = да) =3
5.
Аналогично, так как у тестового объекта значение предиктора𝑋2 – средняя
температура – это холодно, то нам, согласно алгоритму, нужно разделить
количество тренировочных объектов, имеющих отклик «да» и значение пре-
дикторасредняятемпература–холодно,наобщееколичествотренировочных
объектов класса «да». Тем самым, получаем
P(𝑋2|Класс = да) =P(Ср. температура= холодно|Класс = да) =1
5.
Продолжая аналогичные рассуждения, получаем
P(𝑋3|Класс = да) =P(Влажность = влажно|Класс = да) =3
5
и
P(𝑋4|Класс = да) =P(Наличие ветра= да|Класс = да) =2
5.
Итого, для класса «да» значение исследуемого выражения равно
ln 5
9 + ln3
5 + ln1
5 + ln3
5 + ln2
5 ≈−4.135.
Аналогичные вычисления для класса «нет» приводят к значению
ln 4
9 + ln1
4 + ln1
4 + ln3
4 + ln2
4 ≈−4.564.
Так как первое из полученных выражений больше, чем втрое, то класс, к
которому следует отнести тестовое наблюдение – класс «да». Значит, игра в
43
Высшая школа цифровой культуры Университет ИТМО
случае пасмурной, влажной, ветреной погоды и холодной средней темпера-
туры скорее состоится, нежели нет. Итак, вопрос классификации решен. Но
насколько уверен наш классификатор в своем решении? Давайте выясним
это. Итак, в обозначениях из лекции,
𝐹(да) ≈−4.135, 𝐹 (нет) ≈−4.564,
тогда
P(Класс = да|𝑋1, 𝑋2, 𝑋3, 𝑋4) ≈ 1
1 +𝑒−4.564+4.135 ≈0.606
и, тем самым,
P(Класс = нет|𝑋1, 𝑋2, 𝑋3, 𝑋4) = 1−P(Класс = да|𝑋1, 𝑋2, 𝑋3, 𝑋4) ≈0.394.
Видно,чтоклассификаторнеочень-тоуверенноотноситнаблюдениекклассу
«да».
3.5 Классификация писем. Сглаживание по Ла-
пласу
Итак, до сих пор мы считали, что количество предикторов у любого
объекта заранее известно. Такая ситуация, однако, бывает не всегда. Давайте
вернемся к задаче классификации писем на спам и не спам и рассмотрим ее
подробнее. Что выступает в качестве предикторов в письме? Понятно, что
слова. Но разве мы можем изначально сказать, сколько слов будет в том или
ином письме? Вряд ли.
Наверное разумнее всего предположить, что количество предикторов в
каждом письме равно количеству входящих в него слов. Значения же все
предикторы принимают из набора𝑉 – некоторого словаря слов, сформиро-
ванного на основе тренировочных данных. Итак, словарь представляет из
себя большую таблицу с тремя колонками: первая колонка отвечает за само
слово, вторая – за количество вхождений этого слова в письма, отнесенные к
категории «спам», а третья – за количество вхождений этого слова в письма,
отнесенные к категории «не спам».
Сразурассмотримпример.Нашимитренировочнымиданнымибудуттри
письма с текстом:
• Win a million rubles – «спам»;
• Ruble drops again – «не спам»;
• A million ways to get rich – «спам».
44
Высшая школа цифровой культуры Университет ИТМО
Слово (X) «спам» «не спам»
win 1 0
million 2 0
ruble 1 1
again 0 1
drops 0 1
ways 1 0
get 1 0
rich 1 0
Сформируем упомянутый словарь𝑉 . Отметим, что на практике приме-
няются методы обработки языка, поэтому слова rubles и ruble будем отож-
дествлять, а частицы и артикли будем опускать. Получим следующую таб-
лицу.
Легко оценить вероятности встретить какое-то слово в зависимости от
того, попало письмо в «спам» или нет. Делается это, как и раньше, например:
P(𝑋 = win|Класс = спам) =1
7,
так как слово win встретилось в спам-письмах один раз, а всего в спам-
письмах встретилось 7 слов (с учетом повторений). К сожалению, такая оцен-
ка может привести и к довольно странным результатам. Например,
P(𝑋 = win|Класс = не спам) = 0,
а это значит, что письмо, содержащее слово win, никогда не будет отнесено к
категории «не спам», даже если все остальные слова в письме – слова again
и drops – явные индикаторы того, что письмо спамом не является. Выход –
применить так называемое сглаживание по Лапласу – предположить, что мы
видели каждое слово на один раз больше. Тогда
P(𝑋|Класс = 𝑦) =1 +количество слов𝑋 в классе𝑦
|𝑉 |+ количество слов в классе𝑦 , 𝑦 ∈{спам, не спам}
где |𝑉 |– количество слов в словаре. В этом случае
P(𝑋 = win|Класс = спам) =1 + 1
8 + 7= 2
15,
P(𝑋 = win|Класс = не спам) =0 + 1
8 + 3= 1
11.
Как классифицировать письмо с текстом «Win a ruble to get rich»? Ровно так,
как это делалось и раньше. Вычислим для начала𝐹(спам):
𝐹(спам) = lnP(спам) + lnP(𝑋 = win|Класс = спам)+
45
Высшая школа цифровой культуры Университет ИТМО
+ lnP(𝑋 = ruble|Класс = спам) + lnP(𝑋 = get|Класс = спам)+
+ lnP(𝑋 = rich|Класс = спам) =
= ln2
3 + ln 2
15 + ln 2
15 + ln 2
15 + ln 2
15 ≈−8.465.
Теперь вычислим𝐹(не спам):
𝐹(не спам) = lnP(не спам) + lnP(𝑋 = win|Класс = не спам)+
+ lnP(𝑋 = ruble|Класс = не спам) + lnP(𝑋 = get|Класс = не спам)+
+ lnP(𝑋 = rich|Класс = не спам) =
= ln1
3 + ln 1
11 + ln 2
11 + ln 1
11 + ln 1
11 ≈−9.997.
В итоге, письмо следует отнести к классу «спам».
Остался последний момент. А что, если в письме встретилось слово, ко-
торого нет в словаре? Есть два варианта: либо это слово можно выкинуть
из рассмотрения, то есть пропустить, либо в момент классификации «пере-
учить» построенный классификатор, используя снова сглаживание по Лапла-
су:
P(𝑋|Класс = 𝑦) = 1 +количество слов𝑋 в классе𝑦
|𝑉 |+ 𝑟 + количество слов в классе𝑦, 𝑦 ∈{спам, не спам},
где 𝑟 – количество слов в письме, которых нет в словаре.
Для примера, классифицируем письмо с текстом «Get rich with ruble».
Мы видим, что слова with нет в нашем словаре. Применим сглаживание по
Лапласу, учитывая, что𝑟 = 1, тогда
𝐹(спам) = lnP(спам) + lnP(𝑋 = get|Класс = спам)+
+ lnP(𝑋 = rich|Класс = спам) + lnP(𝑋 = with|Класс = спам)+
+ lnP(𝑋 = ruble|Класс = спам) =
= ln2
3 + ln 2
16 + ln 2
16 + ln 1
16 + ln 2
16 ≈−9.416.
Теперь вычислим𝐹(не спам):
𝐹(не спам) = lnP(не спам) + lnP(𝑋 = get|Класс = не спам)+
+ lnP(𝑋 = rich|Класс = не спам) + lnP(𝑋 = with|Класс = не спам)+
+ lnP(𝑋 = ruble|Класс = не спам) =
= ln1
3 + ln 1
12 + ln 1
12 + ln 1
12 + ln 2
12 ≈−10.345.
Значит, письмо следует отнести к категории «спам».
46
Высшая школа цифровой культуры Университет ИТМО
4 Заключение
Итак, в этой лекции мы начали изучать подходы к решению задачи клас-
сификации, изучив метрические классификаторы k-NN и взвешенный k-NN,
а также вероятностный наивный байесовский классификатор. Кроме того,
мы поняли, как важно правильно делить набор данных на тестовый и трени-
ровочный, как оценивать модель, а также что знание расстояний – это еще не
все. В дальнейшем мы изучим и другие методы и подходы к классификации,
ну а пока что все. До новых встреч!
47
