!адача кластери,ации
Высша& Школа Цифровой Кул3туры
Университет ИТМО
dc@itmo.ru
Содер&ание
1 Введение 2
1.1 Мотивировка . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Виды кластеров и мно>ественност3 алгоритмов . . . . . . . . . 4
2 Метод К-средних 7
2.1 Предварител3ные сведени& и выбор рассто&ни& . . . . . . . . . 7
2.2 Описание алгоритма . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 Пример: хруст и сладост3 продуктов . . . . . . . . . . . . . . . . 15
2.4 РаEличные способы начал3ной инициалиEации. Сходимост3 ме-
тода . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.5 Выбор числа K. Камениста& осып3 . . . . . . . . . . . . . . . . . 25
3 Агломеративна3 кластери5аци3 26
3.1 Небол3ша& мотивировка . . . . . . . . . . . . . . . . . . . . . . . 26
3.2 Описание алгоритма. Способы иEмерени& рассто&ни& ме>ду
кластерами . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.3 Пример: хруст и сладост3 продуктов. Дендрограмма . . . . . . 33
3.4 Камениста& осып3 и определение числа кластеров . . . . . . . . 38
4 DBSCAN 42
4.1 Небол3ша& мотивировка и описание . . . . . . . . . . . . . . . . 42
4.2 Основные определени& и описание алгоритма . . . . . . . . . . . 42
5 7акл8чение 47
Высша& школа цифровой кул3туры Университет ИТМО
1 Введение
1.1 Мотивировка
Kдравствуйте, ува>аемые слушатели! Сегодн& лекци& будет посв&ще-
на одному иE направлений «Обучени& беE учител&» – Eадаче кластериEации.
Начнем >е мы, конечно, с того, что попробуем вы&снит3: а в чем EаклOчаетс&
Eадача кластериEации и Eачем ее решат3?
Давайте обP&сним сут3 на примере. Представим, что мы отправилис3 Eа
покупками в какой-нибуд3 гипермаркет. В гипермаркете, как иEвестно, про-
даетс& мно>ество раEличных товаров: там мо>но найти и фрукты, и овощи,
и м&со, и молочнуO продукциO, бытовуO химиO, а кое-где да>е чайники,
щетки стеклоочистителей, гладил3ные доски и другие полеEные вещи. В то
>е врем&, все эти товары вр&д ли ле>ат кучей пр&мо перед входом, не так ли?
Товары раEделены на какие-то группы или отделы (о чем часто свидетел3-
ствуOт вывески под потолком вдол3 р&да полок), причем, в Eависимости от
гипермаркета, как сами группы, так и их количество, могут быт3 раEными.
Более того, на территории магаEина группы могут быт3 располо>ены по-
раEному друг относител3но друга. Решение относител3но группировки това-
ров и располо>ени& этих групп в магаEине принимает отдел мерчендайEинга.
Но иE каких сообра>ений?
Во-первых, раEделение на группы дол>но быт3 интуитивно пон&тно по-
купателO: вр&д ли кто-то будет искат3 молоко на соседней полке с куриными
грудками (хот& такуO группу и мо>но наEват3, например, «продукты >ивот-
ного происхо>дени&», но неу>ели это сраEу приходит на ум?). Во-вторых, ко-
личество групп не дол>но быт3 слишком бол3шим: отдел3ные группы «&бло-
ки», «груши», «персики» находит3 будет куда сло>нее, чем бол3шуO группу
«фрукты» (или да>е «фрукты и овощи»); совершенно &сно, что внутри этой
укрупненной группы мы и сами быстро поймем, где ле>ат персики, а где гру-
ши, и выберем то, что нам по душе. Кстати, мо>ет быт3 мы и вовсе внеEапно
(!) предпочтем апел3сины, так как именно сегодн& они у> бол3но хорошо
выгл&д&т на прилавке.
Последнее наблOдение мо>ет подводит3 и к мысли, что располо>ение
групп товаров в магаEине (хот& это и нескол3ко друга& Eадача) – немалова>-
на& детал3. Например, почти всегда торты и конфеты располо>ены р&дом с
отделом кофе и ча&: лOди, покупа& чай, достаточно часто прот&гиваOт руку
на соседнOO полку, чтобы купит3 что-то «сладен3кое». Если бы конфет р&-
дом не было, то соблаEн был бы менее велик, конфет покупалос3 бы мен3ше,
а Eначит и магаEин нес бы потери в прибыли.
Ну а при чем тут кластериEаци&? Да вот при чем. В приведенном приме-
ре товары – это обPекты, подле>ащие кластериEации, а группы или отделы
2
Высша& школа цифровой кул3туры Университет ИТМО
– это кластеры, на которые они раEбиты. Kаранее как количество кластеров,
так и их состав неиEвестны, однако хочетс& верит3 в то, что наиболее блиEкие
по типу товары дол>ны находит3с& как мо>но бли>е друг к другу (обра-
Eовыват3 группу, кластер). Кроме того, чем бол3ше обPекты раEличаOтс&,
тем дал3ше друг от друга они дол>ны находит3с&, впрочем, как и кластеры,
которым они принадле>ат, хот& это и не всегда так, но об этом чут3 поE>е.
Давайте тепер3 ответим на вопрос: чем >е все-таки обучение с учителем
отличаетс& от обучени& беE учител&? Мы видим, что в отличие от Eадач вет-
ки обучени& с учителем, в рассмотренном примере нам не даны правил3ные
ответы, наEываемые нами откликами. Это и &вл&етс& глобал3ным отличием
кластериEации от классификации: при решении Eадачи кластериEации набор
классов (точнее – кластеров) иEначал3но неиEвестен, он дол>ен быт3 опре-
делен либо алгоритмом, либо, что чаще, исследователем.
Подыто>ив, мо>но скаEат3, что Eадача кластериEации – это Eадача раE-
делени& мно>ества обPектов на группы, внутри которых наход&тс& похо>ие
обPекты. У>е иE приведенного примера &сно, что эта Eадача далеко не все-
гда имеет очевидное (и единственное) решение. Посмотрите, например, на
рисунок 1. На какое количество кластеров вы бы раEбили представленные
обPекты? На два, на четыре, на шест3 или на какое-то другое количество?
Конечно, правил3ного ответа на поставленный вопрос не существует. Опре-
деление количества кластеров – это сло>на& Eадача, котора& часто требует
вовлечени& экспертного мнени& дл& правил3ной интерпретации полученных
реEул3татов.
РаEобравшис3 с концепцией Eадачи кластериEации, полеEно пон&т3 и сле-
дуOщее: продукты в магаEине, точки на плоскости – это обPекты, характери-
стики которых нам часто видны и пон&тны. На практике >е обPекты могут
характериEоват3с& бол3шим количеством приEнаков-предикторов и не под-
дават3с& виEуалиEации. Именно поэтому работу на раEделение обPектов на
кластеры и отдаOт на откуп машинам, а аналитику остаетс& не менее интри-
гуOща& Eадача: проинтерпретироват3 полученное раEбиение.
Что >е, давайте тепер3 обратимс& к вопросу о том, как машины раEби-
раOт предло>енные данные на похо>ие и непохо>ие.
3
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 1: НеодноEначност3 кластериEации
1.2 Виды кластеров и мно&ественност3 алгорит-
мов
Наверное, у вас у>е сформировалос3 интуитивное представление о Eа-
даче кластериEации: насыпанные каким-то обраEом точки ну>но раEделит3
на кучки, внутри которых точки «блиEки» друг к другу. Пон&тно, что така&
вол3на& (и не совсем точна&) постановка Eадачи дает волO фантаEии, имен-
но поэтому и способов решени& Eадачи кластериEации довол3но много. На
рисунке 2 представлено шест3 раEличных конфигураций «насыпанных» то-
чек и реEул3таты работы алгоритмов по раEделениO их на кластеры (раEные
цвета точек отвечаOт раEным кластерам). Смотрите, наскол3ко по-раEному
алгоритмы справл&Oтс& с одной и той >е Eадачей.
Давайте тепер3 проаналиEируем полученные реEул3таты. Наверное, ин-
туитивное представление о кластерах и «блиEости» внутри этих кластеров
до этого момента бол3ше всего походило на картину, соответствуOщуO п&-
той конфигурации. Kдес3 кластеры представл&Oт собой, как говор&т «друг
от друга отделенные плотные шаровые сгустки», и, как легко видет3, все
представленные алгоритмы хорошо справл&Oтс& с Eадачей. С точки Eрени&
математики представленна& ситуаци& характериEуетс& так: рассто&ние ме>-
ду обPектами внутри кластера, как правило, много мен3ше, чем рассто&ние
ме>ду кластерами. Мо>но смело утвер>дат3, что на насто&щий момент кла-
стериEаци& отделенных друг от друга плотных шаровых сгустков не пред-
4
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 2: Сравнение алгоритмов кластериEации
ставл&ет труда, бол3шинство алгоритмов справл&Oтс& с этой Eадачей легко
и просто, а главное – качественно.
Тепер3 посмотрите на трет3O конфигурациO. КаEалос3 бы, Eдес3 то>е
ест3 шаровые сгустки, и да>е плотные, однако видно, что тепер3 они «пло-
хо раEделены» – ме>ду ними ест3 так наEываемые перемычки. Видно, что
эти перемычки начинаOт мешат3 некоторым алгоритмам. Дл& бол3шей на-
гл&дности мо>но привести и такуO иллOстрациO дл& описываемой ситуа-
ции, см. рисунок 3. Перемычки ме>ду кластерами &вл&Oтс& чем-то вроде
мостов ме>ду островами и сил3но усло>н&Oт работу алгоритмов. С точки
Eрени& математики така& ситуаци& характериEуетс& так: рассто&ние ме>ду
обPектами внутри кластера сопоставимо с рассто&нием ме>ду кластерами.
Впрочем, многие алгоритмы кластериEации умеOт справл&т3с& с подобными
ситуаци&ми.
Тепер3 обратимс& к первой, второй и четвертой конфигураци&м. А
что тут? Перед нами &вное опровер>ение интуитивным (и, честно говор&,
нескол3ко наивным) представлени&м о том, что кластеры – это шаровые
сгустки. Несмотр& на то, что обPекты, согласно нашему виEуал3ному пред-
ставлениO, легко дел&тс& на кластеры с четкой границей, алгоритмам это
раEделение совершенно неочевидно. Кластеры такой конфигурации наEы-
ваOтс& ленточными. По своей сути, они представл&Oт собой многообраEи&
5
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 3: Кластеры и перемычки ме>ду ними
мен3шей раEмерности, чем пространство, в котором они располо>ены: обP-
екты внутри одного кластера могут быт3 располо>ены вес3ма далеко друг
от друга, и это рассто&ние мо>ет быт3 весомо бол3ше, чем рассто&ние ме>ду
кластерами. Современные алгоритмы умеOт находит3 ленточные кластеры,
хот& это и не очен3 просто, об этом мы поговорим чут3 поE>е.
Наконец, а что с последней конфигурацией? Наверное, мо>но скаEат3,
что все обPекты принадле>ат одному кластеру, а мо>ет быт3 раEумнее ска-
Eат3 и следуOщее: рассматриваемые приEнаки обPектов не даOт никакой по-
леEной дл& кластериEации информации – кластеров просто нет. В этом при-
мере особенно интересно, что многие алгоритмы все-таки пытаOтс& эту массу
раEделит3 на какие-то части. Что >е, это их право.
На практике случаOтс& и другие ситуации, не нашедшие отра>ени& в
рассматриваемом примере. Например, кластеры могут быт3 окру>ены неко-
торым фоном-шумом, рисунок 4, накладыват3с& друг на друга, рисунок 5,
или вообще обраEовыват3с& вовсе не по сходству, а по каким-то другим типам
Eависимостей, рисунок 6, могут быт3 и другие ситуации.
Рис. 4: Кластеры на фоне шума
6
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 5: ПересекаOщиес& кластеры
Рис. 6: Друга& Eависимост3 «похо>ести»
Именно иE-Eа такого раEнообраEи& воEмо>ных ситуаций, выбор ме-
тода кластериEации – чут3 ли не основна& Eадача, котора& стоит перед
аналитиком-исследователем. Почему не основна&? Потому что еще ну>но
определит3с& с тем, как иEмер&т3 рассто&ни& ме>ду обPектами и ме>ду
кластерами. Но обо всем по пор&дку. Итак, первый метод, который мы будем
рассматриват3 – это метод K-средних (K-means).
2 Метод К-средних
2.1 Предварител3ные сведени8 и выбор рассто-
8ни8
Перед тем как начат3 обсу>дение алгоритма K-средних, дадим фор-
мал3ное определение пон&тиO кластера.
Определение 2.1.1 Пуст% X ∕= ∅ – мно)ество всех рассматриваемых
об2ектов. Непустые мно)ества C1, C2, . . . , CK ⊂ X на6ыва7тс8 класте-
рами, если их об2единение совпадает с X, а пересечение л7бых двух и6 них
(конечно, с ра6личными индексами) ест% пустое мно)ество, то ест%
X =
K!
i=1
Ci, причем Ci ∩ Ci′ = ∅ при i ∕= i′, i, i′ ∈ {1, 2, ..., K}.
7
Высша& школа цифровой кул3туры Университет ИТМО
7амечание 2.1.1 В математике семейство мно)еств C1, C2, . . . , CK и6
предыдущего определени8 на6ыва7т ра6биением мно)ества X.
Классически будем считат3, что у нас имеетс& набор данных X =
(x1, x2, ..., xn) обPема n с числовыми приEнаками, где
xi = (xi1, xi2, ..., xip), i ∈ {1, 2, ..., n},
и этот набор данных нам и требуетс& раEбит3 на кластеры. На первый вEгл&д
Eадача распределени& n обPектов по K кластерам выгл&дит не очен3 простой,
особенно если K и n достаточно велики, вед3 существует Kn вариантов по-
строени& искомого раEбиени&. Перебор всех воEмо>ных вариантов в поисках
«оптимал3ного», конечно, не самое раEумное, а EачастуO и вычислител3но
невоEмо>ное решение. Да и что вообще Eначит – «оптимал3ный вариант»?
Интуици& подскаEывает нам, что кластериEаци& тем лучше, чем более
«похо>и» обPекты в рамках одного кластера. Похо>ест3, видимо, эквива-
лентна блиEости обPектов с точки Eрени& рассто&ни& ме>ду ними. Если
вспомнит3 случай отделенных плотных шаровых сгустков, о которых мы го-
ворили ранее, то мо>но сформулироват3 следуOщуO эмпирическуO харак-
теристику «оптимал3ности»: в случае «оптимал3ной» кластериEации рассто-
&ние ме>ду лOбыми двум& обPектами внутри одного кластера невелико по
сравнениO с рассто&нием ме>ду обPектами, наход&щимис& в двух раEных
кластерах. Kначит, мо>но прийти к следуOщему умоEаклOчениO: раEумно
считат3, что кластериEаци& проведена удачно, если среднее Eначение попар-
ных рассто&ний ме>ду обPектами ка>дого кластера мало. Вот он и критерий!
7амечание 2.1.2 Еще ра6 обратим ваше внимание на то, что о6вученный
«критерий» – это не рецепт на все случаи )и6ни. Это – вол%но сформу-
лированный принцип довол%ствовани8 бли6ост%7 в алгоритме K-средних.
Вспомина8 ре6ул%таты работы алгоритмов, и6обра)енные на рисунке 2,
видно, что, в частности, когда сгустки плохо отделены, такой критерий
у)е работает не очен% хорошо, а в случае ленточных кластеров или шума
– не работает вовсе. Поэтому област% применени8 алгоритма K-средних
не очен% велика: она ограничиваетс8 вес%ма )есткими предполо)ени8ми
на форму и располо)ение кластеров – плотные хорошо ра6деленные шаро-
вые сгустки.
Итак, раEобравшис3 с идейной стороной вопроса, перейдем к формал3-
ным определени&м.
Определение 2.1.2 Пуст% X = {x1, x2, ..., xn} – мно)ество рассматри-
ваемых об2ектов, C1, C2, ..., CK – ра6биение X, d – функци8 рассто8ни8,
6аданна8 на X.
8
Высша& школа цифровой кул3туры Университет ИТМО
Внутрикластерным рассто8нием W(Ck) в кластере Ck на6ыва7т сум-
му попарных рассто8ний ме)ду всеми об2ектами этого кластера, то ест%
W (Ck) =
"
xi,xi′ ∈Ck
d (xi, xi′ ) , k ∈ {1, 2, ..., K}.
Тепер3 совершенно &сно, как ввести пон&тие среднего внутрикластерного рас-
сто&ни&.
Определение 2.1.3 В обо6начени8х предыдущего определени8, средним
внутрикластерным рассто8нием в кластере Ck на6ываетс8 величина, рав-
на8
W (Ck)
|Ck| , k ∈ {1, 2, ..., K},
где |Ck| – количество об2ектов, принадле)ащих кластеру Ck.
Итак, чтобы минимиEироват3 средние внутрикластерные рассто&ни&, мо>но
попробоват3 минимиEироват3 их сумму по всем кластерам, то ест3 миними-
Eироват3 следуOщее выра>ение:
K"
k=1
W (Ck)
|Ck| =
K"
k=1
1
|Ck|
"
xi,xi′ ∈Ck
d (xi, xi′ ) .
Осталос3 определит3с& с функцией рассто&ни&. В качестве последней мы бу-
дем испол3Eоват3 квадрат евклидова рассто&ни&.
7амечание 2.1.3 Напомним, что евклидово рассто8ние ме)ду об2ектами
xi = (xi1, xi2, . . . , xip) и xi′ = (xi′1, xi′2, . . . , xi′p)
определ8етс8 следу7щим обра6ом:
dE (xi, xi′ ) =
#$$%
p"
j=1
(xij − xi′j)2.
Квадрат евклидова рассто8ни8 вычисл8етс8, как
d2
E (xi, xi′ ) =
p"
j=1
(xij − xi′j)2 .
При таком выборе функции рассто&ни& блиEкие обPекты (евклидово рассто-
&ние ме>ду которыми мен3ше единицы) внос&т мен3ший вклад в миними-
EируемуO сумму, а далекие (евклидово рассто&ние ме>ду которыми бол3ше
9
Высша& школа цифровой кул3туры Университет ИТМО
единицы) – бол3ший. Само минимиEируемое выра>ение выгл&дит следуO-
щим обраEом:
K"
k=1
1
|Ck|
"
xi,xi′ ∈Ck
d2
E (xi, xi′ ) .
7амечание 2.1.4 Обратим внимание на один технический момент. От-
сле)иват% и6менение 6начени8 6а8вленной миними6ируемой функции
K"
k=1
1
|Ck|
"
xi,xi′ ∈Ck
d2
E (xi, xi′ )
достаточно сло)но, так как при и6менении принадле)ности об2екта кла-
стеру во внешней сумме мен8етс8 не тол%ко числител% дроби, но и 6на-
менател%, причем сра6у у двух слагаемых: у слагаемого, отвеча7щего кла-
стеру и6 которого ушел об2ект, и у слагаемого, отвеча7щего кластеру, в
который пришел об2ект.
Снова обраща&с3 к геометрическим сообра>ени&м (шаровые сгустки) стано-
витс& пон&тно, что, скорее всего, полеEной характеристикой кластера &вл&-
етс& точка, &вл&Oща&с& центром масс обPектов, принадле>ащих кластеру, –
так наEываемый центроид кластера.
Определение 2.1.4 Центроидом xk = (xk1, . . . , xkp) кластера Ck на6ыва-
етс8 об2ект, координаты которого вычисл87тс8 следу7щим обра6ом:
xkj = 1
|Ck|
"
xi∈Ck
xij, j = {1, 2, . . . , p}.
ОкаEываетс&, справедлива следуOща& лемма: при испол3Eовании в качестве
функции рассто&ни& квадрата евклидова рассто&ни&, среднее внутрикластер-
ное рассто&ние в кластере Ck равно удвоенной сумме квадратов рассто&ний
от обPектов кластера Ck до центроида этого кластера xk.
Лемма 2.1.1 В рамках обо6начений, введенных ранее, справедливо равен-
ство: 1
|Ck|
"
xi,xi′ ∈Ck
d2
E(xi, xi′ ) = 2
"
xi∈Ck
d2
E(xi, xk).
Дока5ател>ство. Дл& простоты и нагл&дности обоEначений предполо>им,
что кластер Ck содер>ит n обPектов x1, x2, ..., xn,
xi = (xi1, xi2, ..., xip), i ∈ {1, 2, ..., n},
10
Высша& школа цифровой кул3туры Университет ИТМО
тогда
1
|Ck|
"
xi,xi′ ∈Ck
d2
E(xi, xi′ ) = 1
n
n"
i,j=1
d2
E(xi, xj) = 1
n
n"
i,j=1
p"
t=1
(xit − xjt)2.
Кроме того,
2
"
xi∈Ck
d2
E(xi, xk) = 2
n"
i=1
p"
t=1
(xit − xkt)2.
Итого, ну>но докаEат3, что
1
n
n"
i,j=1
p"
t=1
(xit − xjt)2 = 2
n"
i=1
p"
t=1
(xit − xkt)2.
Пуст3 t ∈ {1, 2, ..., p}, справедлива цепочка преобраEований:
1
n
n"
i,j=1
(xit −xjt)2 = 1
n
n"
i,j=1
(x2
it −2xitxjt + x2
jt) =
n"
i=1
x2
it − 2
n
n"
i,j=1
xitxjt +
n"
j=1
x2
jt =
= 2
&
'
n"
i=1
x2
it − 1
n
( n"
i=1
xit
)2*
+ = 2n
&
'1
n
n"
i=1
x2
it −
(
1
n
n"
i=1
xit
)2*
+.
Kаметим, что последнее выра>ение – это умно>енна& на 2n выборочна& дис-
перси& набора данных Xt = (x1t, x2t, ..., xnt) – t-тых координат исходного на-
бора данных. Kначит, в стандартных обоEначени&х,
2n
&
'1
n
n"
i=1
x2
it −
(
1
n
n"
i=1
xit
)2*
+ = 2n
,
X2
t − Xt
2-
= 2n · S2(Xt).
Пол3Eу&с3 определением выборочной дисперсии, получим
2n · S2(Xt) = 2n · 1
n
n"
i=1
(xit − xkt)2 = 2
n"
i=1
(xit − xkt)2.
Итого, при t ∈ {1, 2, ..., p} справедливо равенство
1
n
n"
i,j=1
(xit − xjt)2 = 2
n"
i=1
(xit − xkt)2.
Дл& Eавершени& докаEател3ства осталос3 просуммироват3 эти равенства по
t. □
11
Высша& школа цифровой кул3туры Университет ИТМО
Итого, испол3Eу& лемму, сумма средних внутрикластерных рассто&ний пере-
писываетс& в виде
K"
k=1
1
|Ck|
"
xi,xi′ ∈Ck
d2
E(xi, xi′ ) = 2
K"
k=1
"
xi∈Ck
d2
E(xi, xk) = 2
K"
k=1
"
xi∈Ck
p"
j=1
(xij − xkj)2 .
Kначит, минимиEаци& первого выра>ени& эквивалентна минимиEации по-
следнего. Так как двойка, сто&ща& перед суммой, не Eависит от обPектов
и не вли&ет на минимиEациO, впред3 мы будем рассматриват3 (и минимиEи-
роват3) лиш3 следуOщее выра>ение:
K"
k=1
"
xi∈Ck
p"
j=1
(xij − xkj)2 .
Итого, Eадача минимиEации суммы средних внутрикластерных «рассто&ний»
свелас3 к Eадаче минимиEации суммы по всем кластерам квадратов рассто-
&ний от обPектов кластера до центроида соответствуOщего кластера. Цент-
роид на ка>дой итерации &вл&етс& чем-то вроде инварианта, относител3но
которого очен3 удобно следит3 Eа процессом кластериEации. Что >е, тепер3
мы готовы сформулироват3 способ кластериEации методом K-средних.
2.2 Описание алгоритма
Итак, пришло врем& сформулироват3 алгоритм применени& метода K-
средних. Пуст3 имеетс& набор данных X = (x1, x2, ..., xn) обPема n с число-
выми приEнаками, где
xi = (xi1, xi2, ..., xip), i ∈ {1, 2, ..., n}.
1. Выбираетс& число K ∈ N.
2. Инициали5аци3. Ка>дый обPект случайным обраEом относ&т к
какому-то кластеру иE набора C1, C2, ..., CK.
3. НахоAдение центроидов. Дл& ка>дого кластера Ck наход&т коор-
динаты центроида:
xk = (xk1, xk2, ..., xkp), xkj = 1
|Ck|
"
xi∈Ck
xij,
где k ∈ {1, 2, . . . , K}, j ∈ {1, 2, . . . , p}.
12
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 7: ИнициалиEаци&
4. Вычисление квадратов рассто3ний до центроидов. Наход&т
квадрат евклидова рассто&ни& от i-го обPекта до центроида ка>дого
кластера:
d2
E(xi, xk) =
p"
j=1
(xij − xkj)2 ,
k ∈ {1, 2, . . . , K}, i ∈ {1, 2, . . . , n}.
5. Перераспределение. ОбPект xi относ&т к кластеру с наиболее блиE-
ким к нему центроидом, то ест3 xi ∈ Ck∗ , где k∗ – лOбое иE решений
Eадачи
Arg min
k∈{1,2,...,K}
d2
E(xi, xk).
Отметим, что если среди решений поставленной Eадачи ест3 номер теку-
щего кластера, то обPект не мен&ет своей кластерной принадле>ности.
6. Шаги 2 – 6 повтор&Oтс&, пока обPекты не перестанут перераспреде-
л&т3с& по кластерам
На рисунке 7 слева представлены исходные данные: ка>дый обPект обла-
дает двум& числовыми предикторами, число K иE виEуал3ных сообра>ений
13
Высша& школа цифровой кул3туры Университет ИТМО
выбрано равным п&ти. На том >е рисунке справа сначала проиEведена иници-
алиEаци&: ка>дый обPект случайным обраEом отнесен к одному иE п&ти воE-
мо>ных кластеров (раEным кластерам соответствуOт раEные цвета), а Eатем
вычислены (и нарисованы) центроиды ка>дого иE кластеров – им отвечаOт
>ирные красные точки в центре картинки. На рисунке 8 видно располо>ение
центроидов и принадле>ност3 обPектов к кластерам после первой, второй и
так далее до шестой итераций. Скорее всего, финал3ный вариант кластери-
Eации никого не удивил – он был о>идаем с самого начала.
7амечание 2.2.1 Отметим ва)ное 6амечание. Так как алгоритм К-
средних испол%6ует дл8 определени8 бли6ости пон8тие рассто8ни8, то
при6наки кластери6уемых об2ектов перед началом кластери6ации имеет
смысл либо стандарти6ироват%, либо нормироват%.
Рис. 8: Итерации кластериEации
7амечание 2.2.2 Отметим (пока лиш% на словах), что описанный алго-
ритм сходитс8 (то ест% существует шаг, после которого об2екты пере-
ста7т перераспредел8т%с8 по кластерам). Почему? Давайте посмотрим.
На этапе перераспределени8 об2ектов по новым кластерам выра)ение
K"
k=1
"
xi∈Ck
p"
j=1
(xij − xkj)2 ,
14
Высша& школа цифровой кул3туры Университет ИТМО
которое мы миними6ируем, не увеличиваетс8, так как об2екты, если и ме-
н87т кластер, переход8т в тот, центроид которого бли)е. В итоге квад-
рат рассто8ни8 до старого кластера (бол%шее «рассто8ние») мен8етс8 на
квадрат рассто8ни8 до нового кластера (мен%шее «рассто8ние»). Нахо)-
дение новых центроидов на ка)дой итерации, в сво7 очеред%, как минимум
не увеличивает сумму квадратов рассто8ний до центроида внутри ка)до-
го кластера.
Определение 2.2.1 Ситуаци8, когда алгоритм сошелс8, то ест% когда
об2екты перестали мен8т% сво7 кластерну7 принадле)ност%, на6ывает-
с8 локал%ным оптимумом.
Почему локал3ным, спросите вы? Дело в том, что реEул3тат кластериEации
методом K-средних очен3 чувствителен к начал3ной инициалиEации (иными
словами – к выбору начал3ных центроидов). Мен&& начал3нуO инициалиEа-
циO, вообще говор&, мен&етс& и конечна& кластериEаци&. Продемонстрируем
это на синтетическом примере. Рассмотрим один и тот >е набор данных.
Ка>дый обPект обладает двум& атрибутами и мо>ет быт3 иEобра>ен на
координатной плоскости. Проведем кластериEациO, испол3Eу& приведенный
алгоритм, распредел&& обPекты на K = 5 кластеров. Отличие в ка>дой иE
четырех ситуаций EаклOчаетс& лиш3 в иEначал3ной инициалиEации обPектов
(шаг 2). РеEул3таты представлены на рисунке 9.
Как мо>но Eаметит3, несмотр& на то, что исходные обPекты одни и те
>е, в реEул3тате работы алгоритма кластериEовалис3 они по-раEному (и де-
ло вовсе не в цветах, а в качественном составе кластеров). Особенно Eаметно
отличие верхней правой конфигурации от трех других. Впрочем, если пригл&-
дет3с&, легко найти несоответстви& и в остал3ных конфигураци&х. ВоEникли
эти несоответстви& иE-Eа того, что кластеры, хот3 и четко видны человеческо-
му (и комп3Oтерному) глаEу, плохо отделены – они соединены мостами или
шумом, что мешает одноEначной кластериEации. Впрочем, мы об этом по-
дробно говорили еще во введении. Кроме того, причиной мо>ет быт3 неудач-
на& начал3на& инициалиEаци&. О том, что это такое, мы поговорим чут3 поE-
>е. Сейчас >е обратимс& к расчетному примеру, чтобы детал3но раEобрат3
все этапы алгоритма.
2.3 Пример: хруст и сладост3 продуктов
РаEберем описанный алгоритм, как говор&т, «на пал3цах», а дл& этого
рассмотрим у>е Eнакомый по предыдущим лекци&м пример про сладост3 и
хруст раEличной пищи. Будем считат3, что Eадача EаклOчаетс& не в клас-
сифицировании нового обPекта, а в выделении групп обPектов по у>е име-
Oщимс& данным. Естественно, эту Eадачу легко решит3 устно, основыва&с3
на >иEненном опыте, но мы, как нормал3ные герои, конечно >е пойдем в
15
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 9: РаEличные варианты инициалиEации
обход. В конце концов, нам ва>но пон&т3 принцип, а не решит3 конкретнуO
гастрономическуO Eадачу. Исходные данные представлены в таблице. Дл&
простоты вычислений, не будем нормироват3 или стандартиEироват3 данные.
Снова подчеркнем, что на практике это шаг &вл&етс& об&Eател3ным.
Номер Продукт Сладост> Хруст
1 банан 10 1
2 апел3син 7 4
3 виноград 8 3
4 креветка 2 2
5 бекон 1 5
6 орехи 3 3
7 сыр 2 1
8 рыба 3 2
9 огурец 2 8
10 &блоко 9 8
11 морков3 4 10
12 сел3дерей 2 9
13 салат 3 7
14 груша 8 7
15 перец 6 9
16
Высша& школа цифровой кул3туры Университет ИТМО
Первым и, наверное, самым ва>ным встает вопрос: на какое >е чис-
ло кластеров мы будем делит3 исходные обPекты? На бол3шом количестве
данных с Eаведомо неиEвестным числом кластеров имеет смысл провести
нескол3ко экспериментов, чтобы подобрат3 более-менее оптимал3ное число,
если, конечно, это воEмо>но в рамках поставленной Eадачи (нет строгих огра-
ничений по времени и по ресурсам). Мы то>е могли бы решит3 эту Eадачу с
раEличными Eначени&ми, но, так как данные нам интуитивно пон&тны, давай-
те рассмотрим распределение по трем кластерам (о>идаемо – овощи, фрукты
и протеины) и посмотрим, что у нас получитс&.
Согласно описанному алгоритму, после выбора количества кластеров
необходимо проиEвести инициалиEациO – отнести ка>дый обPект исходных
данных к одному иE трех кластеров случайным обраEом. ОбPекты и присво-
енные им номера кластеров мо>но увидет3 в таблице:
Номер Продукт Сладост> Хруст Кластер
1 банан 10 1 2
2 апел3син 7 4 2
3 виноград 8 3 2
4 креветка 2 2 3
5 бекон 1 5 1
6 орехи 3 3 1
7 сыр 2 1 3
8 рыба 3 2 1
9 огурец 2 8 2
10 &блоко 9 8 1
11 морков3 4 10 2
12 сел3дерей 2 9 2
13 салат 3 7 2
14 груша 8 7 1
15 перец 6 9 2
Далее находим центроиды ка>дого кластера. Кластеру 1 принадле>ат следу-
Oщие точки: (1, 5), (3, 3), (3, 2), (9, 8), (8, 7). Найдем координаты x11, x12 цен-
троида этого кластера:
x11 = 1 + 3 + 3 + 9 + 8
5 = 4.8,
x12 = 5 + 3 + 2 + 8 + 7
5 = 5.
Аналогичным обраEом найдем координаты xk1, xk2, k ∈ {2, 3}, центроидов
17
Высша& школа цифровой кул3туры Университет ИТМО
кластеров C2 и C3.
x21 = 10 + 7 + 8 + 2 + 4 + 2 + 3 + 6
8 = 5.25,
x22 = 1 + 4 + 3 + 8 + 10 + 9 + 7 + 9
8 = 6.375,
x31 = 2 + 2
2 = 2,
x32 = 2 + 1
2 = 1.5.
ПроиллOстрируем полученное на рисунке 10: красным иEобра>ены точки-
центроиды, исходные данные раскрашены в раEные цвета в Eависимости от
иEначал3ной инициалиEации.
Рис. 10: ИнициалиEаци& и нахо>дение центроидов
Двигаемс& дал3ше. Тепер3, согласно алгоритму, необходимо вычислит3
квадраты рассто&ний от ка>дой точки до всех трех центроидов и поместит3
обPект в тот кластер, квадрат рассто&ни& до центроида которого ока>етс&
наимен3шим. Точка под кодовым словом «банан» &вл&етс& первой Eапис3O
18
Высша& школа цифровой кул3туры Университет ИТМО
в таблице (i = 1) и имеет координаты (10, 1). Находим квадраты рассто&ний
до центроидов (4.8, 5), (5.25, 6.375), (2, 1.5):
d2
E(x1, x1) = (10 − 4.8)2 + (1 − 5)2 = 43.04,
d2
E(x1, x2) = (10 − 5.25)2 + (1 − 6.375)2 = 51.453125,
d2
E(x1, x3) = (10 − 2)2 + (1 − 1.5)2 = 64.25.
Наимен3шим получилс& квадрат рассто&ни& до центроида кластера 1, тем са-
мым обPект «банан» тепер3 принадле>ит кластеру 1. АналогичнуO процеду-
ру проделываем со всеми остал3ными обPектами. Kаполним таблицу новыми
Eначени&ми соответствуOщих кластеров.
Продукт Сладост> Хруст Кластер
банан 10 1 1
апел3син 7 4 1
виноград 8 3 1
креветка 2 2 3
бекон 1 5 3
орехи 3 3 3
сыр 2 1 3
рыба 3 2 3
огурец 2 8 2
&блоко 9 8 1
морков3 4 10 2
сел3дерей 2 9 2
салат 3 7 2
груша 8 7 1
перец 6 9 2
Далее повтор&ем шаг 3 и находим новые центроиды полученных кластеров.
x11 = 10 + 7 + 8 + 9 + 8
5 = 8.4,
x12 = 1 + 4 + 3 + 8 + 7
5 = 4.6,
x21 = 2 + 4 + 2 + 3 + 6
5 = 3.4,
x22 = 8 + 10 + 9 + 7 + 9
5 = 8.6,
x31 = 2 + 1 + 3 + 2 + 3
5 = 2.2,
x32 = 2 + 5 + 3 + 1 + 2
5 = 2.6.
19
Высша& школа цифровой кул3туры Университет ИТМО
Повтор&& шаг 4, находим квадрат рассто&ни& от ка>дого обPекта до новых
центроидов и наEначаем обPекту тот кластер, квадрат рассто&ни& до цент-
роида которого окаEалс& наимен3шим. Выполн&& расчеты, мо>но Eаметит3,
что ни один обPект не иEмен&ет своей принадле>ности кластеру, а Eначит
кластериEаци& Eавершена. Таким обраEом, обPекты распределены по трем
кластерам, в ка>дом иE которых обPекты обладаOт схо>ими приEнаками.
Ну а так как мы Eаведомо Eнаем, что это были Eа обPекты, мо>но скаEат3,
что раEбиение получилос3 верным, обPекты распределилис3 правил3но (ри-
сунок 11).
Рис. 11: РеEул3таты кластериEации
2.4 Ра>личные способы начал3ной инициали>а-
ции. Сходимост3 метода
Инициали5аци3 методами K-means++ и наибол>шим удалением
Итак, как мы у>е отмечали ранее, реEул3тат кластериEации методом К-
средних, вообще говор&, Eависит от начал3ной инициалиEации, то ест3 от
начал3ного распределени& рассматриваемых данных на кластеры. Метод,
предло>енный в алгоритме, интуитивно пон&тен и баEируетс& вот на каких
20
Высша& школа цифровой кул3туры Университет ИТМО
сообра>ени&х: если наши кластеры – плотные, хорошо раEделенные шаро-
вые сгустки, имеOщие примерно одинаковый обPем, то после случайной ини-
циалиEации центроиды, будучи центрами масс, ока>утс& располо>енными
где-то посередине ме>ду «облаков данных», см. рисунок 12. Тогда ка>дое
облако, конечно, чут3 ли не сраEу будет отнесено к бли>айшему (конкретно-
му) центроиду, и, видимо, буквал3но Eа одну итерациO алгоритм проиEведет
беEошибочнуO кластериEациO. Конечно, така& стерил3на& ситуаци& бывает
не всегда.
Рис. 12: ИEначал3ный вариант инициалиEации – хороший вариант
Посмотрите на рисунок 13. Прекрасно видно, что тепер3 один кластер
имеет существенно бол3ший обPем, чем два других. При случайной начал3-
ной инициалиEации на три кластера, все три центроида ока>утс& прит&нуты-
ми к верхнему облаку. Совершенно &сно, что тепер3 два верхних центроида
будут пытат3с& раEбит3 бол3шое облаков на две части, а третий центроид
встанет где-то посередине ме>ду двум& мален3кими облаками, обPединив их
в один кластер. Така& ситуаци&, конечно, крайне не>елател3на. Обратите
внимание, не помогло да>е то, что мы иEначал3но правил3но определили
количество кластеров!
Дл& исправлени& описанной ситуации, мы предло>им два других (не
единственных, но наиболее попул&рных) эмпирических подхода к начал3ной
инициалиEации. Оба подхода реалиEованы в бол3шинстве современных ин-
струментов. При испол3Eовании описываемых способов, шаги 2 и 3 приве-
денного ранее алгоритма мен&Oтс& на один иE двух алгоритмов, описанных
ни>е.
1. Инициали5аци3 наибол>шим удалением. Данный подход предпо-
лагает первоначал3ный выбор K центроидов среди набора исходных
данных по следуOщему алгоритму:
21
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 13: ИEначал3ный вариант инициалиEации – плохой вариант
• В качестве первых двух центроидов берутс& те два обPекта ис-
ходных данных, которые располо>ены на наибол3шем рассто&нии
друг от друга. Если таких пар нескол3ко, выбираетс& лOба&.
• Ка>дый следуOщий центроид (до K-ого вклOчител3но) выбира-
етс& таким обраEом, что рассто&ние от него до бли>айшего иE
выбранных ранее центроидов максимал3но. Если претендентов
нескол3ко, выбираетс& лOбой иE них.
Пон&тно, что описанный метод пытаетс& располо>ит3 центроиды как
мо>но дал3ше друг от друга и «раEбросат3» их по пространству при-
Eнаков Rp так, чтобы иEбе>ат3 описанных ранее склеек, нену>ных обP-
единений или, наоборот, раEделений кластеров.
2. K-means++. Данный подход очен3 похо> на предыдущий и нацелен
на внесение некоторой веро&тностной подоплеки в описанный детерми-
ниEм. Алгоритм первоначал3ного выбора K центроидов таков:
• Случайно выбранный обPект исходного набора данных наEначает-
с& первым центроидом.
• Пуст3 выбрано 1 ≤ m ≤ K − 1 центроидов среди n исходных дан-
ных. Перенумеруем оставшиес& данные, обоEначив их x1, ..., xn−m,
и вычислим рассто&ние di от ка>дого иE обPектов до бли>айшего
центроида, i ∈ {1, 2, ..., n − m}. СледуOщий центроид выбираетс&
веро&тностным обраEом, причем веро&тност3 выбрат3 в качестве
22
Высша& школа цифровой кул3туры Университет ИТМО
центроида обPект xi равна
P((m + 1)-ый центроид – это xi) = di
d1 + d2 + ... + dn−m
.
Отметим, что в качестве рассто&ни& мо>ет быт3 выбрано проиEвол3-
ное рассто&ние или проиEвол3на& функци& сходства, котора& интересна
исследователO. Например, часто в качестве функции рассто&ни& выби-
раOт квадрат евклидова рассто&ни&.
Сходимост> метода К-средних
Ранее мы у>е говорили о том, что метод K-средних сходитс&, то ест3
существует шаг, начина& с которого обPекты перестаOт мен&т3 своO кла-
стернуO принадле>ност3. В этом пункте мы обоснуем это формал3но.
7амечание 2.4.1 Напомним, что если x = (x1, x2, ..., xp), то норма, согла-
сованна8 с евклидовым рассто8нием, вводитс8 следу7щим обра6ом:
'x' =
#$$%
p"
i=1
x2
i .
Если x′ = (x′
1, x′
2, ..., x′
p), то
dE(x, x′) =
#$$%
p"
i=1
(xi − x′
i)2 = 'x − x′'
и, соответсвенно,
d2
E(x, x′) =
p"
i=1
(xi − x′
i)2 = 'x − x′'2.
Сначала дока>ем следуOщуO лемму.
Лемма 2.4.1 Пуст% x1, ..., xn ∈ Rp. Тогда
Arg min
z∈Rp
n"
i=1
'xi − z'2 = x, где x = 1
n
n"
i=1
xi.
Иными словами, лемма утвер>дает, что обPект, до которого сумма квадратов
рассто&ний от всех обPектов кластера минимал3на – это центроид соответ-
свуOщего кластера.
23
Высша& школа цифровой кул3туры Университет ИТМО
Дока5ател>ство. Рассмотрим цепочку преобраEований
Arg min
z∈Rp
n"
i=1
'xi − z'2 = Arg min
z∈Rp
n"
i=1
(xi − z, xi − z) =
= Arg min
z∈Rp
n"
i=1
.
'xi'2 − 2(xi, z) + 'z'2/
= Arg min
z∈Rp
n"
i=1
.
'z'2 − 2(xi, z)
/
=
= Arg min
z∈Rp
(
n'z'2 − 2
( n"
i=1
xi, z
))
= Arg min
z∈Rp
n
(
'z'2 − 2
(
1
n
n"
i=1
xi, z
))
=
= Arg min
z∈Rp
.
'z'2 − 2 (x, z)
/
= Arg min
z∈Rp
.
'z'2 − 2 (x, z) + 'x'2/
=
= Arg min
z∈Rp
'z − x'2 ⇒ z = x.
Более того, выкладки покаEываOт, что решение Eадачи единственно. □
Тепер3 обоснуем сходимост3 описанного ранее алгоритма. Испол3Eу& вве-
денные обоEначени&, вспомним, что мы минимиEируем функциO
Q =
K"
k=1
"
xi∈Ck
'xi − xk'2.
Пуст3 по окончании итерации p мы имеем набор кластеров Cp
1 , ..., Cp
K и со-
ответствуOщих им центроидов xp
1, ..., xp
K. Тогда, на итерации p + 1, если су-
ществуOт обPекты, дл& которых мо>но найти центроид, квадрат рассто&ни&
до которого будет мен3ше, чем квадрат рассто&ни& до центроида текущего
кластера, то, иEменив принадле>ност3 этих обPектов и сформировав новые
кластеры, имеем
Qp =
K"
k=1
"
xi∈Cp
k
'xi − xp
k'2 >
K"
k=1
"
xi∈Cp+1
k
'xi − xp
k'2,
иначе алгоритм Eавершен. Пересчитав центроиды после перераспределени&,
испол3Eу& докаEаннуO лемму, получим следуOщее неравенство
Qp >
K"
k=1
"
xi∈Cp+1
k
'xi − xp
k'2 ≥
K"
k=1
"
xi∈Cp+1
k
'xi − xp+1
k '2 = Qp+1,
откуда Qp > Qp+1. Итого, алгоритм на ка>дом шаге умен3шает рассматрива-
емуO функциO Q. Так как всего воEмо>но не более Kn раEличных раEбиений
на кластеры, а центроид по кластеру строитс& единственным обраEом, то ал-
горитм конечен.
24
Высша& школа цифровой кул3туры Университет ИТМО
2.5 Выбор числа K. Камениста8 осып3
ИEучив как алгоритм, так и его применение достаточно подробно, вопрос
иEначал3ного определени& числа кластеров пока что так и остаетс& откры-
тым. Wсно, что гарантированно верного ответа о правил3ном количестве кла-
стеров дат3 нел3E&, но мо>но попробоват3 отследит3 «степен3 качества» кла-
стериEации, испол3Eу& так наEываемуO «каменистуO осып3» или «локот3».
При кластериEации исходного набора данных на K кластеров, мы ста-
ралис3 умен3шат3 Eначение выра>ени&
K"
k=1
"
xi∈Ck
d2
E(xi, xk),
численно равного сумме квадратов рассто&ний от обPектов кластера до цент-
роида этого кластера по всем кластерам. В реEул3тате кластериEации (после
окончани& работы алгоритма), мы мо>ем вычислит3 Eначение Qfinal(K) –
сумму квадратов рассто&ний от обPектов кластера до центроида этого кла-
стера по всем кластерам после того, как обPекты перестаOт мен&т3 своO
кластернуO принадле>ност3 (то ест3 после того, как алгоритм сошелс&). По-
н&тно, что при увеличении числа кластеров, Eначение Qfinal(K) будет умен3-
шат3с&.
Определение 2.5.1 Каменистой осып%7 или локтем на6ываетс8 график
6ависимости 6начени8 выра)ени8 Qfinal(K) от количества кластеров K.
На рисунке 14 представлен пример такого графика. По оси абсцисс отло>е-
но количество кластеров, по оси ординат – Eначение функции Qfinal(K) в
Eависимости от K. Легко видет3, что Eначени& Qfinal(K) перестаOт реEко иE-
мен&т3с&, начина& с K = 3, или K = 4, или K = 5. Скорее всего, именно
эти Eначени& и имеет смысл проверит3 на «истинност3»: осуществит3 кла-
стериEациO, проинтерпретироват3 кластеры и посмотрет3, кака& иE интер-
претаций окаEываетс& наиболее адекватной с точки Eрени& как выводов, так
и предметной области.
Почему так, спросите вы? Потому что при K > 5, Eначение Qfinal(K) пе-
рестает мен&т3с& реEко, а Eначит, скорее всего, основные кластеры выбраны,
точки раEделилис3 на сгустки, а дал3нейшее деление (то ест3 увеличение K)
начинает раEдел&т3 на части у>е обоEначенные сгустки, что, скорее всего,
иEлишне и неправил3но.
Описанный график наEываOт каменистой осып3O иE достаточно >и-
тейских сообра>ений. При малых K мы видим реEкое иEменение Eначени&
Qfinal(K) – так наEываемый обрыв. Дал3нейшее >е иEменение очен3 медлен-
но и плавно и практически выро>даетс& в «легкое Eатухание» – так наEыва-
25
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 14: График каменистой осыпи
емый пл&>. Вот вам и камениста& осып3. Наверное, ка>дый иE вас мо>ет
провести похо>уO аналогиO и с наEванием «локот3».
3 Агломеративна8 кластери>аци8
3.1 Небол3ша8 мотивировка
Итак, рассмотренный ранее метод K-средних, несомненно, имеет свои
преимущества – это и нагл&дност3, и интерпретируемост3, и простота ре-
алиEации. В то >е врем&, он обладает и существенными недостатками. Во-
первых, метод К-средних предполагает, что кластеры – это плотные шаровые
сгустки, а во-вторых он требует Eадани& количества кластеров еще до начала
исследовани&, что, конечно, сравнимо с подходом «ткнут3 пал3цем в небо».
Конечно, у нас ест3 способ более интеллектуал3ной настройки алгоритма –
так наEываема& камениста& осып3, но и она не всегда &вл&етс& панацеей.
Кроме того, иE-Eа вычислител3ной сло>ности алгоритма, кластериEаци& при
раEных Eначени&х K мо>ет длит3с& очен3 и очен3 долго.
В этом раEделе мы рассмотрим еще один метод кластериEации – так на-
EываемуO агломеративнуO или, как еще ее наEываOт, иерархическуO класте-
риEациO. Данный метод, во-первых, вычислител3но более прост, во-вторых,
не требует первоначал3ного Eадани& числа кластеров, в-трет3их, поEвол&ет
удобным обраEом виEуалиEироват3 реEул3таты кластериEации и в-четвертых
(но далеко не в последних), умет находит3 ленточные кластеры.
26
Высша& школа цифровой кул3туры Университет ИТМО
3.2 Описание алгоритма. Способы и>мерени8
рассто8ни8 ме&ду кластерами
СраEу приведем формал3ное описание алгоритма, дела& ставку, скорее,
на интуициO; технические детали по&сним нескол3ко поE>е. Пуст3 имеетс&
набор данных X = (x1, x2, ..., xn) обPема n.
1. Выбираетс& функци& рассто&ни& ρ(X, Y ), отра>аOща& «похо>ест3»
(или «блиEост3») кластеров X и Y .
2. Ка>дый обPект xi, i ∈ {1, 2, ..., n}, помещаетс& в отдел3ный кластер:
C1 = {x1}, C2 = {x2}, ...., Cn = {xn}.
3. Пуст3 имеетс& K > 1 кластеров. Ищутс& два наиболее «похо>их» отно-
сител3но выбранной функции рассто&ни& ρ кластера и обPедин&Oтс&
ме>ду собой, остал3ные кластеры остаOтс& неиEменными. В случае,
если кандидатов (пар) на обPединение нескол3ко, обPедин&етс& лOба&
пара кандидатов. В итоге остаетс& K − 1 кластер.
4. Шаг 3 повтор&етс&, пока количество кластеров не станет равным 1.
Все шаги алгоритма дол>ны быт3 интуитивно пон&тны, их легко иEоб-
раEит3 на рисунке, см. рисунок 15. У нас ест3 5 обPектов, ка>дый иE которых
на начал3ном этапе &вл&етс& отдел3ным кластером, итого на старте (после
шага 2) мы имеем 5 кластеров. Wсно, что бли>айшими кластерами, с точки
Eрени& человеческого воспри&ти&, &вл&Oтс& кластеры, содер>ащие крестик
и треугол3ник, Eначит они и будут обPединены на первом шаге (кластеры,
получившиес& после первой итерации обPединени&, обведены оран>евым).
Итак, осталос3 4 кластера. Двигаемс& дал3ше. Тепер3 бли>е всего располо-
>ены кластеры, содер>ащие квадрат и ромб, обPедин&ем их. Кластеры по
окончании этой итерации обведены красным. Осталос3 три кластера. Тепер3,
вроде как, бли>е всего наход&тс& кластеры, один иE которых содер>ит кре-
стик и треугол3ник, а второй – круг, обPединим их. РеEул3тат обведен синим.
Осталос3 два кластера – обPедин&ем их и... алгоритм Eавершен. Не останав-
лива&с3 на данный момент на вопросе интерпретации полученных кластеров,
обсудим немалова>ный технический вопрос: а что Eначит «бли>айший» кла-
стер или, что то >е самое, а откуда беретс& упом&нута& в алгоритме (и, в
некотором смысле, клOчева&) функци& ρ?
Пуст3 кластер X состоит иE элементов x1, x2, . . . , xs, а кластер X′ – иE
элементов x′
1, x′
2, . . . , x′
r, причем все элементы кластеров принадле>ат про-
странству Rp, то ест3 описываOтс& p числовыми приEнаками-предикторами.
Пуст3 так>е d – некотора& функци& рассто&ни&, покаEываOща& схо>ест3
27
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 15: ВиEуалиEаци& алгоритма
обPектов иE Rp. Пока>ем нескол3ко основных приемов определени& рассто-
&ни& ме>ду кластерами X и X′.
1. Метод полной св35и (метод дал>него соседа). В качестве рассто-
&ни& ρ(X, X′) ме>ду кластерами X и X′ принимаетс& максимал3ное
рассто&ние ме>ду элементами соответствуOщих кластеров, то ест3:
ρ(X, X′) = maxx∈X, x′∈X′
d(x, x′),
см. рисунок 16. Данное рассто&ние достаточно хорошо выдел&ет плот-
ные шаровые сгустки.
2. Метод одиночной св35и (метод блиAайшего соседа). В качестве
рассто&ни& ρ(X, X′) ме>ду кластерами X и X′ принимаетс& минимал3-
ное рассто&ние ме>ду элементами соответствуOщих кластеров, то ест3:
ρ(X, X′) = minx∈X, x′∈X′
d(x, x′),
см. рисунок 17. Данное рассто&ние отлично выдел&ет ленточные кла-
стеры.
3. Метод средней св35и (среднее нев5вешенное рассто3ние). В
качестве рассто&ни& ρ(X, X′) ме>ду кластерами X и X′ принимаетс&
среднее арифметическое всех попарных рассто&ний ме>ду элементами
соответствуOщих кластеров, то ест3:
ρ(X, X′) = 1
|X| · |X′|
"
x∈X
"
x′∈X′
d(x, x′),
28
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 16: Метод полной св&Eи
где |X| и |X′| – количество элементов в кластерах X и X′, соответствен-
но, cм. рисунок 18. Данное рассто&ние хорошо справл&етс& с выделени-
ем плотных шаровых сгустков.
4. Центроидный метод (центроидал>ный метод). В качестве рас-
сто&ни& ρ(X, X′) ме>ду кластерами X и X′ принимаетс& рассто&ние
ме>ду их центроидами, то ест3
ρ(X, X′) = d(x, x′),
где x и x′ – центроиды кластеров X и X′, соответственно, см. рисунок
19. Данный метод то>е справл&етс& с плотными шаровыми сгустка-
ми, однако примен&етс& достаточно редко иE-Eа воEмо>ных самопере-
сечений в так наEываемой дендрограмме при виEуалиEации реEул3татов
кластериEации; мы обратим на это внимание чут3 поE>е.
5. Метод Варда. В качестве рассто&ни& ρ(X, X′) ме>ду кластерами X и
X′ принимаетс& величина, покаEываOща& иEменение суммы квадратов
рассто&ний от элементов кластера до центроида до и после обPедине-
ни&, то ест3:
ρ(X, X′) =
"
x∈X∪X′
d2
E(x, x∪) −
"
x∈X
d2
E(x, x) −
"
x∈X′
d2
E(x′, x′) =
29
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 17: Метод одиночной св&Eи
= |X||X′|
|X| + |X′|d2
E(x, x′),
где x, x′ и x∪ – центроиды кластеров X, X′ и X ∪X′, соответственно, а
|X| и |X′| – количество элементов в кластерах X и X′, соответственно,
dE – евклидово рассто&ние.
Видно, что первое слагаемое в последней формуле отвечает Eа сумму квад-
ратов рассто&ний от обPектов обPединенного кластера до центроида этого
кластера, а второе и трет3е – Eа сумму квадратов рассто&ний от обPектов
кластера X и X′ до центроидов кластеров X и X′, соответсвенно. В итоге, в
методе Варда рассто&ние ме>ду кластерами покаEывает что-то вроде иEме-
нени& «плотности» кластера при обPединении двух кластеров; нацелено оно,
скорее, на выделение мелких кластеров.
Kаметим и следуOщее. Рассто&ние, испол3Eуемое в методе Варда, имеет
отношение к статистике, к дисперсионному аналиEу. Оно оценивает иEменение
раEброса в данных после обPединени& кластеров; в частности поэтому оно
достаточно попул&рно в прило>ени&х.
7амечание 3.2.1 Отметим, что в приведенном нами аналитическом вы-
ра)ении дл8 рассто8ни8 ρ(X, X′) в методе Варда испол%6уетс8 именно ев-
клидово рассто8ние dE. В инструментах часто бывает во6мо)ным ис-
30
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 18: Метод средней св&Eи
пол%6оват% и каку7-то другу7 функци7 d; в этом случае последнее равен-
ство, вообще говор8, неверно.
Так)е отметим, что при испол%6овании метода Варда, некоторые
инструменты вычисл87т рассто8ние ме)ду кластерами следу7щим об-
ра6ом: 0 "
x∈X∪X′
d2
E(x, x∪) −
"
x∈X
d2
E(x, x) −
"
x∈X′
d2
E(x′, x′).
Это – не что иное, как корен% и6 введенного нами рассто8ни8. Более того,
в некоторых инструментах написанное выра)ение умно)аетс8 на поло-
)ител%ну7 константу. Не вдава8с% в детали о6вученных отличий ска-
)ем, что подобные и6менени8, по бол%шому счету, не ска6ыва7тс8 на ре-
6ул%тате кластери6ации, вед% по сути к введенной ранее функции ρ(X, X′)
просто-напросто примен8етс8 некоторое монотонное преобра6ование, мас-
штабиру7щее рассто8ни8.
7амечание 3.2.2 Пон8тно, что так как пон8тие бли6ости кластеров в
иерархической кластери6ации основано на пон8тии рассто8ни8, то при6на-
ки кластери6уемых об2ектов перед началом кластери6ации имеет смысл
либо стандарти6ироват%, либо нормироват%.
31
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 19: Центроидный метод
В EаклOчение данного пункта, приведем формал3ное обоснование напи-
санной в пункте 5 формулы.
Лемма 3.2.1 Пуст% X и X′ – два кластера об2ектов и6 Rp, dE – евклидово
рассто8ние в Rp, тогда
"
x∈X∪X′
d2
E(x, x∪) −
"
x∈X
d2
E(x, x) −
"
x∈X′
d2
E(x′, x′) = |X||X′|
|X| + |X′|d2
E(x, x′),
где x, x′ и x∪ – центроиды кластеров X, X′ и X ∪ X′, соответственно, а
|X| и |X′| – количество элементов в кластерах X и X′, соответственно.
Дока5ател>ство. Во-первых Eаметим, что в силу того, что кластеры не пе-
ресекаOтс&, справедливо соотношение
x∪ = 1
|X| + |X′|
"
x∈X∪X′
x = |X|
|X| + |X′|x + |X′|
|X| + |X′|x′.
Так как
d2
E(x, x′) = 'x − x′'2,
то
"
x∈X
d2
E(x, x∪) =
"
x∈X
'x − x∪'2 =
"
x∈X
1111x − x − |X′|
|X| + |X′|(x′ − x)
1111
2
=
32
Высша& школа цифровой кул3туры Университет ИТМО
=
"
x∈X
'x − x'2 − 2 |X′|
|X| + |X′|
"
x∈X
(x − x, x′ − x) + |X|
2 |X′|
|X| + |X′|
32
'x − x′'2.
Wсно, что среднее слагаемое равно нулO. Симметрично,
"
x∈X′
d2
E(x, x∪) =
"
x∈X′
'x − x′'2 + |X′|
2 |X|
|X| + |X′|
32
'x − x′'2.
Складыва& равенства, получим
"
x∈X∪X′
d2
E(x, x∪) =
"
x∈X
d2
E(x, x∪) +
"
x∈X′
d2
E(x, x∪) + |X||X′|
|X| + |X′|'x − x′'2.
□
Тепер3 рассмотрим пример применени& написанного алгоритма, а так>е на-
учимс& виEуалиEироват3 кластериEациO – строит3 и интерпретироват3 так
наEываемуO дендрограмму.
3.3 Пример: хруст и сладост3 продуктов. Денд-
рограмма
Пример кластери5ации
Рассмотрим процедуру кластериEиции и построени& дендрограммы на
у>е Eнакомом нам примере с хрустом и сладост3O пищи. Исходные данные
приведены в следуOщей таблице:
Номер Продукт Сладост> Хруст
1 банан 10 1
2 апел3син 7 4
3 виноград 8 3
4 креветка 2 2
5 бекон 1 5
6 орехи 3 3
7 сыр 2 1
8 рыба 3 2
9 огурец 2 8
10 &блоко 9 8
11 морков3 4 10
12 сел3дерей 2 9
13 салат 3 7
14 груша 8 7
15 перец 6 9
33
Высша& школа цифровой кул3туры Университет ИТМО
ВиEуалиEируем наши данные. Дл& удобства воспри&ти&, вместо иEобра>ени&
точек на плоскости будем прописыват3 соответствуOщие номера обPектов
иE таблицы исходных данных, рисунок 20. Например, обPект «банан» имеет
номер 1 и координаты (10, 1), обPект «апел3син» – номер 2 и координаты
(7, 4), и так далее.
Рис. 20: Исходные данные по номерам обPектов
Сначала определимс& с методами нахо>дени& рассто&ни& как ме>ду
кластерами, так и ме>ду обPектами. В рассматриваемом примере в качестве
первого мы будем испол3Eоват3 метод полной св&Eи, а второго – евклидово
рассто&ние:
ρ(X, X′) = maxx∈X, x′∈X′
dE(x, x′), dE(x, x′) =
#$$%
p"
j=1
(xj − x′
j)2,
x = (x1, x2, . . . , xp), x′ = (x′
1, x′
2, . . . , x′
p).
34
Высша& школа цифровой кул3туры Университет ИТМО
В самом начале, согласно алгоритму, ка>дый обPект представл&ет собой от-
дел3ный кластер, а Eначит у нас в распор&>ении 15 кластеров. Вычислим
все попарные рассто&ни& ме>ду кластерами и выберем два наиболее блиE-
ких. Дл& удобства поиска рассто&ни& ме>ду кластерами составим матрицу
рассто&ний. В качестве элементов матрицы выступаOт рассто&ни& ме>ду
рассматриваемыми обPектами. На рисунке 21 приведены округленные ре-
Eул3таты с точност3O до 1 Eнака после Eап&той; при насто&щих расчетах
округлени& лучше не испол3Eоват3.
Рис. 21: Матрица рассто&ний
Мо>но Eаметит3, что у нас получилос3 нескол3ко пар (обPектов, или,
что на первой итерации то >е самое, кластеров), рассто&ни& ме>ду которыми
равны единице:
dE(4, 7) = dE(4, 8) = dE(6, 8) = dE(9, 12) = 1.
Согласно описанному алгоритму, мы мо>ем обPединит3 в новый кластер лO-
буO иE представленных пар. ОбPединим, например, «креветку» и «сыр» (обP-
екты с номерами 4 и 7).
Тепер3 у нас осталос3 14 кластеров, причем один иE них состоит иE 2-ух
элементов:
{4, 7}, {1}, {2}, {3}, {5}, {6}, {8}, {9}, {10}, {11}, {12}, {13}, {14}, {15}.
Снова ищем попарные рассто&ни& ме>ду всеми кластерами и выбираем наи-
мен3шее. Дл& примера, найдем рассто&ние от кластера {4, 7} до кластера
{13}. Мо>но воспол3Eоват3с& у>е найденной матрицей рассто&ний. Так как
dE(4, 13) ≈ 5.1, dE(7, 13) ≈ 6.1,
35
Высша& школа цифровой кул3туры Университет ИТМО
а в качестве рассто&ни& ме>ду кластерами беретс& метод полной св&Eи (или
дал3него соседа), то
ρ({4, 13}, {7}) = max(dE(4, 13), dE(7, 13)) ≈ 6.1.
Приведем список всех обPединений (по шагам) в рамках рассматриваемой
кластериEации:
1. {4} ∪ {7}.
2. {6} ∪ {8}.
3. {9} ∪ {12}.
4. {2} ∪ {3}.
5. {10} ∪ {14}.
6. {4, 7} ∪ {6, 8}.
7. {9, 12} ∪ {13}.
8. {11} ∪ {15}.
9. {4, 7, 6, 8} ∪ {5}.
10. {9, 12, 13} ∪ {11, 15}.
11. {1} ∪ {2, 3}.
12. {1, 2, 3} ∪ {10, 14}.
13. {4, 7, 6, 8, 5} ∪ {9, 12, 13, 11, 15}.
14. {1, 2, 3, 10, 14} ∪ {4, 7, 6, 8, 5, 9, 12, 13, 11, 15}.
После окончани& алгоритма кластериEации, мо>но приступат3 к построениO
дендрограммы.
Построение дендрограммы
Дендрограмма, отвечаOща& проведенной кластериEации, представлена
на рисунке 22. Давайте раEберемс& в том, как ее строит3 и как интерпретиро-
ват3 реEул3таты. Дл& этого отло>им по оси абсцисс номера наших обPектов
в соответствии с пор&дком, полученным на последней итерации:
{1, 2, 3, 10, 14, 4, 7, 6, 8, 5, 9, 12, 13, 11, 15}.
36
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 22: Агломеративна& кластериEаци&
На первой итерации соедин&Oтс& кластеры (провод& аналогиO с дерев3&ми
– лист3&) {4} и {7} «веткой» (гориEонтал3ным отреEком), располо>енной на
высоте, равной рассто&ниO ме>ду кластерами {4} и {7}; так как рассто&ние
равно 1, то гориEонтал3ный отреEок имеет игрековуO координату, равнуO 1,
вертикал3ные отреEки от лист3ев до «ветки» провод&тс& лиш3 дл& удобства
и нагл&дности.
Kатем аналогичным обраEом соедин&Oтс& «ветками» пары лист3ев
(6, 8), (9, 12), (2, 3), (10, 14) (рассто&ние ме>ду элементами двух последних
пар равно 1.4, поэтому ветки, соедин&Oщие элементы этих пар, выше и на-
ход&тс& на отметке 1.4 по оси игрек).
Дал3ше необходимо соединит3 кластеры {4, 7} и {6, 8}. Высота «ветки»
оп&т3 >е равна рассто&ниO ме>ду кластерами {4, 7}, {6, 8}, то ест3
ρ({4, 7}, {6, 8}) = max(dE(4, 6), dE(4, 8), dE(7, 6), dE(7, 8)) = dE(7, 6) ≈ 2.2.
Попробуйте самосто&тел3но найти, например, рассто&ние ме>ду кластерами
{9, 12, 13} и {11, 15}), испол3Eу& матрицу рассто&ний. Дол>но получит3с&
так:
ρ({9, 12, 13}, {11, 15}) = dE(9, 15) ≈ 4.1.
Все дал3нейшие действи& аналогичны описанным ранее. Вс& последовател3-
ност3 построений покаEана на рисунке 23.
37
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 23: Последовател3ност3 построени& дендрограммы
Итак, кластериEаци& проведена, дендрограмма построена, но все еще
остаOтс& неEатронутыми два клOчевых вопроса. Первый – это вопрос о коли-
честве кластеров, а второй – о составе получившихс& кластеров. ОкаEывает-
с&, име& перед глаEами дендрограмму, варианты ответов на первый и второй
вопросы станов&тс& очен3 нагл&дными.
СраEу ска>ем, что исследовател3 дол>ен самосто&тел3но определит3 ко-
личество кластеров. В прочем, ест3 и некоторый бонус, он это мо>ет сделат3
у>е постфактум, подбира& наиболее оптимал3ное и правдоподобное (об этом
подробнее мы ска>ем в следуOщем пункте) число кластеров, рисунок 24. Дл&
получени& итоговых кластеров, достаточно провести гориEонтал3нуO черту
на выбранной «высоте». Тогда количество пересечений с вертикал3ными со-
единени&ми «веток» и будет равно количеству отсеченных кластеров. Лист3&
отсеченных кластеров – это и ест3 кластериEованные обPекты. Если в соот-
ветствии с нашими представлени&ми о еде выдел&т3 3 кластера, то реEул3тат
мо>но наблOдат3 на рисунке 25. Римскими цифрами отмечены итерации, на
которых проиEошло обPединение.
3.4 Камениста8 осып3 и определение числа кла-
стеров
Как вы у>е, наверное, пон&ли, оEвученный ранее бонус иерархической
кластериEации – отсутствие надобности иEначал3ного выбора количества
38
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 24: Определение количества кластеров
кластеров – вовсе не бонус как таковой. Конечно, име& n кластериEуемых
обPектов, в реEул3тате кластериEации мы получаем n воEмо>ных вариантов
раEделени& обPектов на группы, но как пон&т3 какуO иE кластериEаций вы-
брат3, в какой момент остановит3с&? Наверное, по опыту метода К-средних,
вы и сами мо>ете предло>ит3 практическое решение: посмотрет3 на каме-
нистуO осып3. Чем >е она &вл&етс& в случае иерархической кластериEации?
А вот чем.
Определение 3.4.1 Каменистой осып%7 или локтем на6ываетс8 график
6ависимости 6начени8 рассто8ни8 ρ(X, X′) ме)ду об2един8емыми класте-
рами X и X′ от количества оставшихс8 после об2единени8 кластеров.
Посмотрите на рисунок 26, на нем иEобра>ена камениста& осып3 дл& иерар-
хической кластериEации. Как интерпретироват3 данный график? Давайте
раEбират3с&. В случае, когда все обPекты обPединены в один кластер, со-
гласно графику рассто&ние ме>ду последними обPедин&емыми кластерами
чут3 бол3ше, чем 25. На предыдущем этапе, когда после обPединени& оста-
лос3 два кластера, наимен3шее рассто&ние ме>ду обPедин&емыми класте-
рами было равно примерно 10. На этапе до этого – около 9, и так далее.
Как и в случае метода К-средних, по каменистой осыпи имеет смысл най-
ти такое число кластеров, после которого иEменение рассто&ни& становитс&
Eначител3но мен3ше, чем до которого. Суд& по приведенному графику, имеет
39
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 25: Этапы обPединени& кластеров
смысл попробоват3 оставит3 либо 2, либо 4, либо 6 кластеров. ВиEуал3на& ин-
терпретаци& с обрывом и пл&>ем остаетс& неиEменной – такой >е, как и при
обсу>дении метода К-средних; более подробно на этом мы останавливат3с&
не будем.
Рис. 26: Камениста& осып3 в иерархической кластериEации
Отметим отдел3но, что график каменистой осыпи вовсе не всегда ока-
Eываетс& монотонным. Рассмотрим все тот >е пример с хрустом и сладо-
ст3O продуктов, тол3ко тепер3 рассто&ние ме>ду кластерами будем вычис-
л&т3, испол3Eу& центроидал3ный метод; рассто&ние ме>ду обPектами оста-
вим пре>ним – евклидовым. Дендрограмма приведена на рисунке 27, а гра-
фик каменистой осыпи – на рисунке 28. Видно, что на последней итерации,
40
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 27: Дендрограмма дл& примера
то ест3 при обPединении двух кластеров в один, рассто&ние ме>ду обPеди-
н&емыми кластерами окаEалос3 мен3ше, чем на предыдущей итерации: когда
три кластера об3един&лис3 в два. Это – особенност3 центроидал3ного метода
иEмерени& рассто&ний ме>ду кластерами; в частности иE-Eа этого данный ме-
тод все ре>е испол3EуOт в иерархическом кластерном аналиEе. Скол3ко >е
кластеров выбрат3 в такой ситуации? Похо>е, хороший вариант – это либо 4,
либо 9, либо 12. Последние варианты отметем сраEу – у нас все 15 обPектов,
кластеры будут очен3 малочисленными, это не представл&ет интерес. А что
будет, если выбрат3 4 кластера? Чтобы не Eапутат3с&, достаточно выбрат3 ρ
равным, например, 4 и провести на соответствуOщем уровне дендрограммы
пр&муO (в данном случае – вертикал3нуO). Мы видим, что груша и &бло-
ко отделилис3 от винограда, апел3сина и банана. Что >е, скорее всего они
просто более хруст&щие, раEделение достаточно раEумно.
Напоследок отметим, что, выбрав в качестве рассто&ни& ме>ду класте-
рами метод полной св&Eи (как было в примере ранее), и отсека& 4 кластера,
мы получили бы точно такуO >е кластериEациO, как и сейчас. Совпадение
кластериEаций – хороший Eнак, укаEываOщий скорее всего на то, что класте-
ры действител3но ест3 и реEул3таты кластериEации состо&тел3ны.
41
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 28: Камениста& осып3 дл& примера
4 DBSCAN
4.1 Небол3ша8 мотивировка и описание
Последним методом, который мы будем рассматриват3 в данной лекции,
&вл&етс& набравший Eаслу>еннуO попул&рност3 метод DBSCAN (Density-
based spatial clustering of applications with noise) – плотностный алгоритм
пространственной кластериEации с присутствием шума. Если вдумат3с& в
само наEвание алгоритма, то становитс& пон&тной и иде&: обPекты составл&-
Oт один и тот >е кластер, если все они плотно располо>ены друг к другу.
Что Eначит плотно? Это Eначит, что р&дом с ка>дым обPектом кластера ест3
достаточно много соседей – элементов этого >е кластера.
ОкаEываетс&, что DBSCAN с успехом справл&етс& с нахо>дением как
плотных шаровых сгустков, так и ленточных кластеров. Кроме того, при
фиксированных параметрах модели количество кластеров определ&етс& од-
ноEначным обраEом, а Eначит наконец-то исследователO и правда не ну>но
определ&т3 последнее каким-то хитрым обраEом. Но.. Все, конечно, далеко
не так беEоблачно. Давайте дл& начала раEберемс& с алгоритмом.
4.2 Основные определени8 и описание алгорит-
ма
Пуст3 имеетс& набор данных X = (x1, x2, ..., xn) обPема n, причем
xi ∈ Rp, i ∈ {1, 2, ..., n}, то ест3 ка>дый обPект описываетс& p числовыми
приEнаками. Пуст3 d – выбранна& функци& рассто&ни& в Rp. РаE в преамбу-
ле к этому методу реч3 шла о сосед&х, то давайте сначала раEберемс& с тем,
каким обраEом отвечат3 на вопрос: &вл&етс& ли обPект x′ соседом дл& x, или
42
Высша& школа цифровой кул3туры Университет ИТМО
нет. Дл& этого введем следуOщее определение.
Определение 4.2.1 Пуст% ε > 0. Мно)ество
B(x0, ε) = {y ∈ Rp : d(x0, y) ≤ ε}
на6ываетс8 6амкнутым (в Rp) шаром радиуса ε с центром в точке x0.
Так как мы кластериEуем исходный набор данных X, то нам совершенно не
интересны точки иE Rp, в X не вход&щие, поэтому раEумно ввести и следуO-
щее определение (и обоEначение) дл& Eамкнутого шара в X.
Определение 4.2.2 Пуст% ε > 0, x0 ∈ X. Мно)ество
BX(x0, ε) = {y ∈ X : d(x0, y) ≤ ε}
на6ываетс8 6амкнутым шаром в X с центром в x0 и радиуса ε.
Нас с вами будут интересоват3 тол3ко шары в смысле тол3ко что данного
определени&, поэтому мы, дл& краткости, будем писат3 B(x0, ε), подраEуме-
ва& BX(x0, ε). Учитыва& это соглашение, а так >е тот факт, что мно>ество
X конечно, окаEываетс& раEумным (и полеEным) ввести следуOщее обоEна-
чение:
|B(x0, ε)| – количество элементов в мно>естве BX(x0, ε).
Определение 4.2.3 Пуст% ε > 0. Элементы мно)ества B(x0, ε) на6ыва-
7тс8 сосед8ми элемента x0.
Итак, соседи элемента x0 ∈ X – это точки иE X, ле>ащие в Eамкнутой ε-
окрестности точки x0; интуитивно пон&тно, не так ли?
7амечание 4.2.1 Отметим нескол%ко поле6ных 6амечаний. Во-первых,
если выбранна8 функци8 рассто8ни8 d обладает свойством, что d(x, x) = 0,
то мно)ество B(x0, ε) никогда не пусто, а ка)дый элемент 8вл8етс8 со-
седом сам дл8 себ8. Кроме того, если d(x, x′) = d(x′, x), то если x – сосед дл8
x′, то и x′ – сосед дл8 x. В дал%нейшем мы будем предполагат%, что функ-
ци8 рассто8ни8 обладает о6вученными свойствами. В частности, л7ба8
метрика обладает написанными свойствами.
Тепер3 определим класс «хороших обPектов» – обPектов, на которые будут
опират3с&, или Eа счет которых будут строит3с& наши кластеры – обPекты,
собираOщие вокруг себ& достаточное количество соседей.
43
Высша& школа цифровой кул3туры Университет ИТМО
Определение 4.2.4 Пуст% m ∈ N – некоторое наперед 6аданное число.
Если
|B(x0, ε)| ≥ m,
то об2ект x0 на6ываетс8 корневым об2ектом.
Итак, корневой обPект – это обPект, у которого ест3 «достаточное» число
(как минимум m штук) соседей.
Ну что, мы все подготовили дл& того, чтобы перейти к формал3ному
описаниO алгоритма DBSCAN. В рамках обоEначений и терминологии, при-
н&тых в этом раEделе, алгоритм таков:
1. ВыбираOтс& ε > 0, m ∈ N. Пуст3 i = 1.
2. Находитс& какой-нибуд3 корневой обPект x в X. Если таковых нет, то
проиEводитс& переход к пункту 7.
3. Все соседи обPекта x помещаOтс& в кластер Ci.
4. Дл& ка>дого элемента иE Ci провер&етс&, &вл&етс& ли он корневым. Ес-
ли &вл&етс&, то все соседи найденного корневого элемента добавл&Oтс&
к Ci.
5. Пункт 4 повтор&етс& до тех пор, пока состав кластера Ci не перестанет
иEмен&т3с&.
6. Полагаетс& X = X \ Ci, i = i + 1. ПроиEводитс& переход к пункту 2.
7. Все обPекты мно>ества X, если они ест3, помещаOтс& в отдел3ный
кластер – выбросы. КластериEаци& Eавершена.
ПроиллOстрируем описанный алгоритм. Пуст3 имеетс& набор данных, иEоб-
ра>енный на рисунке 29, m = 3, функци& рассто&ни& d – обычное евклидово
рассто&ние, ε > 0. Согласно алгоритму, сначала находим какой-нибуд3 кор-
невой обPект; например, обPект, обведенный в синий квадрат, &вл&етс& кор-
невым, так как имеет трех соседей, вед3 круг радиуса ε вокруг него содер>ит
ровно 3 обPекта. Следу& алгоритму, относим обPекты, наход&щиес& внутри
круга, к кластеру C1.
Тепер3 ну>но проверит3: будут ли элементы кластера C1 корневыми, и
если да – добав&т ли они в него элементов? ИE геометрических сообра>ений
пон&тно, что ка>дый (а всего их 3) обPект обраEовавшегос& кластера &вл&ет-
с& корневым, поэтому помещаем в кластер C1 и соседей тол3ко что найденных
обPектов. Kа второй проход к кластеру C1 примкнут еще три элемента (най-
дите их!), а на трет3ем проходе к кластеру C1 примкнет лиш3 один, причем
на этот раE некорневой обPект, и, тем самым, первый кластер будет выделен,
44
Высша& школа цифровой кул3туры Университет ИТМО
рисунок 30. Легко пон&т3, что аналогичным обраEом будет выделен и второй
кластер, см. рисунок 31. На этом этапе хочетс& обратит3 внимание на два
Рис. 29: Пример исходных данных
Рис. 30: Выделение первого кластера
момента. Сначала вEгл&ните на обPект с >елтым восклицател3ным Eнаком.
Совершенно &сно, что он попал к синим квадратам тол3ко Eа счет того, что
первым был выбран корневой элемент, сформировавший синий кластер. Ес-
ли бы первым был выбран какой-либо корневой элемент, сформировавший
>елтый кластер, то этот обPект был бы Eачислен к >елтым.
Тепер3 вEгл&ните на обPект с синим восклицател3ным Eнаком. Он не
&вл&етс& корневым, в окру>аOщем его шаре радиуса ε лиш3 две точки, при-
надле>ащие к >елтому кластеру, поэтому точка, блиEка& к нему, в >елтый
45
Высша& школа цифровой кул3туры Университет ИТМО
Рис. 31: Два выделенных кластера
кластер не попадает. Так как бол3ше не осталос3 ранее нерассмотренных кор-
невых точек, то все обPекты, нарисованные синими точками – это выбросы
или шум (их 4 штуки). КластериEаци& окончена.
7амечание 4.2.2 Не вдава8с% в детали отметим, что при фиксированных
параметрах алгоритма m и ε, корневые элементы, если они ест%, распреде-
л87тс8 по кластерам одно6начно. То )е самое касаетс8 и выбросов. В то
)е врем8, некорневые об2екты, не 8вл87щиес8 выбросами, могут мен8т%
кластерну7 принадле)ност% в 6ависимости от пор8дка выбора корневых
элементов, мы это обсудили в приведенном примере.
Данное Eамечание покаEывает, что как тол3ко выбраны параметры алгорит-
ма, все кластериEуемые обPекты дел&тс& на один иE трех типов: корневые
обPекты, выбросы и так наEываемые граничные обPекты.
Определение 4.2.5 Некорневой об2ект, не 8вл87щийс8 выбросом, на6ыва-
етс8 граничным об2ектом или граничной точкой.
Еще раE отметим, что граничные обPекты могут мен&т3 своO кластернуO
принадле>ност3 при переEапуске алогоритма да>е с неиEменными парамет-
рами.
ИE приведенных иллOстраций так>е &сно, что при малых Eначени&х m
DBSCAN хорошо справл&етс& с поиском и выделением ленточных кластеров.
Однако, Eдес3 ест3 и подводный камен3: если кластеры соединены не очен3
раEре>енными перемычками, то, скорее всего, DBSCAN обPединит эти кла-
стеры в один единственный, и отработает куда ху>е, чем ранее рассмотрен-
ные методы. Главна& проблема EаклOчаетс& в том, что мы EачастуO не мо>ем
пон&т3: перед нами ленточный кластер или кластеры с перемычками, а Eна-
чит не мо>ем и «подскаEат3» алгоритму, поднастроив его параметры. Все
46
Высша& школа цифровой кул3туры Университет ИТМО
потому, что EачастуO мы не мо>ем виEуалиEироват3 кластериEуемые данные
иE-Eа бол3шой раEмерности пространства приEнаков.
7амечание 4.2.3 Отметим так)е, что так как пон8ти8 корневого об2-
екта, соседей 6ав86аны на пон8тии рассто8ни8, то при6наки кластери6у-
емых об2ектов перед началом кластери6ации имеет смысл либо стандар-
ти6ироват%, либо нормироват%.
Естественно, воEникает и следуOщий вопрос: а как подобрат3 параметры ал-
горитма DBSCAN оптимал3ным обраEом? Конечно, мо>но устраиват3 пере-
бор и каким-то обраEом оцениват3 качество кластериEации, но и Eдес3 быва-
Oт проблемы: многие методы оценки качества кластериEации считаOт, что
кластеры – это плотные шаровые сгустки, и тогда ленточных кластеров нам
просто не найти. В принципе, на этот счет существует достаточно много эв-
ристик, мы не будем говорит3 о них в нашей лекции, а Eаинтересованных
слушателей отправл&ем к дополнител3ным материалам.
5 FаклGчение
Итак, в этой лекции мы поEнакомилис3 с некоторыми алгоритмами кла-
стериEации, которые успешно примен&Oтс& в повседневных Eадачах. Выбор
алгоритма, его параметров, – Eадача, котора& до сих пор под силу лиш3 тол3-
ко аналитику-исследователO. Пробуйте, набирайтес3 опыта, и со временем
реEул3тат превEойдет лOбые ваши о>идани&. Удачи!
47
