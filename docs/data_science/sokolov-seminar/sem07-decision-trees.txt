Машинное обучение, ФКН ВШЭ
Семинар №7
На этом семинаре мы поговорим про обучение деревьев. Чаще всего при их обу-
чении для задачи классификации используют энтропию и критерий Джини. Сначала
мы обсудим смысл этих критериев разбиения, затем научимся использовать их для
строительства деревьев и заодно поговорим про интересную интерпретацию дивер-
генции Кульбака-Лейблера.
1 Энтропия
Давайте посмотрим на распределение двух случайных величин. Какая из них
более предсказуемая, левая или правая?
0 3
1
3
t
pY (t)
1 2 t
pZ(t)
Случайная величинаY (левая) сконцентрирована на широком отрезке. Она
равновероятно может выскочить из любой его части. Плотность случайной величины
Z (правая) имеет пикообразную форму. Её вероятностная масса сконцентрирована
на узком отрезке, из-за этого её проще прогнозировать. Ошибка, которую мы будем
допускать, окажется меньше из-за формы распределения.
Какая из следующих двух случайных величин более предсказуема, левая или
правая?
0 1
1
t
pY (t)
1 2
1
t
pZ(t)
1
2
Случайные величиныY и Z отличаются друг от друга только отрезком. Одна
распределена на[0; 1],вторая на[1; 2].Их форма одинакова. Они принимают разные
значения, но одинаково непредсказуемо. Их одинаково сложно прогнозировать.
Энтропия – это мера непредсказуемости случайной величиныY, это то коли-
чество информации, которое мы получаем, наблюдая её. Она никак не опирается
на те значения, которые принимает случайная величина и для дискретного случая
определяется как
H(X) =E(−log P(X)).
Для непрерывного случая энтропия определяется как
H(X) =E(−log pX(t)).
В качества основания логарифма берут либо2 либо натуральный логарифм.
ЕслиA– событие,P(A) – вероятность событияA,тогда величинуS(A) =−log P(A)
можно интерпретировать как/guillemotleft.cyrудивление/guillemotright.cyr (surprise)от того, что событиеAпроизо-
шло. Чем ниже вероятность события, тем сильнее мы удивляемся. ЕслиP(A) = 0.5,
тоS(A) = 1. ЕслиP(A) = 0.510, тоS(A) = 10.
Энтропия довольно часто используется в машинном обучении. Например, с по-
мощью неё обучают деревья. Кроме того, на ней базируется понятие спутанности
(perplexity), которое определяется как
Perplexity(X) =eH(X)
Со спутанностью мы встретимся, когда будем говорить про TSNE (метод с
помощью которого высоко-размерные данные можно визуализировать на плоскости).
Задача 1.1.Посчитайте энтропию для случайных величин:
а) Y 1 17 26
P(Y = k) 1
3
1
3
1
3
б) Y ∼U[0; a]
в) Y ∼N(0,σ2)
Решение.Энтропия никак не смотрит на то, какие именно значения принимает
случайная величина. Её интересует только то, как вероятность размазана по этим
значениям:
H(Y) =−1
3 ·ln
(1
3
)
−1
3 ·ln
(1
3
)
−1
3 ·ln
(1
3
)
= ln 3.
Для случайной величины, принимающей4 значения с вероятностями1
4 энтро-
пия будет равнаln 4, а в общем случае дляY ∼U[0; a] энтропия составит
H(Y) =
∫ a
0
1
a ·
(
−ln
(1
a
))
dt= lna.
Чем больше значений принимает равномерная случайная величина, тем она
непредсказуемее. Для вырожденного распределения энтропия окажется нулевой.
Для нормального распределения
3
H(Y) =E(−ln(pY (t))) =
∫ +∞
−∞
pY (t) ·ln pY (t)dt=
= E
(1
2 ln(2πσ2) + Y2
2 ·σ2
)
= 1
2 ln(2πσ2) +1
2.
Еслипопробоватьподставитьвформулуразныезначенияσ,томожнополучить
следущую примерную табличку:
σ 1 10 100
H(Y) ln 4.13 ln 41.3 ln 413
Выходит, что случайные величиныX ∼U[0; 4]и Y ∼N(0,1) в плане непред-
сказуемости очень похожи.
■
Давайте обсудим смысл энтропии и поймём, почему именно она служит хоро-
шим критерием разбиения в деревьях. Пусть Женя загадал какое-тоX. Мы знаем,
что женя обычно загадывает что-то из четырёх вариантов с конкретными вероятно-
стями
X бустинглогрег EM-алгоритм калик
P(X) 1/2 1/4 1/8 1/8
Наша задача отгадатьX за наименьшее количество-вопросов. Стратегию по
отгадыванию можно строить по-разному. Например, следующая стратегия позволит
нам гарантированно отгадатьX за два вопроса. В дереве ниже при ответе да мы
движемся в правый лист.
бустинг или логрег?
калик?
EM-алгоритм калик
логрег?
бустинг логрег
Если мы постоянно играем с Женей в эту игру, мы можем попробовать уга-
дыватьX за наименьшее число вопросов в среднем. Тогда первым вопросом надо
задавать самый вероятный вариант, а дальше спрашивать по мере убывания вероят-
ности. Оптимальная стратегия будет выглядеть следующим образом
бустинг?
логрег?
калик?
ЕМ-алгоритм калик
логрег
бустинг
4
Величина энтропииH(X) будет тогда интерпретироваться как минимальное
количество вопросов, которое мы в среднем зададим Жене. С вероятностью1
2 мы
зададим только1 вопрос, с вероятностью1
4 два и так далее
H(X) =1
2 ·1 +1
4 ·2 +2
8 ·3 =−
[1
2 ·log2
1
2 + 1
4 ·log2
1
4 + 2
8 ·log2
1
8
]
= 7
4 <2.
Когда дерево решает задачу бинарной классификации, ему на каждом объек-
те надо отгадывать класс. Кажется, что при разбиении, логично оптимизировать
энтропию.
2 Критерий Джини
Пусть случайная величинаX принимаетK значений. Тогдакритерием (индек-
сом) Джини называют величину
J(X) =
K∑
k=1
pk(1 −pk) = 1 =
K∑
k=1
p2
k.
Задача 2.1.ШаманОдэхингум1 попрошлымнаблюдениямзнает,чтобольшаяохо-
та на мамонта оказывается удачной с вероятностью0.3. Если племя ждёт от него про-
гноз охоты, то Одэхингум, поплясав вокруг костра (10 минут) и постуча в бубен (42
раза) прогнозирует удачную охоту с вероятностью0.3 и неудачную с вероятностью
0.7.
Конкурирующий шаман Пэпина2 всегда прогнозирует неудачную охоту, как бо-
лее вероятную. Когда шаман даёт неверный прогноз, его бьют палками.
1. Какова вероятность того, что Одэхингум ошибётся?
2. Кто чаще бывает бит палками?
3. Чему равен индекс Джини для случайной величины равной удаче с вероятно-
стью0.3 и неудаче с вероятностью0.7?
Решение.Ошибку Одэхингума мы можем найти по формуле полной вероятности
как0.3·0.7+0 .7·0.3 = 0.42 В первом слагаемом шаман сказал, что мамонта поймают,
но его не поймали. Во втором наоборот. Выходит, что после охоты его будут бить
палкой с вероятностью0.42.
Шаман Пэпина всегда говорит, что охота будет неудачной. То есть её побьют
палками с вероятностью0.3.
Найдём значение индекса Джини для случайной величины
X 1 0
P(X = k) 0.3 0.7.
1легкое колебание воды
2виноградная лоза, растущая вокруг дуба
5
J(X) =
K∑
k=1
pk(1 −pk) = 0.3 ·0.7 + 0.7 ·0.3 = 0.42.
Выходит, что индекс Джини измеряет, как часто шамана Одэхингума будут бить
палками и отражает неопределённость, заложенную в случайную величину.
Мы будем использовать индекс Джини для обучения деревьев. Шаманами бу-
дутпредикатывнутривершин.Предикатывнихбудутпытатьсяраздробитьвыборку
на две части так, чтобы их пореже били палками. При этом, деревья будут бинар-
ными. То есть мы не будем позволять вершинам скатываться в наивные прогнозы.
■
Задача 2.2.Предположим, что мы оказались в конкретной вершине дереваm. В
ней лежат объектыR. Поставим в соответствие вершинеm алгоритмa(x), который
выбирает класс случайно, причем классk выбирается с вероятностьюpk, гдеpk –
это доля объектов классаkв вершине. Покажите, что матожидание частоты ошибок
этого алгоритма равно индексу Джини.
Решение.
E

 1
|R|
∑
(xi,yi)∈R
[yi ̸= a(xi)]

= 1
|R|
∑
(xi,yi)∈R
E[yi ̸= a(xi)] = 1
|R|
∑
(xi,yi)∈R
(1 −pyi ) =
=
K∑
k=1
∑
(xi,yi)∈R[yi = k]
|R| (1 −pk) =
K∑
k=1
pk(1 −pk).
■
Задача 2.3.Случайная величинаX принимает значение1 с вероятностьюpи зна-
чение0 с вероятностью1 −p. Постройте график зависимости индекса Джини и
энтропии отp.
Решение.Найдём требуемые величины
J(X) =p(1 −p) + (1−p)p= 2(p−p2)
H(X) =−(p·ln p+ (1−p) ·ln(1 −p)).
На графике ниже пунктирной линией изображена энтропия. Непрерывная линия –
критерий Джини.
0 1 p
H(p),J(p)
6
Видно, что поведение этих функций очень похоже. Наибольшая неопределённость
достигается в точкеp = 0.5. На практике, обычно, нет особой разницы, от того,
какой из двух критериев использовать для обучения дерева.
■
3 Связь критериев разбиения с функциями потерь
При построении решающего дерева необходимо задатьфункционал качества,
на основе которого осуществляется разбиение выборки на каждом шаге. Этот функ-
ционал определяет, какой именно предикат лучше всего выбрать для данной внут-
ренней вершины. Обозначим черезRm множество объектов, попавших в вершину,
разбиваемую на данном шаге, а черезRℓ и Rr /emdash.cyr объекты, попадающие в левое и
правое поддерево соответственно при заданном предикате. Мы будем использовать
функционалы следующего вида:
Q(Rm,j,s ) =H(Rm) −|Rℓ|
|Rm|H(Rℓ) −|Rr|
|Rm|H(Rr).
ЗдесьH(R) /emdash.cyr этокритерий информативности(impurity criterion), который оцени-
вает качество распределения целевой переменной среди объектов множестваR. Чем
меньше разнообразие целевой переменной, тем меньше должно быть значение кри-
терия информативности /emdash.cyr и, соответственно, мы будем пытаться минимизировать
его значение. Функционал качестваQ(Rm,j,s ) мы при этом будем максимизировать.
В каждом листе дерево будет выдавать константу /emdash.cyr вещественное число, ве-
роятность или класс. Исходя из этого, можно предложить оценивать качество мно-
жества объектовRтем, насколько хорошо их целевые переменные предсказываются
константой (при оптимальном выборе этой константы):
H(R) = min
c∈Y
1
|R|
∑
(xi,yi)∈R
L(yi,c),
гдеL(y,c) /emdash.cyr некоторая функция потерь.
§3.1 Регрессия
Как обычно, в регрессии выберем квадрат отклонения в качестве функции по-
терь. В этом случае критерий информативности будет выглядеть как
H(R) = min
c∈Y
1
|R|
∑
(xi,yi)∈R
(yi −c)2.
Как известно, минимум в этом выражении будет достигаться на среднем значении
целевой переменной. Значит, критерий можно переписать в следующем виде:
H(R) = 1
|R|
∑
(xi,yi)∈R

yi − 1
|R|
∑
(xj ,yj )∈R
yj


2
.
7
Мы получили, что информативность вершины измеряется её дисперсией /emdash.cyr чем ниже
разброс целевой переменной, тем лучше вершина. Разумеется, можно использовать
и другие функции ошибкиL /emdash.cyr например, при выборе абсолютного отклонения мы
получим в качестве критерия среднее абсолютное отклонение от медианы.
§3.2 Классификация
Обозначим черезpk долю объектов классаk (k ∈{1,...,K }), попавших в вер-
шину R:
pk = 1
|R|
∑
(xi,yi)∈R
[yi = k].
Черезk∗ обозначим класс, чьих представителей оказалось больше всего среди объ-
ектов, попавших в данную вершину:k∗= arg max
k
pk.
3.2.1 Ошибка классификации
Рассмотрим индикатор ошибки как функцию потерь:
H(R) = min
c∈Y
1
|R|
∑
(xi,yi)∈R
[yi ̸= c].
Легко видеть, что оптимальным предсказанием тут будет наиболее популярный
классk∗ /emdash.cyr значит, критерий будет равен следующей доле ошибок:
H(R) = 1
|R|
∑
(xi,yi)∈R
[yi ̸= k∗] = 1−pk∗.
Данный критерий является достаточно грубым, поскольку учитывает часто-
туpk∗ лишь одного класса.
3.2.2 Энтропийный критерий
Для классификации в качестве функции потерь мы обычно использовали ло-
гистическую функцию потерь. Давайте выведем для неё оптимальный константный
прогноз:
H(R) = min∑
k ck=1

− 1
|R|
∑
(xi,yi)∈R
K∑
k=1
[yi = k] logck

.
Для вывода оптимальных значенийck вспомним, что все значенияck должны сум-
мироваться в единицу. Как известного из методов оптимизации, для учёта этого
ограничения необходимо искать минимум лагранжиана:
L(c,λ) =− 1
|R|
∑
(xi,yi)∈R
K∑
k=1
[yi = k] logck + λ
K∑
k=1
ck →min
ck
Дифференцируя, получаем:
∂
∂ck
L(c,λ) =− 1
|R|
∑
(xi,yi)∈R
[yi = k] 1
ck
+ λ= −pk
ck
+ λ= 0,
8
откуда выражаемck = pk/λ. Суммируя эти равенства поk, получим
1 =
K∑
k=1
ck = 1
λ
K∑
k=1
pk = 1
λ,
откудаλ= 1. Значит, минимум достигается приck = pk, как и в предыдущем случае.
Подставляя эти выражения в критерий, получим, что он будет представлять собой
энтропию распределения классов:
H(R) =−
K∑
k=1
pk log pk.
Из теории вероятностей известно, что энтропия ограничена снизу нулем, при-
чем минимум достигается на вырожденных распределениях (pi = 1, pj = 0дляi̸= j).
Максимальное же значение энтропия принимает для равномерного распределения.
Отсюда видно, что энтропийный критерий отдает предпочтение более /guillemotleft.cyrвырожден-
ным/guillemotright.cyr распределениям классов в вершине.
На всякий случай докажем утверждение про максимум энтропии.
Задача 3.1.Покажите, что энтропия ограничена сверху и достигает своего макси-
мума на равномерном распределенииp1 = ··· = pK = 1/K.
Решение.Нам понадобится неравенство Йенсена: для любой вогнутой функцииf
выполнено
f
( n∑
i=1
aixi
)
⩾
n∑
i=1
aif(xi),
если∑n
i=1 ai = 1.
Применим его к логарифму в определении энтропии (он является вогнутой
функцией):
H(p) =
K∑
k=1
pk log2
1
pk
⩽log2
( K∑
k=1
pi
1
pi
)
= log2 K.
Наконец, найдем энтропию равномерного распределения:
−
K∑
k=1
1
K log2
1
K = −K 1
K log2
1
K = log2 K.
■
3.2.3 Критерий Джини
Рассмотрим ситуацию, в которой мы выдаём в вершине не один класс, а распре-
деление на всех классахc= (c1,...,c K), ∑K
k=1 ck = 1. Качество такого распределения
можно измерять, например, с помощью критерия Бриера (Brier score):
H(R) = min∑
k ck=1
1
|R|
∑
(xi,yi)∈R
K∑
k=1
(ck −[yi = k])2.
9
Легко заметить, что здесь мы, по сути, ищем каждыйck как оптимальную
с точки зрения MSE константу, приближающую индикаторы попадания объектов
выборки в классk. Это означает, что оптимальный вектор вероятностей состоит из
долей классовpk:
c∗= (p1,...,p K)
Если подставить эти вероятности в исходный критерий информативности и провести
ряд преобразований, то мы получим критерий Джини:
H(R) =
K∑
k=1
pk(1 −pk).
Задача 3.2.Иногда критерий Джини записывают в виде
H(R) =
∑
k̸=k′
pkpk′.
Покажите, что эта запись эквивалентна нашему определению.
Решение.
∑
k̸=k′
pkpk′ =
K∑
k=1
pk
∑
k′̸=k
pk′ =
K∑
k=1
pk(1 −pk).
■
Выясним теперь, какой смысл имеет максимизация функционала качества, ос-
нованного на критерии Джини. Сразу выбросим из критерияH(Rm), поскольку дан-
ная величина не зависит отj и s. Обозначим долю объектов классаk в вершинеm
черезpmk. Преобразуем критерий:
−|Rℓ|
|Rm|H(Rℓ) −|Rr|
|Rm|H(Rr) =− 1
|Rm|
(
|Rℓ|−
K∑
k=1
p2
ℓk|Rℓ|+ |Rr|−
K∑
k=1
p2
rk|Rr|
)
=
= 1
|Rm|
( K∑
k=1
p2
ℓk|Rℓ|+
K∑
k=1
p2
rk|Rr|−|Rm|
)
= {|Rm|не зависит отj и s}=
=
K∑
k=1
p2
ℓk|Rℓ|+
K∑
k=1
p2
rk|Rr|.
Запишем теперь в наших обозначениях число таких пар объектов(xi,xj), что
оба объекта попадают в одно и то же поддерево, и при этомyi = yj. Число объ-
ектов классаk, попавших в поддеревоℓ, равноpℓk|Rℓ|; соответственно, число пар
объектов с одинаковыми метками, попавших в левое поддерево, равно∑K
k=1 p2
ℓk|Rℓ|2.
Интересующая нас величина равна
K∑
k=1
p2
ℓk|Rℓ|2 +
K∑
k=1
p2
rk|Rr|2. (3.1)
Заметим, что данная величина очень похожа на полученное выше представление
для функционала Джини. Таким образом, максимизацию критерия Джини можно
условно интерпретировать как максимизацию числа пар объектов одного класса,
оказавшихся в одном поддереве. Более того, иногда функционал Джини определяют
именно через выражение (3.1).
10
4 Обучаем дерево
Задача 4.1.Винни-Пух регулярно ходит в гости к кролику и ест мёд. Иногда он
съедает слишком много и застревает в двери. Кролик собрал выборку из числа горш-
ков съеденного мёда,x и фактов застревания в двери,y.
xi 1 4 2 3 3 1
yi 0 1 1 0 1 0
Помогите Кролику построить дерево для классификации. В качестве крите-
рия информативности используется критерий Джини. Дерево строится до глубины
равной трём.
Решение.Функционал для оптимизации вышлядит как
Q(Rm,j,s ) =H(Rm) −|Rℓ|
|Rm|H(Rℓ) −|Rr|
|Rm|H(Rr) →max
j,s
.
Можно переписать его для рассматриваемой пары листьев как
|Rℓ|
|Rm|H(Rℓ) + |Rr|
|Rm|H(Rr) →min
j,s
.
В качествеH(R) мы берём критерий Джини. Посчитаем неопределённость в
корне до первого разбиения
H(R0) =J(R0) = 1−
(1
22 + 1
22
)
= 0.5
Обычно в качестве точек для разбиения рассматривают величиныγk = xk+xk+1
2 .
У нас есть три точки для разбиения.
xi 1 1 | 2 3 3 4
yi 0 0 | 1 0 1 1
xi 1 1 2 | 3 3 4
yi 0 0 1 | 0 1 1
xi 1 1 2 3 3 | 4
yi 0 0 1 0 1 | 1
Посчитаем значение функционала качества для каждого из разбиений. Если
разбиение будет сделано по порогу1+2
2 = 1.5, в первую вершину попадут объекты
только нулевого класса. В левой вершине не будет никакой неопределенности, крите-
рий Джини будет равен нулю. В правой вершине четверть объектов будет нулевого
класса. Чтобы посчитать информативность разбиения, нам надо на последнем шаге
взвесить оба листа на размеры их подвыборок
J1(RL) = 1−(12) = 0
J1(RR) = 1−((3/4)2 + (1/4)2) = 0.375
J(R1) = 2/6 ·0 + 4/6 ·0.375 = 0.25.
11
По аналогии посчитаем информативность разбиения, сделанного по порогу
2+3
2 = 2.5
J1(RL) = 1−(1/32 + (2/3)2) = 0.44
J1(RR) = 1−(1/32 + (2/3)2) = 0.44
J(R1) = 1/2 ·0.44 + 1/2 ·0.44 = 0.44.
Остаётся порог3+4
2 = 3.5
J1(RL) = 1−((2/5)2 + (3/5)2) = 12/25
J1(RR) = 0
J(R1) = 5/6 ·12/25 + 1/6 ·0 = 0.4.
Самое маленькое значениеJ(R1) нам даёт первое разбиение. Оно же даст нам
самое большое значение
J(R0) −J(R1) =J(R0) −|Rℓ|
|R0|H(Rℓ) −|Rr|
|R0|H(Rr).
Выходит, что еслиx> 1.5, мы прогнозируем1, если меньше, тогда0.
x> 1.5
0 1
В правом листе все объекты нулевого класса. Продолжить растить дерево мы можем
только из левого листа. В рамках него мы можем сделать два разбиения
xi 2 | 3 3 4
yi 1 | 0 1 1
xi 2 3 3 | 4
yi 1 0 1 | 1
Они оба обладают одинаковой информативностью, поэтому выберем их в любом
порядке. Итоговое дерево имеет вид
x> 1.5
0 x> 2.5
1 x> 3.5
? 1
В нашем примере не очень понятно, что прогнозировать если мы оказываем в
листе, соответствующемуx = 3. Можно попробовать прогнозировать1 с вероятно-
стью0.5.
■
12
5 Дивергенция Кульбака-Лейблера
Дивергенция Кульбака-Лейблера тесно связана с энтропией. Давайте проин-
терпретируем её в том же контексте, что и энтропию. Пусть распределениеp – это
то, как Женя загадывает величинуX, а распределениеq – это моё мнение о нём.
X бустинглогрег EM-алгоритм калик
p 0.5 0.25 0.125 0.125
q 0.125 0.125 0.25 0.5
Отгадывающий не знает распределенияp и задаёт вопросы поq.
Кросс-энтропиейCE(p||q) называют среднее количество вопросов, которое
необходимо задать, если я использую для угадывания распределениеq, а в реаль-
ность варианты загадываются по распределениюp.
CE(p||q) =−Ep(ln q(v)) =−
∑
v
p(v) ·ln q(v)
Дивергенцией Кульбака-ЛейблераKL(p||q) называются количество лишних во-
просов, которые мне пришлось задать Жене
KL(p||q) = CE(p||q) −H(p) = Ep(−ln q(v)) −Ep(−ln p(v)) =
∑
v
p(v) ·ln p(v)
q(v)
