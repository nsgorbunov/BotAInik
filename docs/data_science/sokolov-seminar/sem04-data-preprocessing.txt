Машинное обучение, ФКН ВШЭ
Семинар №4
Предобработка данных
1 Пропущенные значения
В реальных задачах значения некоторых признаков у некоторы х объектов от-
сутствуют . Это может происходить по разным причинам: ошибк и при записи данных,
отказ респондента отвечать на вопрос, невозможность описа ть конкретное свойство
у конкретного объекта (в таблице с данными об автомобилях бу дут пропуски у элек-
тромобилей в графе /guillemotleft.cyrобъём топливного бака/guillemotright.cyr). Многие алгоритмы машинного обуче-
ния (в частности, линейная регрессия) не могут работать с пр опущенными данными,
поэтому эти пропуски необходимо заполнить.
Заполнять пропуски у объектов можно различными способами:
1. Константным уникальным значением /emdash.cyr неудачный вариант для линейных ме-
тодов (модель начнёт считать пропуск близким к некоторому д ругому значению
выборки), но быстрый и популярный способ с другими алгоритм ами в машин-
ном обучении.
2. Средним арифметическим, медианой, модой /emdash.cyr сохранение статистик выборки,
но потеря информации о наличии пропуска в данных.
3. Предсказаниями другого алгоритма /emdash.cyr затратно по времени (однако всё равно не
приносит новой информации в датасет , хотя и может положител ьно сказаться
на общем качестве).
Заметим, что в некоторых случаях наличие пропуска в данных н есёт определён-
ную информацию об объекте (например, отказ в ответе на вопро с о доходах клиента
банка), поэтому полезно добавлять новые признаки /emdash.cyr индикаторы пропусков. Иногда
признаки содержат слишком много пропусков и их выгоднее уда лить.
Добавление индикаторов пропусков даёт интересную возможн ость. Допустим,
у нас встречаются пропуски в признаке x1, и мы добавляем второй признак с соот-
ветствующим индикатором:
a(x) =w0 + w1x1 + w2[был пропуск в x1] +. . .
Заметим, что если мы заменяем пропуски в x1 на какую-то константу , то w2, по
сути, может скорректировать эту константу . Получается, чт о добавление индикатора
позволяет выучить оптимальную константу для замены пропус ков.
1
2
2 Выбросы
На практике могут встречаться объекты, сильно отличающиес я от остальных.
Их называют выбросами. Отличия могут выражаться как в значе ниях признаков, так
и в целовой величине. Причины бывают различными: ошибки в за полнении данных
(добавили лишний ноль), /guillemotleft.cyrисключительность/guillemotright.cyr отдельных объектов (низкие цены на
дома могут быть связаны с попыткой обхода налогов, а не с их ха рактеристиками).
Выбросы могут сильно сказываться на решении /emdash.cyr например, квадратичная функ-
ция ошибок /guillemotleft.cyrреагирует/guillemotright.cyr на выбросы и линейная регрессия с таким функционалом
ошибки отклоняется в их сторону , в отличие от модели, оптими зирующей среднюю
абсолютную ошибку (рис. 1).
Рис. 1. Влияние выбросов на обученную линейную регрессию для MSE и MAE в качестве функции
потерь.
Искать выбросы можно следующим образом. Выбросы в признака х можно обна-
ружить, исследуя распределение признаков и в особенности х восты распределений.
Выбросы в целевой величине можно искать, считая ошибку пред сказания модели
на объектах обучающей выборки (вспомогательная модель не д олжна наблюдать
при обучении проверяемый объект). Если ошибка велика (алго ритм с уверенностью
3
предсказывает отрицательный класс, хотя метка у объекта по ложительная), то объ-
ект можно считать выбросом (если, конечно, дело не в плохой м одели). Объекты-
выбросы чаще всего не корректируют , а удаляют из выборки.
Заметим, что не всегда необходимо удалять объекты-выбросы из выборки. В
одном конкурсе помог следующий подход: оставить выбросы, чтобы не изменил ось
среднее предсказание алгоритма, при этом качество модели с читать только по /guillemotleft.cyrнор-
мальным/guillemotright.cyr объетам, чтобы исключить шум от объектов-выбросов.
Как уже было сказано выше, некоторые функции потерь чувстви тельнее от-
носятся в выбросам, поэтому в таких ситуациях имеет смысл ис пользовать более
устойчивые функции потерь для обучения моделей.
3 Обработка категориальных признаков
Часто в данных встречаются категориальные по смыслу призна ки. Если они
представлены в виде чисел, то, подавая напрямую в модель, мы задаём порядок
над этими категориями, что обычно неправильно. Например, е сли красный цвет ко-
дировался как /guillemotleft.cyr1/guillemotright.cyr , зелёный /emdash.cyr /guillemotleft.cyr2/guillemotright.cyr , а синий /emdash.cyr /guillemotleft.cyr3/guillemotright.cyr , то модель будет считать зелёный
цвет находящимся ровно между красным и синим. Если же катего риальный признак
представлен в выборке не в виде чисел, то мы и вовсе не можем ис пользовать его
для обучения модели. Изучим способы кодирования категориа льных признаков.
§3.1 Label encoding
В простом случае, если категориальный признак представлен в виде нечисло-
вых данных, можно построить обратимое отображение для кажд ого уникального
значения в некоторое число. Это позволит использовать приз нак для обучения мо-
дели. Например, зелёному цвету будет соответствовать /guillemotleft.cyr1/guillemotright.cyr ,красному /emdash.cyr /guillemotleft.cyr2/guillemotright.cyr и так
далее.
У этого подхода две основные проблемы: задание порядка над к атегориями и
работа с неизвестными в процессе обучения значениями катег ориального признака
(нужно не забывать обрабатывать такой случай отдельно).
§3.2 One-hot encoding
Другой способ кодирования заключается в добавлении призна ков-индикаторов
категориальных значений. Например, появятся новые бинарн ые признаки: /guillemotleft.cyrкрасный
цвет/guillemotright.cyr , /guillemotleft.cyrзелёный цвет/guillemotright.cyr и так далее. В этом случае решается проблема с заданием
порядка над категориями и новыми значениями категориально го признака (у такого
объекта просто будут /guillemotleft.cyrнули/guillemotright.cyr по всем признакам индикаторам).
Проблема этого подхода в увеличении количества признаков ( а это затрачива-
емая память и скорость обучения модели) пропорционально ко личеству категорий.
Эти разреженные признаки допускают хранение в виде разреже нных матриц, но
только некоторые алгоритмы умеют работать с ними. Т акже мож но для экономии
памяти не создавать признаки для редко встречающихся катег орий.
4
§3.3 Кодирование с учётом целевой переменной
Более сложный метод кодирования категориальных признаков /emdash.cyr через целевую
переменную. Идея в том, что алгоритму для предсказания цены необходимо знать
не конкретный цвет автомобиля, а то, как этот цвет сказывает ся на цене. Разберём
сначала базовый подход /emdash.cyr mean target encoding (иногда его называют /guillemotleft.cyrсчётчиками/guillemotright.cyr).
Заменим каждую категорию на среднее значение целевой перем енной по всем
объектам этой категории. Пусть j-й признак является категориальным. Для бинар-
ной классификации новый признак будет выглядеть следующим образом:
gj(x, X) =
∑ ℓ
i=1[fj(x) =fj(xi)][yi = +1]
∑ ℓ
i=1[fj (x) =fj(xi)]
, (3.1)
где fj(xi) – j-й признак i-го объекта, yi – класс i-го объекта.
Отметим, что эту формулу легко перенести как на случай много классовой клас-
сификации (в этом случае будем считать K признаков, по одному для каждого клас-
са, и в числителе будет подсчитывать долю объектов с заданно й категорией и с
заданным классом), так и на случай регрессии (будем вычисля ть среднее значение
целевой переменной среди объектов данной категории).
Вернёмся к бинарной классификации. Можно заметить, что для редких катего-
рий мы получим некорректные средние значения целевой перем енной. Например, в
выборке было только три золотистых автомобиля, которые ока зались старыми и де-
шёвыми. Из-за этого наш алгоритм начнёт считать золотистый цвет дешёвым. Для
исправления этой проблемы можно регуляризировать признак средним значением
целевой переменной по всем категориям так, чтобы у редких ка тегорий значение бы-
ло близко к среднему по всей выборке, а для популярных к средн ему значению по
категории. Формально для задачи бинарной классификации эт о выражается так:
gj(x, X) =
∑ ℓ
i=1[fj(x) =fj(xi)][yi = +1] +C
ℓ
∑ ℓ
i=1[yi = +1]
∑ ℓ
i=1[fj (x) =fj(xi)] +C
, (3.2)
где C – коэффициент , отвечающий за баланс между средним значение м по категории
и глобальным средним значением.
Однако если мы вычислим значения gj(x, X) по всей выборке, то столкнёмся
с переобучением, так как мы внесли информацию о целевой пере менной в призна-
ки (новый признак слабая, но модель, предсказывающая целев ое значение). Поэто-
му вычисление таких признаков следует производить по блока м, то есть вычислять
средние значения на основе одних фолдов для заполнения на др угом блоке (анало-
гично процессу кросс-валидации). Если же ещё планируется о ценка качества модели
с помощью кросс-валидации по блокам, то придётся применить /guillemotleft.cyrдвойную кросс-
валидацию/guillemotright.cyr для подсчёта признаков. Этот подход заключается в кодировании кате-
гориальных признаков по /guillemotleft.cyrвнутренним/guillemotright.cyr блокам внутри /guillemotleft.cyrвнешних/guillemotright.cyr блоков, по кото-
рым оценивается качество модели.
Разберём этот процесс (иллюстрация на рис.
2). Представим, что хотим посчи-
тать качество модели на 3-м блоке. Для этого:
1. Разбиваем все внешние блоки, кроме 3-го, на внутренние бл оки. Количество
внутренних блоков может не совпадать с количеством внешних (на иллюстра-
ции их также 3).
5
2. Рассмотрим конкретный внешний блок. Для каждого из его вн утренних блоков
считаем значение g(x, X) на основе средних значений целевой переменной по
блокам, исключая текущий. Для 3-го внешнего блока (который сейчас играет
роль тестовой выборки) вычисляем g(x, X) как среднее вычисленных признаков
по каждому из внутренних фолдов.
3. Обучаем модель на всех блоках, кроме 3-го, делаем предска зание на 3-м и счи-
таем на нём качество.
Рис. 2. Кросс-валидация при кодировании средним значением
Существуют альтернативы кодированию категориальных приз наков по блокам.
Зашумление. Можно посчитать новые признаки по базовой формуле ( 3.1), а затем
просто добавить к каждому значению случайный шум (например , нормальный). Это
действительно снизит уровень корреляции счётчиков с целев ой переменной. Пробле-
ма в том, что это делается за счёт снижения силы такого призна ка, а значит , мы
ухудшаем итоговое качество модели. Поэтому важно подбират ь дисперсию шума,
чтобы соблюсти баланс между борьбой с переобучением и предс казательной силой
счётчиков.
Сглаживание. Можно немного модифицировать формулу ( 3.2), чтобы сила регуля-
ризации зависела от объёма данных по конкретной категории:
gj(x, X) =λ (n(fj (x)))
∑ ℓ
i=1[fj (x) =fj(xi)][yi = +1]
∑ ℓ
i=1[fj (x) =fj (xi)]
+(1 − λ (n(fj (x)))) 1
ℓ
ℓ∑
i=1
[yi = +1],
где n(z) = ∑ ℓ
i=1[fj(xi) = z] /emdash.cyr число объектов категорииz, λ(n) /emdash.cyr некоторая моно-
тонно возрастающая функция, дающая значения из отрезка [0, 1]. Примером может
служить λ(n) = 1
1+exp(− n) . Если грамотно подобрать эту функцию, то она будет вы-
чищать значение целевой переменной из редких категорий и ме шать переобучению.
6
Кодирование по времени. Можно отсортировать выборку некоторым образом и
для i-го объекта вычислять статистики только по предыдущим объе ктам:
gj(xk, X ) =
∑ k− 1
i=1 [fj(x) =fj (xi)][yi = +1]∑ k− 1
i=1 [fj(x) =fj(xi)]
.
Для хорошего качества имеет смысл отсортировать выборку сл учайным образом
несколько раз, и для каждого такого порядка посчитать свои с чётчики. Это даёт
улучшение качества, например, потому что для объектов, нах одящихся в начале вы-
борки, признаки будут считаться по очень небольшой подвыбо рке, и поэтому вряд ли
будут хорошо предсказывать целевую переменную. При наличи и нескольких переста-
новок хотя бы один из новых признаков будет вычислен по подвы борке достаточного
размера. Т акой подход, например, используется в библиотек е CatBoost.
Weight of Evidence. Существует альтернативный способ кодирования категориал ь-
ных признаков, основанный на подсчёте соотношения положит ельных и отрицатель-
ных объектов среди объектов данной категории. Этот подход, в отличие от других,
сложнее обобщить на многоклассовый случай или для регресси и.
Введём обозначение для доли объектов класса b внутри заданной категории c
среди всех объектов данного класса в выборке:
P (c |y = b) =
∑ ℓ
i=1[yi = b][fj (xi) =c] +α
∑ ℓ
i=1[yi = b] + 2α
,
где α /emdash.cyr параметр сглаживания. Вычислим новое значение признака как
gj(x, X) = log
( P (fj(x) |y = +1)
P (fj(x) |y = − 1)
)
.
Если gj(x, X) близко к нулю, то данная категория примерно с одинаковой вер оятно-
стью встречается в обоих классах, и поэтому вряд ли будет хор ошо предсказывать
значение целевой переменной. Чем сильнее отличие от нуля, т ем сильнее категория
характеризует один из классов.
Разумеется, в конкретной задаче лучше всего может себя проя влять любой из
этих подходов, поэтому имеет смысл сравнивать их и выбирать лучший /emdash.cyr или ис-
пользовать все сразу .
4 Извлечение признаков из текстов
При решении некоторых задач мы сталкиваемся с тем, что объек ты выборки
целиком или частично описываются в виде текстов (под текста ми имеем в виду стро-
ки, содержащие как минимум пару слов, иначе такой признак мо жно рассматривать
как категориальный признак). Поэтому стоит задача предста вления текста в виде
векторов чисел фиксированной длины.
7
§4.1 Bag-of-words
Простой способ заключается в подсчёте, сколько раз встрети лось каждое слово
в тексте. Получаем вектор длиной в количество уникальных сл ов, встречающихся во
всех объектах выборки. В таком векторе много нулей, поэтому его удобнее хранить
в разреженном виде. Т акой способ представления текстов наз ывают мешком слов.
§4.2 TF-IDF
Очевидно, что не все слова полезны в задаче прогнозирования . Например, мало
информации несут слова, встречающиеся во всех текстах. Это могут быть как стоп-
слова, так и слова, свойственные всем текстам выборки (в тек стах про автомобили
употребляется слово /guillemotleft.cyrавтомобиль/guillemotright.cyr). Эту проблему решает TF-IDF преобразование
текста. Вычисляются две величины:
TD (T erm Frequency) /emdash.cyr количество вхождений слова в отношении к общему чис-
лу слов в тексте:
tf(t, d) = ntd∑
t∈ d ntd
,
где ntd /emdash.cyr количество вхождений словаt в текст d.
IDF (Inverse Document Frequency):
idf(t, D) = log |D|
|{d ∈ D : t ∈ d}|,
где |{d ∈ D : t ∈ d}|/emdash.cyr количество текстов в коллекции, содержащих словоt.
Т огда для каждой пары (слово, текст) (t, d) вычислим величину:
tf-idf(t, d, D) =tf(t, d) ·idf(t, D).
Это и будем значением нового признака (вместо количества ка ждого слова в
тексте в случае мешка слов). Отметим, что значение tf (t, d) корректируется для часто
встречающихся общеупотребимых слов при помощи значения id f(t, D).
§4.3 N-граммы
На практике каждое слово в языке может иметь несколько смысл ов, а мы в
изученных выше подходах даже не учитываем их порядок. Чтобы передать модели
больше информации, можно передавать не только отдельные сл ова, но и их словосо-
четания или n-граммы. N-граммы /emdash.cyr последовательность изn подряд идущих слов
текста. Последовательности накладываются друг на друга.
Например, из предложения “Ты же знаешь, что Максим строго пр оверяет ра-
боты” получатся следующие 3-граммы: “ты же знаешь”, “же зна ешь что”, “знаешь
что Максим”, “что Максим строго”, “Максим строго проверяет ”, “строго проверяет
работы”.
Каждая n-грамма для мешка слов считается отдельным словом. На практ ике
часто берут n-граммы сразу разных размеров, (например, от 1 до 3). Замети м, что
размер мешка слов растёт экспоненциально с ростом n.
8
§4.4 Лемматизация и стемминг
Заметим, что одно и то же слово может встречаться в различных формах (осо-
бенно для русского языка), но описанные выше методы интерпр етируют их как раз-
личные слова, что делает признаковое описание избыточным. У странить эту пробле-
му можно при помощи лемматизации и стемминга.
Стемминг /emdash.cyr это процесс нахождения основы слова. В результате применения
данной процедуры однокоренные слова, как правило, преобра зуются к одинаковому
виду . Например, вагон /emdash.cyr вагон, вагонов /emdash.cyr вагон и важная /emdash.cyr важн, важно /emdash.cyr важн.
Лемматизация /emdash.cyr процесс приведения слова к его нормальной форме (лемме):
• для существительных /emdash.cyr именительный падеж, единственное число;
• для прилагательных /emdash.cyr именительный падеж, единственное число, мужской род;
• для глаголов, причастий, деепричастий /emdash.cyr глагол в инфинитиве.
Лемматизация /emdash.cyr процесс более сложный по сравнению со стеммингом. Стеммер
просто /guillemotleft.cyrрежет/guillemotright.cyr слово до основы. Реализация лемматизаторови стеммеров можно
найти в различных библиотеках (nltk, pymorphy).
