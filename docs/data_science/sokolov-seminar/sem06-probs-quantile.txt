Машинное обучение, ФКН ВШЭ
Семинар №6
Грубо говоря, к машинному обучению есть два подхода: инженерный и веро-
ятностный. В предыдущих лекциях мы с вами активно строили функции потерь с
помощью инженерного подхода.
В случае классификации естественно было бы минимизировать долю непра-
вильных ответов, но она недифференцируема. Поэтому мы свели задачу к оптими-
зации гладкого функционала и придумали несколько верхних оценок. Одной из них
была логистическая функция потерь.
В случае регрессии мы обсудили среднеквадратичные потери. Оказалось, что
они чувствительны к выбросам. Средние абсолютные потери нечувствительны к вы-
бросам, но из-за модуля оптимизировать эту функцию в окрестности оптимума слож-
нее. Мы скрестили среднеквадратичные потери и абсолютные потери. Получилась
функция потерь Хубера.У функции потерь Хубера есть недостаток. Её вторая про-
изводная имеет разрывы. Возникла идея придумать гладкую функцию, похожую на
функцию Хубера. Так родился Log-Cosh.
Подобное инженерное мышление помогает придумывать хорошие функции по-
терь под различные задачи. Иногда можно попробовать поанализировать функции
потерь с помощью вероятностного подхода и немного лучше понять, как именно они
работают. В этом семинаре мы попытаемся понять, что именно прогнозируют моде-
ли, когда мы используем ту или иную функцию потерь.
1 Эмпирические и теоретические потери
Когда мы обучаем модель, мы считаем ошибку на обучающей выборке и мини-
мизируем её по параметрам моделиa(x)
1
ℓ
ℓ∑
i=1
L(yi,a(xi)) →min
w
В зависимости от того, какая выборка оказалась у нас в руках, могут полу-
чаться разные оценки параметров модели. При разных обучающих выборках, эмпи-
рические потери могут принимать разные значения при одних и тех же параметрах
модели. Эмпирическая функция потерь /emdash.cyr это случайная величина. На самом деле,
нам хотелось бы оптимизировать математическое ожидание ошибки
E(L(y,a(x)) |x).
1
2
Мы его не знаем, поэтому для его оценки мы используем эмпирические поте-
ри. Почему можно это делать? Эмпирические потери /emdash.cyr это выборочное среднее. По
закону больших чисел оно должно при больших значенияхℓ сходиться к математи-
ческому ожиданию.
Ошибка на обучающей выборке1
ℓ ·∑ℓ
i=1 L(yi,a(xi)) /emdash.cyr это эмпирическая оценка
ожидаемых потерьE(L(y,a(x)) |x). Этот факт позволяет по-новому взглянуть на
старые функции потерь. МинимизируяE(L(y,a) |x) по прогнозам,a, можно понять,
что именно прогнозирует наш алгоритм.
Пусть выборка пришла к нам из какого-то распределения. Будем считать, что
в каждой точкеx ∈ X пространства объектов задано вероятностное распределе-
ниеp(y|x) на возможных ответах для данного объекта.
Такоераспределениеможетвозникать,например,взадачепредсказаниякликов
по рекламным баннерам: один и тот же пользователь может много раз заходить
на один и тот же сайт и видеть данный баннер; при этом некоторые посещения
закончатся кликом, а некоторые /emdash.cyr нет.
2 Что предсказывают модели регрессии
Задача 2.1.Пусть для оптимизации мы используемMSE, то естьL(y,a(x)) =
= (y−a(x))2.Покажите, что оптимальным прогнозом в таком случае будет условное
математическое ожиданиеE(y|x).
Решение.Запишем математическое ожидание потерь
E[L(y,a(x)) |x] =E
[
(y−a(x))2 |x
]
=
∫ +∞
−∞
(y−a(x))2p(y|x)dy.
Будем перебирать все возможные значения прогнозаa так, чтобы минимизи-
ровать математическое ожидание функции потерь
∫ +∞
−∞
(y−a)2p(y|x)dy→min
a
.
Найдём производную поa и приравняем её к нулю
∂
∂a
(∫ +∞
−∞
(y−a)2 ·p(y|x)dy
)
= −2 ·
∫ +∞
−∞
(y−a) ·p(y|x)dy= 0
∫ +∞
−∞
y·p(y|x)dy−
∫ +∞
−∞
a·p(y|x)dy= E(y|x) −a·1 = 0⇒a= E(y|x).
Получается, что при квадратичных потерях, оптимальным прогнозом будет
условное математическое ожидание. Из-за этого алгоритмы, которые обучаются на
квадратичные потери, чувствительны к выбросам. Одно большое значение довольно
сильно искажает среднее.
■
Давайте вспомним, как брать производную от интеграла переменным предела-
ми интегрирования
3
d
dα
∫ b(α)
a(α)
f(t,α)dt=
∫ b(α)
a(α)
df(t,α)
dα dt+ f(b(α),α) ·db(α)
dα −f(a(α),α) ·da(α)
dα .
Производная берётся поα. Пределы интегрирования зависят отα. Из-за этого
возникают два дополнительных слагаемых. В следующих задачах прогнозa бедут
присутствовать в пределах интегрирования, ноf(b(α),α) =f(a(α),α) = 0.
Задача 2.2.Пусть для оптимизации мы используемMAE, то естьL(y,a(x)) =
= |y−a(x)|. Покажите, что оптимальным прогнозом в таком случае будет условная
медиана.
Решение.Запишем ожидаемые потери
E[L(y,a(x)) |x] =E[|y−a(x)|| x] =
∫ +∞
−∞
|y−a(x)|·p(y|x)dy.
Будем перебирать все возможные значения прогнозаa так, чтобы минимизи-
ровать математическое ожидание функции потерь
∫ +∞
−∞
|y−a|·p(y|x)dy→min
a
.
Найдём производную поa и приравняем её к нулю. При этом, не будем забы-
вать, что в нуле модуль не дифференцируется. Вероятность того, что непрерывная
случайная величина попадёт в конкретную точку, равна нулю. Поэтому мы можем
переписать нашу задачу как
∫
y>a
(y−a) ·p(y|x)dy+
∫
y<a
(a−y) ·p(y|x)dy→min
a
.
Возьмём производную поa и приравняем её к нулю
∂
∂a
(∫
y>a
(y−a) ·p(y|x)dy−
∫
y<a
(a−y) ·p(y|x)dy
)
=
=
∫
y<a
p(y|x)dy−
∫
y>a
p(y|x)dy= 0.
Получается,чтодляминимизацииожидаемыхпотерьнадо,чтобывыполнилось
равенствоP(y <a|x) =P(y >a|x).Точка, в которой выполняется такое равенство,
называется медианой. Получается, чтоa= Med(y|x).
При абсолютных потерях, оптимальным прогнозом будет условная медиана.
Из-за этого алгоритмы, которые обучаются на абсолютные потери, оказываются ро-
бастными к выбросам.
■
4
3 Квантильная регрессия
Внекоторыхзадачахценызаниженияизавышенияпрогнозовмогутотличаться
друг от друга. Например, при прогнозировании спроса на товары интернет-магазина
гораздо опаснее заниженные предсказания, поскольку они могут привести к потере
клиентов. Завышенные же прогнозы приводят лишь к издержкам на хранение товара
на складе. Функционал в этом случае можно записать как
Q(a,Xℓ) =
ℓ∑
i=1
L(yi,a(xi)),
где
L(yi,a(xi)) =
{
(1 −α) ·(a(xi) −yi), a (xi) >yi
α·(yi −a(xi)), a (xi) ≤yi.
Задача 3.1.Покажите, что оптимальным прогнозом в таком случае будет услов-
ный квантиль уровняα.
Решение.
Запишем ожидаемые потери
E[L(y,a) |x] =
∫ a
−∞
(1 −α) ·(a−y) ·p(y|x)dy+
∫ +∞
a
α·(y−a) ·p(y|x)dy→min
a
.
Возьмём производную и приравняем её к нулю
∂E[L(y,a) |x]
∂a = (1−α)
∫ a
−∞
p(y|x)dy−α
∫ +∞
a
p(y|x)dy= 0.
Перепишем это в терминах вероятностей
(1 −α) ·P(y≤a|x) =α·(1 −P(y≤ˆy|x)).
Решив это уравнение, получаемP(y ≤a |x) = α. Полученное уравнение /emdash.cyr
это определение квантиля уровняα. Именно этот квантиль и будет оптимальным
прогнозомa.
■
4 Предсказание вероятностей
Разберемся, каким требованиям должен удовлетворять классификатор, чтобы
его выход можно было расценивать как оценку вероятности класса.
Пусть в каждой точкеx∈X пространства объектов задана вероятностьp(y =
= +1|x) того, что данный объект относится к классу+1, и пусть алгоритмb(x)
возвращает числа из отрезка[0,1]. Потребуем, чтобы эти предсказания пытались в
каждой точкеx приблизить вероятность положительного классаp(y= +1|x).
Разумеется, выполнение этого требования зависит от функции потерь /emdash.cyr мини-
мум ее матожидания в каждой точкеx должен достигаться на данной вероятности:
arg min
b∈R
E[L(y,b)|x] =p(y= +1|x).
5
Задача 4.1.Покажите, что квадратичная функция потерьL(y,b) = ([y= +1]−b)2
позволяет предсказывать корректные вероятности.
Решение.Заметим, что поскольку алгоритм возвращает числа от 0 до 1, то его ответ
должен быть близок к единице, если объект относится к положительному классу, и
к нулю /emdash.cyr если объект относится к отрицательному классу.
Запишем матожидание функции потерь в точкеx:
E[L(y,b)|x] =p(y= +1|x)(b−1)2 + (1−p(y= +1|x))(b−0)2.
Продифференцируем поb:
∂
∂bE[L(y,b)|x] = 2p(y= +1|x)(b−1) + 2(1−p(y= +1|x))b= 2b−2p(y= +1|x) = 0.
Легко видеть, что оптимальный ответ алгоритма действительно равен вероятности:
b= p(y= +1|x).
■
Задача 4.2.Покажите, что абсолютная функция потерьL(y,b) =|[y= +1]−b|, b∈
[0; 1], не позволяет предсказывать корректные вероятности.
Решение.Запишем матожидание функции потерь в точкеx:
E[L(y,b)|x] =p(y= +1|x)|1 −b|+ (1−p(y= +1|x))|b|=
= p(y= +1|x)(1 −b) + (1−p(y= +1|x))b.
Продифференцируем поb:
∂
∂bE[L(y,b)|x] = 1−2p(y= +1|x) = 0.
Рассмотрим 2 случая:
1. p(y= +1|x) =1
2.ТогдаE[L(y,b)|x] =1
2 ∀b∈[0; 1],а потому классификатор не
позволяет предсказывать корректную вероятность в точкеx.
2. p(y= +1|x) ̸= 1
2.В этом случае интервал(0; 1)не содержит критических точек,
а потому минимум матожидания достигается на одном из концов отрезка[0; 1] :
min
b∈[0;1]
E[L(y,b)|x] = min (E[L(y,0)|x] ,E[L(y,1)|x]) =
min (p(y= +1|x),1 −p(y= +1|x)) .
Отсюда arg minb∈[0;1] E[L(y,b)|x] ∈ {0,1}, а потому классификатор также не
позволяет предсказывать корректную вероятность в точкеx.
■
6
5 Калибровка вероятностей
Часто при обучении моделей для бинарной классификации хочется получать
не только предсказанную метку класса, но и вероятность положительного класса.
Предсказаннаявероятностьможетслужитькакмерауверенностинашегоалгоритма.
Однако некоторые алгоритмы не выдают корректные вероятности классов. В таком
случае калибруют вероятности модели.
Для начала определимся с тем, что хотим получить от предсказанных вероят-
ностей. В задаче бинарной классификации откалиброванным алгоритмом называют
такой алгоритм, для которого доля положительных примеров (на основе реальных
меток классов) для предсказаний в окрестности произвольной вероятностиp совпа-
дает с этим значениемp. Например, если взять объекты, для которых предсказанные
вероятности близки к 0.7, то окажется, что среди них 70% принадлежат положитель-
ному классу. Нет критерия, которое бы установило откалиброванность алгоритма,
однако можно построить калибровочную кривую. На этой кривой абсцисса точки
соответствуют значениюp(предсказаний алгоритма), а ордината соответствует доле
положительных примеров, для которых алгоритм предсказал вероятность, близкую
кp. В идеальном случае эта кривая совпадает с прямойy= x. Примеры такой кривой
на рис. (1).
Рис. 1. Калибровочные кривые нескольких алгоритмов
Изучим два стандартных метода для калибровки вероятностей алгоритма: ка-
либровка Платта и изотоническая регрессия.
§5.1 Калибровка Платта
Пусть наш алгоритм выдаёт значенияf(x) (могут не быть вероятностями).
Тогда итоговая вероятность:
7
P(y= 1|x) = 1
1 + exp(af(x) +b),
гдеa,b – скалярные параметры. Эти параметры настраиваются методом мак-
симума правдоподобия (минимизируя логистическую функцию потерь) на отложен-
ной выборке или с помощью кросс валидации. Также Платт предложил настраивать
параметры на обучающей выборке базовой модели, а для избежания переобучения
изменить метки объектов на следующие значения:
t+ = N+ + 1
N−+ 2
для положительных примеров и
t−= 1
N−+ 2
для отрицательных.
Калибровку Платта можно представить как применения логистической регрес-
сии поверх предсказаний другого алгоритма с отключенной регуляризацией.
§5.2 Изотоническая регрессия
В этом методе также строится отображение из предсказаний модели в откалиб-
рованные вероятности. Для этого используем изотоническую функцию (неубываю-
щая кусочно-постоянная функция), в которойx – выходы нашего алгоритма, аy –
целевая переменная. Иллюстрация изотонической регрессии на рис. (2).
Рис. 2. Изотоническая регрессия
8
Мы хотим найти такую функциюm(t): P(y = 1|x) =m(f(x)). Она настраива-
ется под квадратичную ошибку:
m= arg min
z
∑
(yi −z(f(xi))2,
с помощью специального алгоритма (Pool-Adjacent-Violators Algorithm), изу-
чать который в этом курса не будем.
В результате калибровки получаем надстройку над нашей моделью, которая
применяется поверх предсказаний базовой модели. В случае мультиклассовой клас-
сификации каждый класс калибруется отдельно против остальных (one-versus-all),
вероятности при предсказании нормируются.
