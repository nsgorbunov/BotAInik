Введение в автоматическуюобработку текстов
Высшая школа цифровой культуры
Университет ИТМО
dc@itmo.ru
______________________
Оглавление
Введение 4Для кого этот курс 5Что такое NLP, обзор задач 6NLP vs CompLing 7Примеры трудностей в обработке языка 7Краткая и неполная история компьютерной лингвистики 12Докомпьютерная эра 12Отступление: эксперимент с “Онегиным”  и цепи Маркова 12Компьютерная эпоха 1450-е 1660-е и 70-е 1680-е и 90-е 172000-е 172010-е 18Готовим тексты для работы с машиной 18Стемминг 19Стеммер Портера 20Лемматизация 22Морфологический вывод 22Морфологическая неоднозначность 23Инструментарий для русского языка 24Pymorphy2 24Mystem 26RNNMorph 28
2
Введение
Добропожаловатьнавводныйкурспообработкеестественногоязыка.NLP,naturallanguageprocessing-- прикладнаяобластьзнаний,целаянаука, ставшаяособеннопопулярной в последние десять лет.
Подсказкиследующего слова в письмахи сообщенияхв телефонах,определениеязыка и автоматическийпереводнадругойязык,готовыеответывпочте,автоматическаяпроверкаорфографииисинтаксисавтекстовыхредакторах--итакдалее.Этипомощникистали привычнымив повседневнойжизнимногих.Принципамиработы этихинструментов занимается обработка естественного языка.
Какойсмыслслова crazy, как правило,имеют в видуфанатыхоккеяи какой--участникидискуссий о равноправииполов? Как менялсясмыслкачественныхприлагательныхс девятнадцатого века понастоящеевремя?Какиетропыифигурыречихарактерныдлятого илииногописателя?Вопросыприкладнойлингвистикитакжечастоотносят к NLP.
3
О ком чащепишутарт-критикиМосквыи Петербурга,и в чьихоценках онисходятся?Чтоможноузнатьонегативныхпобочныхэффектахтогоилииноголекарствапотекстампациентовв социальныхсетях?Текстывэлектронномвиде--одинизосновныхспособовпредставленияинформациив интернете,поэтомуименнометодыобработкиестественного языка часто используются как основнойинструментв прикладныхмеждисциплинарных исследованиях на основе данных.
Намомент2020годанаука обавтоматическойобработке языка хорошоразвита.Некоторыезадачирешаютсяпросто,адлярешениядругих--приходитсяобращатьсякнесамойпростой математике. К счастью,общедоступныеинструментыстановятся всёудобнее.И,можетбыть,именновысделаетепрорывввашейнаучнойобласти,взявNLPкакметод анализаи автоматизацииработыс большими(инеочень)массивамиданных.Мы будем очень рады, если наш курс вам в этом поможет.
Для кого этот курс
Это вводныйи обзорныйкурс,которыйпокажетлишьнебольшую,но,возможно,самуюважнуючастьтого, что можносделать современнымиметодамиобработкиестественного языка.
Мыориентировалисьнатех,кому, какмынадеемся,ещётолькопредстоитработатьс текстом в научныхкросс-дисциплинарныхилипросто количественныхисследованияхили заниматься анализом данных, например, в коммерческих организациях.
Курсне предполагаетсерьезнойматематической подготовкислушателя,но непредполагаети страха перед, возможно,новымидляслушателямоделями,подходамииидеями.Основыпрограммированиявбагажетакжепомогут, номожноуспешнопонятьисдать курси без них.Главное -- смелость,открытостьновомуи интерес к машиннымметодам работы с языками.
4
Каждуюнеделювам предстоитпрослушать лекции,ответитьна контрольныевопросы и решить задачу. Как правило, это запуск готовых или почти готовых программ.
Что такое NLP, обзор задач
NLP определяют по-разному, единого мнения нет.
[Jurafskyetal,ed.2]Thegoalofthisnewfieldistogetcomputerstoperformusefultasksinvolvinghumanlanguage,taskslikeenablinghuman-machinecommunication,improvinghuman-human communication, or simply doing useful processing of text or speech.
Перевод:Цельэтойновойпредметнойобласти--спомощьюкомпьютероврешатьполезныезадачи,прирешениикоторыхприходитсяиметьделос естественнымязыком.Сюда относятся человеко-машиннаякоммуникация,поддержка коммуникациимеждулюдьми или попросту осмысленная обработка текста и речи.
[Wikipedia2019-08-25]:Naturallanguageprocessing(NLP)isasubfieldoflinguistics,computerscience,informationengineering,andartificialintelligenceconcernedwiththeinteractionsbetweencomputersandhuman(natural)languages,inparticularhowtoprogramcomputers to process and analyze large amounts of natural language data.
Перевод: Обработка естественного языка -- это поднаправлениелингвистики,информатики,управленияинформациейи искусственного интеллекта,врамкахкоторогорассматривается взаимодействиемеждувычислительнымимашинамии человеческимиестественнымиязыками,в частности,методамикомпьютернойобработкии анализабольших объёмов данных на естественных языках.
Вовсех определенияхважно,что это прикладнаяобластьнастыке лингвистики,информатикии искусственного интеллекта. Можно также отметить,что иногда вопределениевключают обработку речи,то естьзвуков.На нашвзгляд,несмотря на
5
множествопересеченийсNLP, этовсёжеотдельнаянаука,икасатьсявэтомкурсемыеёне будем.
NLP vs CompLing
Важноотличать обработку естественного языка от компьютернойили,точнее,вычислительнойлингвистики.Очевидно,онисильнопересекаются. Но еслизадачалингвистики-- изучениеязыка, его устройство, в том числеабстрактныетеорииимоделирование,то NLPфокусируется,как правило,наприложениях,обеспечивающихмашиннуюобработку языка. Например,для обеспечения человеко-машинноговзаимодействия.Эточастоуказывают какконечнуюцельобработкиестественногоязыка.Озвученные только что определения спорны, но в рамках этого курса будем считать так.
Вэпохиоптимизмаказалосьдажедостойнейшимизлингвистовказалось,чтоязыкможноописатьнаборамипростыхзаконовиправил.Действительно,насжеучатвшколе,что естьправила,естьисключения.Давноготовысловаримногихязыковвэлектронномвиде.Нужнотолько одинразоченьпостараться,задатьвсе правилав однойбольшойсистеме,имашинызапростосмогутрешатьвсестандартныезадачиобработкиязыка,чтои мы.Инекоторыеузкоспециализированныесистемынаправилахдействительноиногдаотлично работают.
Но языккуда сложнее,чемможет показатьсяна первыйвзгляд.И полноепониманиесмыслачастонеформальноотносятктакназываемымAI-hardproblems,тоестьзадачам,решениекоторыхнастолько же сложно,насколько сложнои созданиеобщегоискусственного интеллекта.
Примеры трудностей в обработке языка
Давайте рассмотрим несколько известных примеров.
6
Чтонетакспредложением“Этитипысталиестьвцехе”?Можетбыть,здесьречьидёто типахметалла,которыеоказалисьв наличиив цехе,а можетбыть--о халатныхпосетителяхцеха, поленившихсядойтидостоловой.Незнаем,что зацех,нонадеемся,онине повредилинисвоемуздоровью,нипроизводству. Обе трактовкидопустимы,инужную нам  может подсказать только контекст.
Не так-то просто придётся машинеи с предложением“За песчанойкосойлопоухийкосой пал под острой косой…” “Косой” здесь -- это и коса каксельскохозяйственное орудие труда, и коса как узкиймысв водоёме,и косой --субстантивированноеприлагательное,которымвданномслучае,скореевсего,обозначаютзайца.Еслинамважно,чтобымашинапоняла,какойизсмысловскрываетсязакаждымизсловоупотреблений, придётся постараться.
Рассмотримпредложение“Мамавымылараму, итеперьонаустала”.Невсегдаочевидносходу, вчёмздесьпроблема,всёжетакпонятно.Ноздравогосмыслаумашинывообщеговорянет. Поэтомупонять,кто именноустал -- мама илирама, запросто неполучится. Мызнаемчто“рама”--этопредметиустатьбуквальноонанеможет, поэтомуречь, конечно,о маме.Эта важнаядлялингвистикизадача называется разрешениеманафоры.
7
Похожийпример,частоиспользуемыйдлядемонстрациислучаевнеоднозначностисинтаксического разбора--знаменитаяшутка изфильма 1930годаAnimalCrackers:OnemorningI shotanelephantinmypajamas.HowhegotintomypajamasI’llneverknow.(Однаждыявмоейпижамезастрелилслона.Какслонвлезвмоюпижамумнеузнатьнесуждено.)Еслиотброситьвторое предложение,будетсовершеннонепонятно,кто жебылв пижаме.Важноздесь,что каждойиз возможныхтрактовок соответствует своёдерево разбора в рамках грамматики непосредственных составляющих.
Перед вами неполный список задач по обработке языка.
Spelling checking -- проверка правописания.Hyphenation -- расстановка переносов.Keyword extraction -- выделение ключевых слов.Language modeling -- языковое моделирование.Part-of-speech tagging -- частеречная разметка,Stemming / Lemmatization -- приведение текстов к нормальной, словарной форме;Named entity recognition -- выделение именованных сущностей.Text (topic) classification -- классификация текстовSyntax parsing -- синтаксический разбор.Machine translation -- машинный переводText clustering -- кластеризация текстов.Topic modeling -- тематическое моделирование.QA systems, dialogue systems -- вопросно-ответные и диалоговые системы
8
Text summarization -- автореферированиеPlagiarism detection -- обнаружение плагиата
Spellingchecking-- проверка правописания.Здесь всё понятно,во многихпродвинутых текстовых редакторах есть такая функция, в том числе для русского языка.
Hyphenation-- автоматическая расстановка переносов.
Keywordextraction-- выделениеключевыхслов.Ихизвлекают из текстов ссамымиразнымицелями.Например,дляпоиска сложныхтерминов,илипониманиявпервом приближении, каким проблемам посвящены исследуемые тексты.
Languagemodeling--языковое моделирование.Задачапредсказанияследующегослова по нескольким предыдущим. Имеет очень много приложений в современном NLP.
Part-of-speechtagging--частеречнаяразметка,каквсемыделалившколе.Посутизадача разрешения морфологической неоднозначности.
Stemming/ Lemmatization--приведениетекстовкнормальной,словарнойформе;как мывидимиз примеровтрудностейв обработке языка, это не всегдатривиальнаязадача, иногда не удаётся определить и часть речи без контекста.
Namedentityrecognition--выделениеименованныхсущностей.Далеконевсегдапросто выделитьизтекста топонимы,названияорганизаций,именаитакдалее.Широкоиспользуется во многих прикладных системах.
Text(topic)classification--однаизсамыхчасторешаемыхзадачвобработкеязыка.Приналичиинекоторыхметоку текстов(например,положительныйтонурецензииилинет),суметьрасставлятьметкиунеаннотированныхещётекстов.Сюдаотносятсяанализтональности,классификацияновостейпотематикам,выявлениетоксичныхкомментариеви многое другое.
Syntax parsing-- синтаксический разбор. Тоже знакомаявсем школьная задача.
9
Machinetranslation -- машинныйперевод,оченьдалеко шагнувшийв последниегоды по сравнению с системами, распространявшимися на сиди-дисках в нулевых.
Textclustering--группировкапохожихтекстовиразделениенепохожихбезвсякойчеловеческой разметки.В качестве примераможновспомнитьагрегаторыновостей,которые зачастую удивительно хорошо склеивают новостные статьи по событиям.
Topicmodeling--тематическоемоделирование.Задача,похожаянакластеризацию.Выделениеиз наборатекста тематик и сопоставлениекаждому документу ихраспределения.Часто используется в социальныхнауках дляанализанеразмеченныхтекстов и для разведочныхпоиска и анализаданных.Тематическое моделированиепозволяетбыстродатьответнавопрос,очёмжевсеэтитексты,таккаккаждойтематикесопоставлено распределение наиболее вероятных из составляющих её слов.
QA systems,dialoguesystems-- вопросно-ответныеи диалоговыесистемы.Например,автоматические помощники,которые внедряют всё чаще мобильныеоператоры,банкиидругиекоммерческиекомпаниидлябыстрыхответовна,какправило,простыевопросыпользователей.Сюдаже можноотнестии популярныхвстроенныхвмобильные устройства виртуальных ассистентов последних лет.
Textsummarization-- автореферирование,сложнаязадачасоставлениякраткогосодержания сравнительно большого текста.
Plagiarismdetection--обнаружениеплагиата.Студентамненужнообъяснять,чтотакое системыобнаруженияплагиата, так как через них,как правило,проходятвсекурсовые и дипломные работы. Эту задачу также относят к NLP.
Этот список-- далеко не полный.Естьи другиепрактическиважные,и болееэзотерическиезадачи.Крометого, некоторыезадачиимеют смыслтолько в отдельныхязыках.Например,внекоторыхиероглифическихсистемахподелитьтекстнаслова--ужепроблема.
10
Конечно,всего в этом курсе мынерассмотрим,пока займёмсяосновами.Нов областиобработкиестественного языка ещёмного задач, в которыхна момент2020года нетудовлетворительного решения;особенноеслиречьидётнеобанглийском.NLPсейчас--очень конкурентная область, но работы много, добро пожаловать.
Краткая и неполная история компьютернойлингвистики
Докомпьютерная эра
Начнёмсдокомпьютернойэры.Еслинесчитатьразличныхопытовсшифрованиемтекстов, первыйвычислительныйподход к языку, о котором хочется обязательноупомянуть-- это применениевеликимрусскимматематиком АндреемАндреевичемМарковымразработаннойимвтот моментмоделиктекступоэмыАлександраСергеевичаПушкина “Евгений Онегин”. Эту модель позже назовут марковской цепью.
Отступление: эксперимент с “Онегиным”  и цепи Маркова
11
ПростаяцепьМаркова-- последовательностьсобытий,в которойвероятностьочередного событиязависиттолько от конечного числапредыдущих.Например,одного.АндрейАндреевичМарковрассматривалвкачестветакихсобытийпоявлениегласнойилисогласнойбуквв текстеипредполагал,чтогласнойилисогласнойочереднаябуквабудеттолько в зависимостиот того к какомуиздвухклассовотноситсятекущаябуква.Ведьчащевсего засогласнойследуетгласная-- и наоборот. Приэтоммысчитаем,что весьпредшествующий контекст неважен.
Рассмотревпервые200000знаков “ЕвгенияОнегина”великийматематикбезвсякихкомпьютероввручнуюподсчитал,чтозагласнойгласнаяидётв12.8%случаев,азасогласной гласная в 66.3%.
Как оценитьвероятность,что, наткнувшисьна гласнуюбукву, мыувидим,чтоследующиедве будутсогласными?В рамкахпростойцепиМаркова нампросто нужноперемножитьвероятности“согласнаяпослегласной”и“согласнаяпослесогласной”.Тожевернои для более длинныхпоследовательностей.Используя простейшиесвойствавероятностейможнодлялюбойпономеруNбуквывтекстеоценитьвероятность,чтоонаокажется гласнойилисогласной.Дляэтого нужнопросуммировать вероятностивсехцепочек, длины N, которые заканчиваются гласной буквой.
12
Важнейшеесвойствомарковскойцепи--длябольшихзначенийNвероятности,чтобуква напозицииNбудетгласнойилисогласной,почтиперестают менятьсяинезависятот начального состояния.Это свойство называется эргодическим,вероятности прибесконечно большом N называют предельными.
АндрейАндреевичМарковвычислилс помощьюэтого свойствавероятность,чтослучайновзятая буква будет гласной,и она совпала с долей гласныхбукв врассматриваемом тексте.
Компьютерная эпоха
Однако по-настоящемуисториякомпьютернойлингвистикиначаласьгораздопозже.Онаоченьнепроста.Сложнодажеперечислитьосновныевехитак,чтобыслучайночто-нибудьнезабытьикого-нибудьнеобидеть,обделивветкойпальмыпервенства.Деловтом, что с развитиемэлектронно-вычислительнойтехникисразу появилисьзадачипоработе с человеческимязыком, и в несколькихстранаходновременноначалисьисследования,посвящённыеразличнымчеловеко-машиннымвзаимодействиямспомощьюестественного языка. В первуюочередь в Советском Союзеи СоединённыхШтатахАмерики.
Напротяжениивсейисториивычислительнойлингвистикиоднойизсамыхважныхи сложныхзадач были остаётся машинныйперевод. Сейчасбольшинство успешныхсистеммашинного переводаработают наалгоритмахмашинного обучения,требующихбольшого количества парпредложенийнадвухязыках.Нов далёком1954годумногиеверили,что правиламиможнообойтись,именнотогдав СШАсостояласьдемонстрацияавтоматического перевода 60 предложенийс русского языка на английский.Системапереводаработалавсегона6грамматическихправилахибыласовместнымпроектомIBMи Джорджтаунского университета. Демонстрациясилы,вау-эффект и видимостьуспехапомоглиубедитьчто вразвитиемашинногопереводанужновкладыватьбольшиеденьги.
13
Однако вскоре оказалось, что задача не решается правилами,и через 12 летфинансирование было значительно урезано.
Нужнотакжеотметить,чтоДжорджтаунскийэксперимент--этосамыйгромкий,нонепервыйподход,ичтовтежегодывнесколькихинститутахвСоветскомСоюзетакженад машинным переводом работало несколько исследовательских групп.
Ещёодинэкспериментизэпохиоптимизма,окоторомнельзянеупомянуть,--этоЭлиза,одиниз первыхчат-ботов,виртуальныхассистентов,разработанныйв недрахлабораторииискусственного интеллекта в Массачусетском технологическом ДжозефомВейценбаумом.
Элизабыларазработанакакпародиянатакназываемогоклиент-центрированногопсихотерапевта,практикующегометодактивногослушания.Тоестьпобольшейчастиона,перефразируя,обращалавсе репликисобеседникаввопросыемуже.Давайтепосмотримна типичный диалог с Элизой.
Human: Well, my boyfriend made me come here.
ELIZA: Your boyfriend made you come here?
Human: He says I'm depressed much of the time.
ELIZA: I am sorry to hear you are depressed.
Human: It's true. I'm unhappy.
ELIZA: Do you think coming here will help you not to be unhappy?
Человек: Что ж, сюда прийти меня заставил мой молодой человек.
ELIZA: Вас сюда заставил прийти ваш молодой человек?
Человек: Он говорит, что я постоянно чем-то подавлена.
14
ELIZA: Как жаль, что вы чем-то подавлены.
Человек: Всё так и есть. Я несчастна.
ELIZA: Полагаете, ваш визит сюда поможет вам перестать быть несчастной?
Какнетруднодогадаться,Элиза--тожесистемаработающаянаправилах.Еёответвыбиралсяв соответствиис шаблоном,под которыйподходилафразапользователя,онаничемуне училасьи не держала контекст. И даже приэтом у некоторыхиз еёсобеседников создавалось впечатление,что Элизаэмоциональнововлеченав беседуипроявляеткниминтерес.Поэтомуантропоморфизациювычислительнойтехники,когдаеёпользовательзнает, что имеетделос компьютером,--называют эффектомЭлизы. Стехпорутекломного воды,и сейчассамыесовременныеи сложныечат-ботыработают нанейронных сетях, но обо всём расскажем по порядку.
Увы, для краткости повествования нам придётся опустить многие важные события.
50-е
50-е годы двадцатого века, как мы уже поняли,ознаменовалисьпервымипопыткамик решениюзадач,которыесейчасотносятк вычислительнойлингвистике.В1948годувышластатьяКлодаШеннона“Математическаятеориясвязи”,вкоторойбылиизложеныосновытеорииинформации.В1957годуувиделасветсовершившаяреволюциюв лингвистике работа “Синтаксические структуры” Ноама Хомского.
60-е и 70-е
60-е и 70-е годы можно назвать эройвзлёта и падениясимволическогоискусственного интеллекта,то естьверыв то,что можносоздаватьразумныемашиныспомощьюметодовлогическоговыводаиэкспертныхсистем.В60-еначаласьиразработкасоветскимилингвистамитеории“Смысл-Текст”ИгоряАлександровичаМельчука,котораяоказала значительное влияниена отечественнуюлингвистику, и о состоятельности
15
котороймноголетвелисьжаркиеспоры.В60-хбылразработанкорпусанглийскогоязыкаУниверситета Брауна(Браун-корпус),одиниз важнейшихлингвистическихресурсовмногихпоследующихлет, от момента созданиякоторого принято отсчитыватьсуществованиекорпуснойлингвистики.Одиниз величайшихучёныхдвадцатого векаАндрейНиколаевичКолмогоровзанимаетсяприменениемматметодовканализурусскогостиха.
80-е и 90-е
В80-еи90-егодывобработкуестественногоязыка,открывдверьногой,приходитмашинное обучение.Нормойстановятся мероприятияпо численнойоценке качестваалгоритмованализатекста --наосноверазличныхнаборовданных.Всоветскомсоюзев1985годуакадемикАндрейПетровичЕршовинициируетсозданиеМашинного фондарусского языка. Все будущиеважнейшиекомпонентынейронныхсетейдляобработкитекстов изобретаются в это время.Применяются структурныемоделимашинногообучения,например,дляраспознаванияречи.Разрабатываютсяколичественныеподходыкдистрибутивной семантике.
2000-е
2000-е--началоэпохиинтернета,скаждымгодомтекстовыхданных,доступныхвсетистановитсявсёбольше.ОткрываетсяинтерфейскНациональномуКорпусуРусскогоязыка. Машинное обучениеколонизирует обработку естественного языка. Хранениеданныхи вычислительныемощностистановятсявсёдешевле.Тем,кто сталкиваетсясзадачамиобработки текстов, приходится иметьделосо всё бОльшимии подчасменяющимисяво временинаборамиданных.Поэтомурастётинтерес к такназываемымметодам“без учителя“--алгоритмам,пытающимсябезвсякойразметкиибезэкспертногомнениявыделитьв данныхструктуру -- сгруппировать похожиетексты,выделитьтематикиизнаборатекстов,отематическойструктуремыможемничегонезнатьаприори-- и так далее.
16
2010-е
2010-е,как многие,думаю,слышали, это эпоха гегемонииоднойиз важныхмоделеймашинного обучения--нейронныхсетей.Эпохатакназываемогоглубокогоили,какиногдаговорят, глубинногообучения.Всвязисотносительнойдешевизнойхранилищданныхи вычислительныхресурсов, появлениемновыхметодов обученияиинструментов,упрощающихжизньисследователям,использованиенейронныхсетейсталоэффективными бьётвсе рекордыпокачеству предсказанийв компьютерном зрениииобработке естественного языка. Вработес нимиестьмного тонкостей--и в томчислескоростьпредсказаний,необходимостьналичиямощныхвычислительныхресурсов--такчто на практике нейронныесети далеко не всегда первыйметод, которыйстоитпопробоватьвнадежденамгновенныйуспех.Чтобысохранитьдлительностьисложностькурса подходящейдля того, чтобысчитать его вводным,мы не будемобсуждатьнейронныеметоды,нооченьрекомендуемматериалыстенфордскогокурсапоглубокомуобучению для обработки естественного языка.
Готовим тексты для работы с машиной
В зависимостиот выбраннойзадачи и метода,текстыможноподготовитьдляработыс компьютером по-разному. Скажем,дляанализатематикновостей,намбываетдостаточнокаждуюновостьпредставитькакмножество слов.Дляразметкичастейречиилисинтаксическогоразборапорядоксловнамважен,поэтомумырассматриваемтекстыкак цепочку слов. И так далее.
Давайтеначнёмс простыхзадач предобработкитекста на естественном языке,которые приходится решать почти каждый раз.
Итак,перед намитексты--допустим,нарусскомязыке.И,допустим,нашазадачадляначалаподготовитьтекстытак,чтобымы,какпоисковик,моглипозапросувыбратьвсе документы, содержащие то или иное слово.
17
Изначальнодлямашиныкаждыйиз нашихтекстов просто строка -- конечнаяпоследовательностьсимволов--букв,пробелов,знаковпрепинанияитакдалее.Конечно,чтобыосуществлятьэффективныйпоискпословам,нужноразделитьстрокунаподстроки--отделитьдруготдругасловаизнакипрепинания.Этазадачаразбиениястрокинатакназываемыетокеныназываетсятокенизацией. Обычноненужнопрограммироватьеёсамостоятельно,естьмного готовыхинструментов,которыевсё сделают за вас.В ихоснове часто эвристическиеправилавида“токен -- это подпоследовательностьбукв,отделённая от остального текста пробелами и запятыми или точками”.
Важно!Есливыпользуетесь каким-то лингвистическиминструментом,которыйпринимаетна вход готовыетокены,приобработке вашихтекстовважноиспользоватьименнотот токенизатор,на которыйрассчитываетинструмент. Например,некоторыетокенизаторыопускают знакипрепинания,а некоторыевыделяют ихвотдельныйтокен.Алгоритм,не приспособленныйк работе со знаками препинания,или напротив,использующийих,можетпотерятьвкачествесвоейработы,еслитекстыбудутразбитынатокенынеподходящимобразом.Тожекасаетсянетолькотокенизации,ноидругихэтаповподготовки текста. Вернёмся к ним.
Невсегда,нозачастуюсмыслотдельныхсловнамважнее,чемихпадеж,число,родитакдалее.Скажем,еслимыхотимнайтивседокументы,содержащиеслово“крот”,намявнобудутнужныидокументысословами“кротик”,“крота”,“кроту”,“кроты”,“кротами”итакдалее.Дляэтогоисходныесловапреобразуютсятак,чтобыразличныеформыслова,преобразовывались в какой-то единый для них токен. Есть два стандартных способа.
Стемминг
Первыйспособ-- такназываемыйстемминг. Вразныхисточникахстеммингомназывают разное,нообычноподразумевают, чтостемминг--этопопыткавыделитьчастьслова, общуюдля всех его морфологически выводимыхформ, в английском онаназывается “stem”.В примерес “кротом” такимидеальнымобщимдлявсех формфрагментом будет “крот”. Обратитевнимание,что стем-- это не всегда слово.
18
Гипотетическийстеммеризглагола“солить”,“солю”,“солим”,“солите”--выделит“сол”.Технически,сол -- это слово, которымобозначают марсианскиесутки(в NASAдажепридумалислова yestersol,solmorrowи tosol),новсёже к солионинеимеют никакогоотношения.
Стеммер Портера
Первыйпо-настоящемууспешныйидосихпориногдаиспользующийсястеммерпридумал МартинПортер. Онтак и называется в литературе-- стеммерПортера.Принципработы-- последовательное применениек каждомуслову несколькихправилпреобразованиястрок. Алгоритмподробноописан на сайтеавтора.Мы не будемпогружаться в детали, но дадим исходный алгоритм очень крупными мазками.
Шаг 1а.SSES -> SSIES -> ISS -> SSS ->
Этиправилапреобразовывают постфиксы(то естьконцы)слов.Здесьмыизбавляемся,например,от множественного числасуществительныхи единственного числаглаголов.Еслислово заканчивается на sses, то этот постфикс будетподменённа ss. Например,caresses -> caress.
В этом примереслово осталось словом. А вот второе правилоявнопревратитмногие слова в неслова. Например ties → ti, ponies -> poni через i -- и так далее.
Шаг 1 b.
(m>0) EED -> EE(*v*) ED->(*v*) ING ->
19
“Эмбольшенуля”-- это оценка, сколько раздосуффикса строкиEED(и-и-ди)намвстретиласьпарарядом идущихбукввида“гласная,потомсогласная”.Такслова feedиbreedмы сокращать не должны, а вотdisagreedужеобрежем.
“Вэвзвёздочках”требует, чтобывстемебылигласныебуквы.Так,bedобрезатьнеудастся,аизcarvedвыкинемокончание.Тожеработаетсгерундиями,заканчивающимисянаing.Замыселпонятен--здесьмытожеизбавляемсяот окончаний,без которыхобщийсмысл слова не должен потеряться.
Если применилисьвторое и третьеправилана этом шаге, мы как бывосстанавливаемформуглагола,выкидываяпринеобходимостиудвоенныесогласныенаконце и добавляя букву E, когда следует.
AT -> ATEBL->BLE
IZ->IZE(*d and not (*L or *S or *Z))->single letter(m=1 and *o)->E
Шаг1cзаменяет, когда уместно, уай на ай на концахслов.
(*v*) Y->I
Мы рассмотрелипервый,самыйсложныйшаг стеммераПортера,работающийсвременамиимножественнымичислами.Общаяидеяясна,адальше,наэтапахсовторогопо пятый, всё гораздо проще. Полный алгоритм описан по ссылке на сайт автора.
1
УстатьиМартинаЭфПортераAnalgorithmforsuffixstrippingна2019годболеедесятисполовинойтысячцитированийв работах другихисследователейпо даннымGoogle
1
http://snowball.tartarus.org/algorithms/porter/stemmer.html 20
Scholar. Состеммером Портерапрощевсего ознакомиться“вбою”,воспользовавшисьбиблиотекойNLTK,с котороймыещёнеразстолкнёмсяв этомкурсе.Авторстеммераразработал специальныйфреймворкдлязаданиясвоихстеммеровпреобразований.Онназывается Snowball.
Лемматизация
Второйи,какправило,предпочтительныйспособприведенияразныхформсловакнекоторойединойсущности,называетсялемматизацией, тоестьприведениемсловоформк лемме,то естьсловарнойформе.Скажем,лемматизацияслова “рычали”дастнам“рычать”,слова “укуса” -- “укус”,“острому”-- “острый”и такдалее.Отметим,что влитературе часто лемматизацию называют одним из вариантов стемминга.
Морфологический вывод
Лемматизаторы, чаще прочих использующиеся на практике, как правило,используют определённымобразомобработанныесловари--тоестьспискислов.Однако,увы,нети не может бытьуниверсального словаря,так как языкипостояннорастут.Поэтомутребуютсянекоторыеправила,какпривестиклемменеизвестноелемматизаторусловолишьпоегонаписаниюи,возможно,окружающемутексту. Очевидно,человекуэтопод силу, давайте вспомним Кэрролла
А в глу́ше ры́ мит исполин —Злопастный Брандашмыг!
И ещёодинклассическийпримериз лекцийакадемика Льва ВладимировичаЩербы:
Гло́кая ку́здра ште́ко будлану́ла бо́кра и курдя́чит бокрёнка
Здесь нампомогают окончания.Мыможемопределитьчастьречи,род, число,падежи время.Такполучитсясделатьнево всех языках,и в этом курсе мынебудем
21
углублятьсяв ихклассификациюпоролив каждом изнихморфологии.Нов русскомязыке отдельныеморфемымогутдатьоченьмного информациио происходящем,дажеесли смысл корней слов нам неизвестен.
Длясравнения,вболеесовременномлитературномисточнике--вдебютномроманеВладимираГеоргиевичаСорокина“Норма”--используетсясвоеобразныйхудожественныйприём.Частьсловперсонажейзаменяетсянаслова,которымявнонеприсущаморфологиярусского языка.
— Ну, это слишком серьёзныйпловркнрае, — усмехнулсязам главного редактора.[...]— Старичок, нодомлоанрговпр, дочапвепкнав! — засмеялсяАлександр Павлович.
Впервомслучаемы,конечно,можемдогадаться,чтоскореевсего,пловркнрае--этосуществительное,вероятно,похожеепосмыслунаслово“вопрос”.Номыэтовывелиизконтекста, то естьтолько благодаряокружающимрусскимсловами ихположениювпредложении.Авот вовторойрепликенамостаётсятолькодогадываться,какуюостротуотпустилАлександрПавлович,и как мы могли бы сделать разборпредложения.Контекстнойинформациислишком малодаже длятого, чтобыпонять,изкакихчастейречи составлена фраза.
Морфологическая неоднозначность
Какмыуже отметили,часто дляприведениясловк словарнойформе,алгоритмыучитывают и окружающий это слово текст. Зачем это нужно делать?
Попробуемопределитьсловарнуюформуслова“мыла”.Сразуясно,гдеподвох,нетакли?Без контекста мынеможемопределить,какаяэто частьречи,--либоглагол впрошедшемвремени,и тогдалемма “мыть”.Либоимясуществительное в родительномпадеже. Нетчего?Нетмыла.То естьлемма “мыло”.Однако когдаэто слово появитсявпредложении “мама мыла раму”, все наши сомнения отпадут.
22
Это явлениеназываетсяморфологическойнеоднозначностью.Поэтому, несмотрянато,чтосамонаписаниесловврусскомязыкенампомогает,размечатьчастиречивсёженужнос оглядкойнаконтекст. И поэтому, когдамыхотимполучитьсловарнуюформулексемы, стоит пользоваться инструментами работающимис морфологическойнеоднозначностью.Но этим часто пренебрегают в случаях,когда неправильноеопределение леммы не вносит большого вклада в результат -- ради быстродействия.
Инструментарий для русского языка
Длялемматизациина практике используют разныеинструменты.Рассмотримнесколько средствдляморфологического анализа,широкоиспользуемыхнамомент2019года.
Pymorphy2
Второй pymorphy-- морфологический анализатор Михаила Коробова,распространяемыйкак библиотека на языке Python.Пакет умеетприводитьслово ксловарнойформе,ставитьвнужнуюформуивозвращатьграмматическуюинформациюослове (число,род, падеж,частьречии т.д.).Работаетнаоснове словарей,а именнонаоснове разметкиOpenCorpora,словоформыхранятсявдетерминированномациклическомконечном автомате (DAFSA),что позволяеткомпактнохранитьогромное количествословоформ и эффективно работать с буквой “ё”, которую иногда подменяют на “е”.
Общийподход,пословамавтора,похожнатот, чтоописаннасайтеaot.ru--сайтедругого проекта, очень важного для развития обработки естественного языка в России.
Как работать с пайморфирассказанов оченьподробнойдокументации.Давайтерассмотрим пример.
23
>>> import pymorphy2
>>> morph = pymorphy2.MorphAnalyzer()
>>> morph.parse(" мыла")
[Parse(word=' мыла', tag=OpencorporaTag('NOUN,inan,neutsing,gent'), normal_form=' мыло', score=0.333333,methods_stack=((<DictionaryAnalyzer>, ' мыла', 54, 1),)),
Parse(word=' мыла', tag=OpencorporaTag('VERB,impf,tranfemn,sing,past,indc'),normal_form=' мыть', score=0.333333,methods_stack=((<DictionaryAnalyzer>, ' мыла', 1813, 8),)),
Parse(word=' мыла', tag=OpencorporaTag('NOUN,inan,neutplur,nomn'), normal_form=' мыло', score=0.166666,methods_stack=((<DictionaryAnalyzer>, ' мыла', 54, 6),)),
Parse(word=' мыла', tag=OpencorporaTag('NOUN,inan,neutplur,accs'), normal_form=' мыло', score=0.166666,methods_stack=((<DictionaryAnalyzer>, ' мыла', 54, 9),))]
Вызвавметодparseотслова“мыла”,мыполучилинескольковозможныхвариантов,тоестьнесколькообъектовклассаParse,которыевключают всебявтомчислесловарнуюформу, выведеннуюграмматическуюинформациюинекийscore.Вариантыснаибольшимscore-- имясуществительное -- единственное числородительныйпадеж(генитИв)--иглагол прошедшего времени.Этот параметр--условнаявероятностьнабораграммемдляданного слова (то есть совокупностичастиречи, числа,падежаи так далее).Онаоценивается потойчастиОткрытого корпуса, гдеснята неоднозначность,какпростоеотношениечастот:сколькоразслово“мыла”встречаетсякакглагол(единственногочислаи так далее) делим на общее количество употреблений слова “мыла”.
24
Покатолькотак,неиспользуяокружающийконтекст,pymorphy2помогаетвыбратьвариант в условиях неоднозначного разбора.
Mystem
Майстем--это морфологическийанализаторрусского языкасподдержкойснятияморфологической неоднозначности. Первую версиюразработали ИльяСегаловичиВиталийТитовв компании«Яндекс».Программа работаетнаоснове “Грамматическогословаря русского языка” АндреяАнатольевичаЗализняка и способнаформироватьморфологическиегипотезыонезнакомыхсловах.Распространяетсячерез сайт“Яндекса”в видеисполняемого файла,код закрыт. По лицензииисполняемыйфайлнельзяиспользовать только в случае,есливыразрабатываетекакой-то продукт, потенциальноконкурентоспособный по отношению к какому-либо сервису “Яндекса”.
Так как насинтересует возможностьдальнейшейобработкитого, что будетнавыходепослеобработкимайстемом,стоитупомянутьхотя быоднуизобёртокнаязыкеPython, с которым мы будем иметь дело в ходе работы над заданиями.
Pymystem3-- библиотека, разработаннаяДенисом Сухониными АлександромПанченко. Интерфейсоченьпростойи интуитивный,лемматизациявыполняетсяв пятьстрочек
>>> from pymystem3 import Mystem
>>> mystem = Mystem()
>>> text = ' Как насчёт небольшого стемминга'
>>> lemmas = mystem.lemmatize(text)
>>> print(''.join(lemmas))
как насчет небольшой стемминг
25
(пример позаимствован с сайта nlpub.ru)
Попробуемприменитьmystemкнашемупримерусмамой,моющейраму. Сначаластарый вариант, без работы с морфологической неоднозначностью.
>>> from pymystem3 import Mystem
>>> mystem = Mystem(disambiguation=False)
>>> text = " Мама мыла раму."
>>> lemmas = mystem.lemmatize(text)
>>> " ".join(lemmas)
'мама   мыло   рама . \ n'
А вот вариант с обработкой неоднозначности.
>>> mystem = Mystem(disambiguation=True)
>>> text = " Мама мыла раму."
>>> lemmas = mystem.lemmatize(text)
>>> " ".join(lemmas)
'мама   мыть   рама . \ n'
Как мы видим, на этот раз всё верно.
Mystem также выдаёт всё, что он смог понять о тексте, поступившем на вход
>>> mystem.analyze(text)
[{'analysis':[{'gr':'S, жен,од=им,ед','lex':'мама'}],'text': 'Мама'},{'text':' '}, {'analysis':[{'gr':
26
'V, несов,пе=прош,ед,изъяв,жен','lex': 'мыть'}], 'text':'мыла'}, {'text': ' '}, {'analysis': [{'gr':'S, жен,неод=вин,ед','lex':'рама'}],'text':'раму'},{'text':'.'}, {'text': '\n'}]
Этуинформациюможновдальнейшемтакжеиспользоватьдлярешенияразличныхзадач. Посмотрите документацию, у майстема есть различныевозможностиформатирования вывода, которые делают его удобным.
RNNMorph
RNNMorphИльиГусева --намоментсередины2019годаодинизсамыхточныхморфологическиханализаторовдля русского языка, работающийна рекуррентныхнейронных сетях и использующий pymorphy2 как компонент.
>>> from rnnmorph.predictor import RNNMorphPredictor
>>> predictor = RNNMorphPredictor(language="ru")
>>> forms = predictor.predict([" мама", "мыла", "раму"])
>>> forms
[<normal_form= мама; word= мама; pos=NOUN;tag=Case=Nom|Gender=Fem|Number=Sing; score=0.9998>,<normal_form= мыть; word= мыла; pos=VERB;tag=Gender=Fem|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act;score=0.9651>,<normal_form= рама;word= раму;pos=NOUN;tag=Case=Acc|Gender=Fem|Number=Sing; score=0.5768>]
27
Так как длякаждого поступающего предложениязапускаются предсказанияспомощьюнейроннойсетинаTensorFlow, этабиблиотекаработаетгораздомедленнее,чемрассмотренные выше.
Также средиготовыхрешенийдлярусского языка стоитобратитьвниманиенаTreeTagger и UDPipe, в разные годы в разных работах показывавшие лучшие результаты.
Выбиратьсредства дляморфологического анализаинормализациислов--тоестьприведенияк нормальнойформе-- нужноисходя из требованийк задаче,поняв,чтоважнее -- качество, быстродействие, хорошая лицензия и так далее.
28
