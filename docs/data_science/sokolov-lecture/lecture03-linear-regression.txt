Лекция 3
Линейная регрессия
Е. А. Соколов
ФКН ВШЭ
27 сентября 2021 г.
1 Переобучение
Нередко в машинном обучении модель оказывается переобученной /emdash.cyr её каче-
ство на новых данных существенно хуже качества на обучающей выборке. Действи-
тельно, при обучении мы требуем от модели лишь хорошего каче ства на обучающей
выборке, и совершенно не очевидно, почему она должна при это м хорошо обобщать
эти результаты на новые объекты.
В следующем разделе мы обсудим подходы к оцениванию обобщаю щей спо-
собности, а пока разберём явление переобучения на простом п римере. Рассмотрим
некоторую одномерную выборку , значения единственного при знака x в которой гене-
рируются равномерно на отрезке [0, 1], а значения целевой переменной выбираются
по формуле y = cos(1.5πx) +N (0, 0.01), где N (µ, σ2) /emdash.cyr нормальное распределение
со средним µ и дисперсией σ2. Попробуем восстановить зависимость с помощью ли-
нейных моделей над тремя наборами признаков: {x}, {x, x2, x3, x4} и {x, x2, . . . , x 15}.
Соответствующие результаты представлены на рис. 1.
Видно, что при использовании признаков высоких степеней мо дель получа-
ет возможность слишком хорошо подстроиться под выборку , из -за чего становится
непригодной для дальнейшего использования. Эту проблему м ожно решать многими
способами /emdash.cyr например, использовать более узкий класс моделей или штрафовать за
излишнюю сложность полученной модели. Т ак, можно заметить , что у переобучен-
ной модели, полученной на третьем наборе признаков, получа ются очень большие
коэффициенты при признаках. Как правило, именно норма вект ора коэффициентов
используется как величина, которая штрафуется для контрол я сложности модели.
Т акой подход называется регуляризацией, речь о нём пойдёт в следующих лекциях.
2 Оценивание качества моделей
В примере, о котором только что шла речь, мы не можем обнаружи ть переобу-
ченность модели по обучающей выборке 1. С другой стороны, если бы у нас были
1Конечно, это можно было бы заметить по большим весам в модели , но связь между нормой
весов и обобщающей способностью алгоритма неочевидна.
1
2
Рис. 1. Регрессионные кривые для признаковых наборов различной сложности.
дополнительные объекты с известными ответами, то по ним зам етить низкое каче-
ство модели было бы довольно легко.
На данной идее основан подход с отложенной выборкой. Имеющиеся размечен-
ные данные (т .е. данные с известными ответами) разделяются на две части: обуча-
ющую и контрольную. На обучающей выборке, как это следует из названия, модель
обучается, а на контрольной выборке проверяется её качеств о. Если значение функ-
ционала на контрольной выборке оказалось удовлетворитель ным, то можно считать,
что модель смогла извлечь закономерности при обучении.
Использование отложенной выборки приводит к одной существ енной пробле-
ме: результат существенно зависит от конкретного разбиени я данных на обучение и
контроль. Мы не знаем, какое качество получилось бы, если бы объекты из данно-
го контроля оказались в обучении. Решить эту проблему можно с помощью кросс-
валидации. Размеченные данные разбиваются на k блоков X1, . . . , X k примерно оди-
накового размера. Затем обучается k моделей a1(x), . . . , a k(x), причём i-я модель
обучается на объектах из всех блоков, кроме блока i. После этого качество каждой
модели оценивается по тому блоку , который не участвовал в её обучении, и резуль-
таты усредняются:
CV = 1
k
k∑
i=1
Q (ai(x), Xi) .
Допустим, мы оценили качество некоторого метода обучения с помощью кросс-
валидации и убедились, что выдаваемые им модели хорошо обоб щают . Как получить
финальную модель для дальнейшего использования? Разумным и будут следующие
варианты:
1. Обучить модель тем же способом на всех доступных данных. Е сли мы исполь-
зовали кросс-валидацию, скажем, по 5 блокам, то каждая модель обучалась
на 80% от общего числа объектов обучающей выборки. Если построить итого-
вую модель на всей выборке, то её параметры будут подобраны п о большему
числу объектов и можно надеяться, что качество вырастет .
2. Если возможности обучить финальную модель нет (например , это слишком
долго), то можно построить композицию из моделей a1(x), . . . , a k(x), получен-
3
ных в процессе кросс-валидации. Под композицией может пони маться, напри-
мер, усреднение прогнозов этих моделей, если мы решаем зада чу регрессии.
Позже в курсе мы выясним, что идея такого объединения нескол ьких моделей
оказывается полезной при правильном применении.
3 Обучение линейной регрессии
Нередко линейная регрессия обучается с использованием сре днеквадратичной
ошибки. В этом случае получаем задачу оптимизации (считаем , что среди признаков
есть константный, и поэтому свободный коэффициент не нужен ):
1
ℓ
ℓ∑
i=1
(⟨w, xi⟩ − yi)2 → min
w
Эту задачу можно переписать в матричном виде. Если X /emdash.cyr матрица /guillemotleft.cyrобъекты-
признаки/guillemotright.cyr ,y /emdash.cyr вектор ответов,w /emdash.cyr вектор параметров, то приходим к виду
1
ℓ ∥Xw − y∥2 → min
w
, (3.1)
где используется обычная L2-норма. Если продифференцировать данный функцио-
нал по вектору w, приравнять к нулю и решить уравнение, то получим явную форм у-
лу для решения (подробный вывод формулы можно найти в матери алах семинаров):
w = (XT X)−1XT y.
Безусловно, наличие явной формулы для оптимального вектор а весов /emdash.cyr это
большое преимущество линейной регрессии с квадратичным фу нкционалом. Но дан-
ная формула не всегда применима по ряду причин:
• Обращение матрицы /emdash.cyr сложная операция с кубической сложностью от количе-
ства признаков. Если в выборке тысячи признаков, то вычисле ния могут стать
слишком трудоёмкими. Решить эту проблему можно путём испол ьзования чис-
ленных методов оптимизации.
• Матрица XT X может быть вырожденной или плохо обусловленной. В этом слу-
чае обращение либо невозможно, либо может привести к неусто йчивым резуль-
татам. Проблема решается с помощью регуляризации, речь о ко торой пойдёт
ниже.
Следует понимать, что аналитические формулы для решения до вольно редки
в машинном обучении. Если мы заменим MSE на другой функциона л, то найти та-
кую формулу , скорее всего, не получится. Желательно разраб отать общий подход, в
рамках которого можно обучать модель для широкого класса фу нкционалов. Т акой
подход действительно есть для дифференцируемых функций /emdash.cyr обсудим его подроб-
нее.
4
4 Градиентный спуск и оценивание градиента
Оптимизационные задачи вроде ( 3.1) можно решать итерационно с помощью
градиентных методов (или же методов, использующих как град иент , так и инфор-
мацию о производных более высокого порядка).
§4.1 Градиент и его свойства
Градиентом функции f : Rd → R называется вектор его частных производных:
∇f(x1, . . . , x d) =
( ∂f
∂xj
) d
j=1
.
Известно, что градиент является направлением наискорейше го роста функции,
а антиградиент (т .е. −∇f) /emdash.cyr направлением наискорейшего убывания. Это ключевое
свойство градиента, обосновывающее его использование в ме тодах оптимизации.
Докажем данное утверждение. Пусть v ∈ Rd /emdash.cyr произвольный вектор, лежащий
на единичной сфере: ∥v∥ = 1. Пусть x0 ∈ Rd /emdash.cyr фиксированная точка пространства.
Скорость роста функции в точке x0 вдоль вектора v характеризуется производной
по направлению ∂f
∂v :
∂f
∂v = d
dtf(x0,1 + tv1, . . . , x 0,d + tvd)|t=0.
Из курса математического анализа известно, что данную прои зводную сложной
функции можно переписать следующим образом:
∂f
∂v =
d∑
j=1
∂f
∂xj
d
dt (x0,j + tvj ) =
d∑
j=1
∂f
∂xj
vj = ⟨∇f, v ⟩.
Распишем скалярное произведение:
⟨∇f, v ⟩ = ∥∇f∥∥v∥ cos ϕ = ∥∇f∥ cos ϕ,
где ϕ /emdash.cyr угол между градиентом и векторомv. Т аким образом, производная по на-
правлению будет максимальной, если угол между градиентом и направлением равен
нулю, и минимальной, если угол равен 180 градусам. Иными словами, производная
по направлению максимальна вдоль градиента и минимальна вд оль антиградиента.
У градиента есть ещё одно свойство, которое пригодится нам п ри попытках
визуализировать процесс оптимизации, /emdash.cyr он ортогонален линиям уровня. Докажем
это. Пусть x0 /emdash.cyr некоторая точка,S(x0) ={x ∈ Rd | f(x) =f(x0)} /emdash.cyr соответствующая
линия уровня. Разложим функцию в ряд Т ейлора на этой линии в о крестности x0:
f(x0 + ε) =f(x0) +⟨∇f, ε⟩+ o(∥ε∥),
где x0 + ε ∈ S(x0). Поскольку f(x0 + ε) = f(x0) (как-никак, это линия уровня),
получим
⟨∇f, ε⟩ = o(∥ε∥).
5
Поделим обе части на ∥ε∥:
⟨
∇f, ε
∥ε∥
⟩
= o(1).
У стремим ∥ε∥ к нулю. При этом вектор ε
∥ε∥ будет стремится к касательной к линии
уровня в точке x0. В пределе получим, что градиент ортогонален этой касатель ной.
§4.2 Градиентный спуск
Основное свойство антиградиента /emdash.cyr он указывает в сторону наискорейшего
убывания функции в данной точке. Соответственно, будет лог ично стартовать из
некоторой точки, сдвинуться в сторону антиградиента, пере считать антиградиент и
снова сдвинуться в его сторону и т .д. Запишем это более форма льно. Пусть w(0) /emdash.cyr на-
чальный набор параметров (например, нулевой или сгенериро ванный из некоторого
случайного распределения). Т огда градиентный спуск состо ит в повторении следую-
щих шагов до сходимости:
w(k) = w(k−1) − ηk∇Q(w(k−1)). (4.1)
Здесь под Q(w) понимается значение функционала ошибки для набора парамет ров w.
Через ηk обозначается длина шага, которая нужна для контроля скорос ти дви-
жения. Можно делать её константной: ηk = c. При этом если длина шага слишком
большая, то есть риск постоянно /guillemotleft.cyrперепрыгивать/guillemotright.cyr через точку минимума, а если шаг
слишком маленький, то движение к минимуму может занять слиш ком много итера-
ций. Иногда длину шага монотонно уменьшают по мере движения /emdash.cyr например, по
простой формуле
ηk = 1
k .
В пакете vowpal wabbit, реализующем настройку и применение линейных моделей,
используется более сложная формула для шага в градиентном с пуске:
ηk = λ
( s0
s0 + k
) p
,
где λ, s0 и p /emdash.cyr параметры (мы опустили в формуле множитель, зависящий от номера
прохода по выборке). На практике достаточно настроить пара метр λ, а остальным
присвоить разумные значения по умолчанию: s0 = 1, p = 0.5, d = 1.
Останавливать итерационный процесс можно, например, при б лизости гради-
ента к нулю ( ∥∇Q(w(k−1)∥ < ε ) или при слишком малом изменении вектора весов на
последней итерации ( ∥w(k) − w(k−1)∥ < ε ). Т акже неплохой идеей будет следить за
ошибкой модели на отложенной выборке и останавливаться, ес ли эта ошибка пере-
стала убывать.
Существует большое количество условий сходимости градиен тного спуска.
Обычно они звучат примерно так [ 1]: если функция выпуклая и дифференцируе-
мая, для её первой производной выполнено условие Липшица, д лина шага выбрана
правильно (чем больше липшицева константа, тем меньше долж ен быть шаг), то
6
градиентный спуск сойдётся к минимуму функции. Впрочем, те оретически обосно-
ванную длину шага использовать сложно /emdash.cyr липшицеву константу не всегда легко
посчитать, да и её выбор может дать слишком медленную сходим ость. Проще вы-
брать длину шага, исходя из качества получаемой модели на от ложенной выборке.
Т акже имеет место следующая оценка сходимости для градиент ного спуска:
Q(w(k)) − Q(w∗) =O(1/k).
Ничего не мешает использовать градиентный спуск и для миним изации невы-
пуклых функционалов. Разумеется, гарантий в этом случае ку да меньше: мы можем
попасть в плохой локальный минимум или вообще в седловую точ ку функционала.
§4.3 Оценивание градиента
Как правило, в задачах машинного обучения функционал Q(w) представим в
виде суммы ℓ функций:
Q(w) = 1
ℓ
ℓ∑
i=1
qi(w).
В таком виде, например, записан функционал в задаче ( 3.1), где отдельные функ-
ции qi(w) соответствуют ошибкам на отдельных объектах.
Проблема метода градиентного спуска ( 4.1) состоит в том, что на каждом шаге
необходимо вычислять градиент всей суммы (будем его называ ть полным градиен-
том):
∇wQ(w) = 1
ℓ
ℓ∑
i=1
∇wqi(w).
Это может быть очень трудоёмко при больших размерах выборки . В то же время
точное вычисление градиента может быть не так уж необходимо /emdash.cyr как правило, мы
делаем не очень большие шаги в сторону антиградиента, и нали чие в нём неточно-
стей не должно сильно сказаться на общей траектории. Опишем несколько способов
оценивания полного градиента.
4.3.1 Стохастический градиентный спуск
Оценить градиент суммы функций можно градиентом одного слу чайно взятого
слагаемого:
∇wQ(w) ≈ ∇ wqik (w),
где ik /emdash.cyr случайно выбранный номер слагаемого из функционала. В этом случае мы
получим метод стохастического градиентного спуска (stochastic gradient descent,
SGD) [ 2]:
w(k) = w(k−1) − ηk∇qik (w(k−1)).
7
У обычного градиентного спуска есть важная особенность: че м ближе текущая точка
к минимуму , тем меньше в ней градиент , за счёт чего процесс за медляется и аккурат-
но попадает в окрестность минимума. В случае со стохастичес ким градиентным спус-
ком это свойство теряется. На каждом шаге мы двигаемся в стор ону , оптимальную с
точки зрения уменьшения ошибки на одном объекте. Параметры , оптимальные для
средней ошибки на всей выборке, не обязаны являться оптимал ьными для ошибки
на одном из объектов. Поэтому SGD метод запросто может отдал яться от минимума,
даже оказавшись рядом с ним. Чтобы исправить эту проблему , в ажно в SGD делать
длину шага убывающей /emdash.cyr тогда в окрестности оптимума мы уже несможем делать
длинные шаги и, как следствие, не сможем из этой окрестности выйти. Разумеется,
потребуется выбирать формулу для длины шага аккуратно, что бы не остановить-
ся слишком рано и не уйти от минимума. В частности, сходимост ь для выпуклых
дифференцируемых функций гарантируется (с вероятностью 1 ), если функционал
удовлетворяет ряду условий (как правило, это выпуклость, д ифференцируемость и
липшицевость градиента) и длина шага удовлетворяет услови ям Роббинса-Монро:
∞∑
k=1
ηk = ∞;
∞∑
k=1
η2
k < ∞.
Этим условиям, например, удовлетворяет шаг ηk = 1
k . На практике сходимость с ним
может оказаться слишком медленной, поэтому правильнее буд ет подбирать формулу
для длины шага более аккуратно.
Для выпуклого и гладкого функционала может быть получена сл едующая оцен-
ка:
E
[
Q(w(k)) − Q(w∗)
]
= O(1/
√
k).
Т аким образом, метод стохастического градиента имеет мене е трудоемкие итерации
по сравнению с полным градиентом, но и скорость сходимости у него существенно
меньше.
Отметим одно важное преимущество метода стохастического г радиентного
спуска. Для выполнения одного шага в данном методе требуетс я вычислить гра-
диент лишь одного слагаемого /emdash.cyr а поскольку одно слагаемое соответствует ошибке
на одном объекте, то получается, что на каждом шаге необходи мо держать в памя-
ти всего один объект из выборки. Данное наблюдение позволяе т обучать линейные
модели на очень больших выборках: можно считывать объекты с диска по одному ,
и по каждому делать один шаг метода SGD.
Можно повысить точность оценки градиента, используя неско лько слагаемых
вместо одного:
∇wQ(w) ≈ 1
n
n∑
j=1
∇wqikj (w),
где ikj /emdash.cyr случайно выбранные номера слагаемых из функционала (j пробегает значе-
ния от 1 до n), а n /emdash.cyr параметр метода, размер пачки объектов для одного градиент-
ного шага. С такой оценкой мы получим метод mini-batch gradi ent descent, который
часто используется для обучения дифференцируемых моделей .
8
4.3.2 Метод SAG
В 2013 году был предложен метод среднего стохастического градиента(stochastic
average gradient) [ 3], который в некотором смысле сочетает низкую сложность ите -
раций стохастического градиентного спуска и высокую скоро сть сходимости полного
градиентного спуска. В начале работы в нём выбирается перво е приближение w0,
и инициализируются вспомогательные переменные z0
i , соответствующие градиентам
слагаемых функционала:
z(0)
i = ∇qi(w(0)), i = 1, . . . , ℓ.
На k-й итерации выбирается случайное слагаемое ik и обновляются вспомогательные
переменные:
z(k)
i =
{
∇qi(w(k−1)), если i = ik;
z(k−1)
i иначе.
Иными словами, пересчитывается один из градиентов слагаем ых. Оценка градиента
вычисляется как среднее вспомогательных переменных /emdash.cyr то есть мы используем
все слагаемые, как в полном градиенте, но при этом почти все с лагаемые берутся с
предыдущих шагов, а не пересчитываются:
∇wQ(w) ≈ 1
ℓ
ℓ∑
i=1
z(k)
i .
Наконец, делается градиентный шаг:
w(k) = w(k−1) − ηk
1
ℓ
ℓ∑
i=1
z(k)
i . (4.2)
Данный метод имеет такой же порядок сходимости для выпуклых и гладких функ-
ционалов, как и обычный градиентный спуск:
E
[
Q(w(k)) − Q(w∗)
]
= O(1/k).
Заметим, что для метода SAG требуется хранение последних вы численных гра-
диентов для всех объектов выборки. В некоторых случаях этог о можно избежать.
Например, в случае с линейными моделями функционал ошибки м ожно представить
в виде
Q(w) = 1
ℓ
ℓ∑
i=1
qi(⟨w, xi⟩).
Градиент i-го слагаемого выглядит как
∇wqi(⟨w, xi⟩) =q′
i(⟨w, xi⟩)xi.
Значит , нам достаточно для каждого объекта хранить число q′
i(⟨w, xi⟩) /emdash.cyr этого хватит
для восстановления старого градиента.
Для уменьшения количества вычислений можно инициализиров ать z(0)
i нулями,
а не градиентами отдельных слагаемых из функционала ошибки . В этом случае в
формуле шага ( 4.2) важно делить сумму z(k)
i не на общее число объектов ℓ, а на
число объектов, чей градиент вычислялся хотя бы раз. В проти вном случае на первых
итерациях шаги будут очень маленькими.
9
4.3.3 Другие подходы
Существует множество других способов получения оценки гра диента. Напри-
мер, это можно делать без вычисления каких-либо градиентов вообще [ 4] /emdash.cyr доста-
точно взять случайный вектор u на единичной сфере и домножить его на значение
функции в данном направлении:
∇wQ(w) =Q(w + δu)u.
Можно показать, что данная оценка является несмещённой для сглаженной версии
функционала Q.
В задаче оценивания градиента можно зайти ещё дальше. Если в ычислять гра-
диенты ∇wqi(w) сложно, то можно обучить модель, которая будет выдавать оценку
градиента на основе текущих значений параметров. Этот подх од был предложен для
обучения глубинных нейронных сетей [ 5].
§4.4 Модификации градиентного спуска
С помощью оценок градиента можно уменьшать сложность одног о шага гради-
ентного спуска, но при этом сама идея метода не меняется /emdash.cyr мы движемся в сторону
наискорейшего убывания функционала. Конечно, такой подхо д не идеален, и можно
по-разному его улучшать, устраняя те или иные его проблемы. Мы разберём два
примера таких модификаций /emdash.cyr одна будет направлена на борьбус осцилляциями, а
вторая позволит автоматически подбирать длину шага.
Метод инерции (momentum). Может оказаться, что направление антиградиен-
та сильно меняется от шага к шагу . Например, если линии уровн я функционала
сильно вытянуты, то из-за ортогональности градиента линия м уровня он будет ме-
нять направление на почти противоположное на каждом шаге. Т акие осцилляции
будут вносить сильный шум в движение, и процесс оптимизации займёт много ите-
раций. Чтобы избежать этого, можно усреднять векторы антиг радиента с нескольких
предыдущих шагов /emdash.cyr в этом случае шум уменьшится, и такой средний вектор бу-
дет указывать в сторону общего направления движения. Введё м для этого вектор
инерции:
h0 = 0;
hk = αhk−1 + ηk∇wQ(w(k−1)).
Здесь α /emdash.cyr параметр метода, определяющей скорость затухания градиентов с преды-
дущих шагов. Разумеется, вместо вектора градиента может бы ть использована его
аппроксимация. Чтобы сделать шаг градиентного спуска, про сто сдвинем предыду-
щую точку на вектор инерции:
w(k) = w(k−1) − hk.
Заметим, что если по какой-то координате градиент постоянн о меняет знак,
то в результате усреднения градиентов в векторе инерции эта координата окажется
близкой к нулю. Если же по координате знак градиента всегда о динаковый, то ве-
личина соответствующей координаты в векторе инерции будет большой, и мы будем
делать большие шаги в соответствующем направлении.
10
AdaGrad и RMSprop. Градиентный спуск очень чувствителен к выбору длины ша-
га. Если шаг большой, то есть риск, что мы будем /guillemotleft.cyrперескакивать/guillemotright.cyr через точку
минимума; если же шаг маленький, то для нахождения минимума потребуется мно-
го итераций. При этом нет способов заранее определить прави льный размер шага /emdash.cyr
к тому же, схемы с постепенным уменьшением шага по мере итера ций могут тоже
плохо работать.
В методе AdaGrad предлагается сделать свою длину шага для ка ждой компо-
ненты вектора параметров. При этом шаг будет тем меньше, чем более длинные шаги
мы делали на предыдущих итерациях:
Gkj = Gk−1,j + (∇wQ(w(k−1)))2
j;
w(k)
j = w(k−1)
j − ηt
√
Gkj + ε(∇wQ(w(k−1)))j .
Здесь ε /emdash.cyr небольшая константа, которая предотвращает деление на ноль. В данном
методе можно зафксировать длину шага (например, ηk = 0.01) и не подбирать её в
процессе обучения. Отметим, что данный метод подходит для р азреженных задач, в
которых у каждого объекта большинство признаков равны нулю . Для признаков, у
которых ненулевые значения встречаются редко, будут делат ься большие шаги; если
же какой-то признак часто является ненулевым, то шаги по нем у будут небольшими.
У метода AdaGrad есть большой недостаток: переменная Gkj монотонно рас-
тёт , из-за чего шаги становятся всё медленнее и могут остано виться ещё до того,
как достигнут минимум функционала. Проблема решается в мет оде RMSprop, где
используется экспоненциальное затухание градиентов:
Gkj = αGk−1,j + (1− α)(∇wQ(w(k−1)))2
j.
В этом случае размер шага по координате зависит в основном от того, насколько
быстро мы двигались по ней на последних итерациях.
Adam. Можно объединить идеи описанных выше методов: накапливать градиенты
со всех прошлых шагов для избежания осцилляций и делать адап тивную длину шага
по каждому параметру . Т акой метод называется Adam [ 6].
Список литературы
[1] Nesterov, Y. (2004). Introductory Lectures on Convex Optimization. // S pringer-
V erlag US 2004.
[2] Robbins, H., Monro S. (1951). A stochastic approximation method. // Annals of
Mathematical Statistics, 22 (3), p. 400-407.
[3] Schmidt, M., Le Roux, N., Bach, F.(2013). Minimizing ﬁnite sums with the stochastic
average gradient. // Arxiv.org.
[4] Flaxman, Abraham D. and Kalai, Adam Tauman and McMahan, H. Brendan (2005).
Online Convex Optimization in the Bandit Setting: Gradient Descent Without a
Gradient. // Proceedings of the Sixteenth Annual ACM-SIAM S ymposium on Discrete
Algorithms.
11
[5] Jaderberg, M. et. al(2016). Decoupled Neural Interfaces using Synthetic Gradi ents. //
Arxiv.org.
[6] Diederik P. Kingma and Jimmy Ba (2014). Adam: A Method for Stochastic
Optimization. // https://arxiv.org/abs/1412.6980.
