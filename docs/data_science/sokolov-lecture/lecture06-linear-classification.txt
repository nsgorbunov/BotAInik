Лекция 6
Линейная классификация
Е. А. Соколов
ФКН ВШЭ
28 октября 2021 г.
Ранее мы изучили общий подход к обучению линейных классифик аторов, ос-
нованный на минимизации верхней оценки:
1
ℓ
ℓ∑
i=1
L(yi, ⟨w, xi⟩) → min
w
При этом мы привели примеры нескольких верхних оценок ˜L(M), среди которых
были
• L(y, z ) = log(1 + exp(−yz)) /emdash.cyr логистическая функция потерь;
• L(y, z ) = (1− yz)+ /emdash.cyr кусочно-линейная функция потерь (hinge loss).
В этой лекции мы выясним, откуда взялись эти функции потерь, и изучим некоторые
их важные свойства.
1 Логистическая регрессия
§1.1 Оценивание вероятностей
Метод обучения, которые получается при использовании логи стической функ-
ции потерь, называется логистической регрессией. Основны м его свойством является
тот факт , что он корректно оценивает вероятность принадлеж ности объекта к каж-
дому из классов.
Пусть в каждой точке пространства объектов x ∈ X задана вероятность p(y =
= +1| x) того, что объект x будет принадлежать классу +1. Это означает , что мы
допускаем наличие в выборке нескольких объектов с одинаков ым признаковым опи-
санием, но с разными значениями целевой переменной; причём если устремить коли-
чество объекта x в выборке к бесконечности, то доля положительных объектов с реди
них будет стремиться к p(y = +1| x).
Примером может служить задача предсказания кликов по рекла мным банне-
рам. При посещении одного и того же сайта один и тот же пользов атель может как
кликнуть, так и не кликнуть по одному и тому же баннеру , из-за чего в выборке
могут появиться одинаковые объекты с разными ответами. При этом важно, чтобы
1
2
классификатор предсказывал именно вероятности классов /emdash.cyr если домножить веро-
ятность первого класса на сумму , которую заплатит заказчик в случае клика, то мы
получим матожидание прибыли при показе этого баннера. На ос нове таких матожи-
даний можно построить алгоритм, выбирающий баннеры для пок аза пользователю.
Итак, рассмотрим точку x пространства объектов. Как мы договорились, в ней
имеется распределение на ответах p(y = +1| x). Допустим, алгоритм b(x) возвращает
числа из отрезка [0, 1]. Наша задача /emdash.cyr выбрать для него такую процедуру обучения,
что в точке x ему будет оптимально выдавать число p(y = +1| x). Если в выборке
объект x встречается n раз с ответами {y1, . . . , y n}, то получаем следующее требова-
ние:
arg min
b∈R
1
n
n∑
i=1
L(yi, b) ≈ p(y = +1| x).
При стремлении n к бесконечности получим, что функционал стремится к матожи -
данию ошибки:
arg min
b∈R
E [L(y, b) | x] =p(y = +1| x).
На семинаре будет показано, что этим свойством обладает , на пример, квадратичная
функция потерь L(y, z ) = (y − z)2, если в ней для положительных объектов исполь-
зовать истинную метку y = 1, а для отрицательных брать y = 0.
Примером функции потерь, которая не позволяет оценивать ве роятности, яв-
ляется модуль отклонения L(y, x) = |y − z|. Можно показать, что с точки зрения
данной функции оптимальным ответом всегда будет либо ноль, либо единица.
Это требование можно воспринимать более просто. Пусть один и тот же объект
встречается в выборке 1000 раз, из которых 100 раз он относит ся к классу +1, и 900
раз /emdash.cyr к классу−1. Поскольку это один и тот же объект , классификатор должен
выдавать один ответ для каждого из тысячи случаев. Можно оце нить матожидание
функции потерь в данной точке по 1000 примеров при прогнозе b:
E
[
L(y, b) | x
]
≈ 100
1000L(1, b) + 900
1000L(−1, b).
Наше требование, по сути, означает , что оптимальный ответ с точки зрения этой
оценки должен быть равен 1/10:
arg min
b∈R
( 100
1000L(1, b) + 900
1000L(−1, b)
)
= 1
10.
§1.2 Правдоподобие и логистические потери
Хотя квадратичная функция потерь и приводит к корректному о цениванию
вероятностей, она не очень хорошо подходит для решения зада чи классификации.
Причиной этому в том числе являются и слишком низкие штрафы з а ошибку /emdash.cyr
так, если объект положительный, а модель выдаёт для него вер оятность первого
класса b(x) = 0, то штраф за это равен всего лишь единице: (1 − 0)2 = 1.
3
Попробуем сконструировать функцию потерь из других сообра жений. Если ал-
горитм b(x) ∈ [0, 1] действительно выдает вероятности, то они должны согласовы -
ваться с выборкой. С точки зрения алгоритма вероятность тог о, что в выборке встре-
тится объект xi с классом yi, равна b(xi)[yi=+1](1−b(xi))[yi=−1]. Исходя из этого, можно
записать правдоподобие выборки (т .е. вероятность получит ь такую выборку с точки
зрения алгоритма):
Q(a, X) =
ℓ∏
i=1
b(xi)[yi=+1](1 − b(xi))[yi=−1].
Данное правдоподобие можно использовать как функционал дл я обучения алгорит-
ма /emdash.cyr с той лишь оговоркой, что удобнее оптимизировать его логарифм:
−
ℓ∑
i=1
([yi = +1] logb(xi) + [yi = −1] log(1− b(xi))) → min
Данная функция потерь называется логарифмической (log-lo ss). Покажем, что
она также позволяет корректно предсказывать вероятности. Запишем матожидание
функции потерь в точке x:
E
[
L(y, b) | x
]
= E
[
−[y = +1] logb − [y = −1] log(1− b) | x
]
=
= −p(y = +1| x) logb − (1 − p(y = +1| x)) log(1− b).
Продифференцируем по b:
∂
∂b E
[
L(y, b) | x
]
= −p(y = +1| x)
b + 1 − p(y = +1| x)
1 − b = 0.
Легко видеть, что оптимальный ответ алгоритма равен вероят ности положительного
класса:
b∗ = p(y = +1| x).
§1.3 Логистическая регрессия
Везде выше мы требовали, чтобы алгоритм b(x) возвращал числа из отрез-
ка [0, 1]. Этого легко достичь, если положить b(x) = σ(⟨w, x⟩), где в качестве σ мо-
жет выступать любая монотонно неубывающая функция с област ью значений [0, 1].
Мы будем использовать сигмоидную функцию: σ(z) = 1
1+exp(−z) . Т аким образом, чем
больше скалярное произведение ⟨w, x⟩, тем больше будет предсказанная вероятность.
Как при этом можно интерпретировать данное скалярное произ ведение? Чтобы от-
ветить на этот вопрос, преобразуем уравнение
p(y = 1| x) = 1
1 + exp(−⟨w, x⟩).
Выражая из него скалярное произведение, получим
⟨w, x⟩ = logp(y = +1| x)
p(y = −1 | x).
4
Получим, что скалярное произведение будет равно логарифму отношения вероятно-
стей классов (log-odds).
Как уже упоминалось выше, при использовании квадратичной ф ункции потерь
алгоритм будет пытаться предсказывать вероятности, но дан ная функция потерь
является далеко не самой лучшей, поскольку слабо штрафует з а грубые ошибки.
Логарифмическая функция потерь подходит гораздо лучше, по скольку не позволяет
алгоритму сильно ошибаться в вероятностях.
Подставим трансформированный ответ линейной модели в лога рифмическую
функцию потерь:
−
ℓ∑
i=1
(
[yi = +1] log 1
1 + exp(−⟨w, xi⟩) + [yi = −1] log exp(−⟨w, xi⟩)
1 + exp(−⟨w, xi⟩)
)
=
= −
ℓ∑
i=1
(
[yi = +1] log 1
1 + exp(−⟨w, xi⟩) + [yi = −1] log 1
1 + exp(⟨w, xi⟩)
)
=
=
ℓ∑
i=1
log (1 + exp(−yi⟨w, xi⟩)) .
Полученная функция в точности представляет собой логистич еские потери, упомя-
нутые в начале. Линейная модель классификации, настроенна я путём минимизации
данного функционала, называется логистической регрессие й. Как видно из приве-
денных рассуждений, она оптимизирует правдоподобие выбор ки и дает корректные
оценки вероятности принадлежности к положительному класс у .
2 Метод опорных векторов
Рассмотрим теперь другой подход к построению функции потер ь, основанный
на максимизации зазора между классами. Будем рассматриват ь линейные класси-
фикаторы вида
a(x) = sign(⟨w, x⟩+ b), w ∈ Rd, b ∈ R.
§2.1 Разделимый случай
Будем считать, что существуют такие параметры w∗ и b∗, что соответствующий
им классификатор a(x) не допускает ни одной ошибки на обучающей выборке. В этом
случае говорят , что выборка линейно разделима.
Пусть задан некоторый классификатор a(x) = sign(⟨w, x⟩ + b). Заметим, что
если одновременно умножить параметры w и b на одну и ту же положительную
константу , то классификатор не изменится. Распорядимся эт ой свободой выбора и
отнормируем параметры так, что
min
x∈X
|⟨w, x⟩+ b| = 1. (2.1)
Можно показать, что расстояние от произвольной точки x0 ∈ Rd до гиперплоскости,
определяемой данным классификатором, равно
ρ(x0, a) = |⟨w, x⟩+ b|
∥w∥ .
5
Т огда расстояние от гиперплоскости до ближайшего объекта о бучающей выборки
равно
min
x∈X
|⟨w, x⟩+ b|
∥w∥ = 1
∥w∥ min
x∈X
|⟨w, x⟩+ b| = 1
∥w∥.
Данная величина также называется отступом (margin).
Т аким образом, если классификатор без ошибок разделяет обу чающую выбор-
ку , то ширина его разделяющей полосы равна 2
∥w∥ . Известно, что максимизация ши-
рины разделяющей полосы приводит к повышению обобщающей сп особности клас-
сификатора [ 1]. Вспомним также, что на повышение обобщающей способности на-
правлена и регуляризация, которая штрафует большую норму в есов /emdash.cyr а чем больше
норма весов, тем меньше ширина разделяющей полосы.
Итак, требуется построить классификатор, идеально раздел яющий обучающую
выборку , и при этом имеющий максимальный отступ. Запишем со ответствующую
оптимизационную задачу , которая и будет определять метод о порных векторов для
линейно разделимой выборки (hard margin support vector mac hine):



1
2∥w∥2 → min
w,b
yi (⟨w, xi⟩+ b) ⩾ 1, i = 1, . . . , ℓ.
(2.2)
Здесь мы воспользовались тем, что линейный классификатор д ает правильный ответ
на объекте xi тогда и только тогда, когда yi(⟨w, xi⟩+ b) ⩾ 0. Более того, из условия
нормировки ( 2.1) следует , что yi(⟨w, xi⟩+ b) ⩾ 1.
В данной задаче функционал является строго выпуклым, а огра ничения линей-
ными, поэтому сама задача является выпуклой и имеет единств енное решение. Более
того, задача является квадратичной и может быть решена край не эффективно.
§2.2 Неразделимый случай
Рассмотрим теперь общий случай, когда выборку невозможно и деально разде-
лить гиперплоскостью. Это означает , что какие бы w и b мы не взяли, хотя бы одно
из ограничений в задаче ( 2.2) будет нарушено:
∃xi ∈ X : yi (⟨w, xi⟩+ b) < 1.
Сделаем эти ограничения /guillemotleft.cyrмягкими/guillemotright.cyr , введя штрафξi ⩾ 0 за их нарушение:
yi (⟨w, xi⟩+ b) ⩾ 1 − ξi, i = 1, . . . , ℓ.
Отметим, что если отступ объекта лежит между нулем и единице й ( 0 ⩽
yi (⟨w, xi⟩+ b) < 1), то объект верно классифицируется, но имеет ненулевой
штраф ξ > 0. Т аким образом, мы штрафуем объекты за попадание внутрь раз де-
ляющей полосы.
Величина 1
∥w∥ в данном случае называется мягким отступом (soft margin) .
С одной стороны, мы хотим максимизировать отступ, с другой /emdash.cyrминимизировать
штраф за неидеальное разделение выборки ∑ ℓ
i=1 ξi. Эти две задачи противоречат
друг другу: как правило, излишняя подгонка под выборку прив одит к маленько-
му отступу , и наоборот /emdash.cyr максимизация отступа приводит к большой ошибке на
6
обучении. В качестве компромисса будем минимизировать взв ешенную сумму двух
указанных величин. Приходим к оптимизационной задаче, соо тветствующей методу
опорных векторов для линейно неразделимой выборки (soft ma rgin support vector
machine)











1
2∥w∥2 + C
ℓ∑
i=1
ξi → min
w,b,ξ
yi (⟨w, xi⟩+ b) ⩾ 1 − ξi, i = 1, . . . , ℓ,
ξi ⩾ 0, i = 1, . . . , ℓ.
(2.3)
Чем больше здесь параметр C, тем сильнее мы будем настраиваться на обучающую
выборку .
Данная задача также является выпуклой и имеет единственное решение.
§2.3 Сведение к безусловной задаче
В начале лекции мы планировали разобраться с двумя функциям и потерь: логи-
стической и кусочно-линейной. Функционал логистической р егрессии действительно
имеет вид верхней оценки на долю неправильных ответов, а вот задача метода опор-
ных векторов ( 2.3) пока выглядит совершенно иначе. Попробуем свести её к зада че
безусловной оптимизации.
Перепишем условия задачи:
{
ξi ⩾ 1 − yi(⟨w, xi⟩+ b)
ξi ⩾ 0
Поскольку при этом в функционале требуется, чтобы штрафы ξi были как можно
меньше, то можно получить следующую явную формулу для них:
ξi = max(0, 1 − yi(⟨w, xi⟩+ b)).
Данной выражение для ξi уже учитывает в себе все ограничения задачи ( 2.3). Значит ,
если подставить его в функционал, то получим безусловную за дачу оптимизации:
1
2∥w∥2 + C
ℓ∑
i=1
max(0, 1 − yi(⟨w, xi⟩+ b)) → min
w,b
Эта задача является негладкой, поэтому решать её может быть достаточно тяжело.
Т ем не менее, она показывает , что метод опорных векторов, по сути, тоже строит
верхнюю оценку вида L(y, z ) = max(0, 1 − yz) на долю ошибок, и добавляет к ней
стандартную квадратичную регуляризацию.
Список литературы
[1] Mohri, M., Rostamizadeh, A., Talwalkar, A. Foundations of Machine Learning. //
MIT Press, 2012.
