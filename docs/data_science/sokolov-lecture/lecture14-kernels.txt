Лекция 13
Ядровые методы
Е. А. Соколов
ФКН ВШЭ
27 января 2022 г.
1 Ядровые методы
Нам уже известно некоторое количество способов преобразов ания признаков:
можно добавлять признаки более высоких порядков, логарифм ировать их или при-
менять другие нелинейные преобразования, отбирать с помощ ью L1-регуляризации
или, например, можно порождать новые признаки с помощью реш ающих деревьев.
На предыдущей лекции мы изучили метод главных компонент , ко торый позволяет
формировать новые признаки как линейные комбинации исходн ых. В данной лекции
мы обсудим ещё один подход к изменению признакового простра нства: ядра, которые
позволяют повышать размерность пространства без вычислит ельных трудностей.
§1.1 Восстановление нелинейных зависимостей линейными
методами
Линейные методы классификации и регрессии являются хорошо изученными и
обоснованными, однако предположение о линейной зависимос ти зачастую оказыва-
ется неверным в задачах машинного обучения. Оказывается, ч то линейные методы
можно применять и для восстановления нелинейных зависимос тей, если предвари-
тельно перейти к новым признакам.
Рассмотрим простой пример. На рис. 1 показана двумерная выборка с двумя
классами, разделяющая поверхность для которой никак не мож ет быть приближена
гиперплоскостью. В то же время, если добавить третий призна к x3 = x2
1+ x2
2, то
выборку можно будет идеально разделить гиперплоскостью ви да x3 = C (рис.
2).
Т акое пространство называется спрямляющим. В новом признаковом пространстве
разделяющая поверхность является линейной, однако после е е проецирования на
исходное пространство она окажется нелинейной.
Модель, в которой зависимость ищется как линейная комбинац ия нелинейных
функций от выборки, называется линейной моделью над базисными функциями [1].
Например, для задачи регрессии она имеет вид
a(x) =
m∑
i=1
wiϕi(x),
1
2
−4 −3 −2 −1 0 1 2 3
−4
−3
−2
−1
0
1
2
3
4
5
Рис. 1. Выборка с нелинейной разделяющей по-
верхностью. Разные классы обозначены разными
цветами.
−4 −3 −2 −1 0 1 2 3
0
2
4
6
8
10
12
14
16
18
20
Рис. 2. Выборка после добавления третьего при-
знака. Изображена проекция на первый и третий
признаки.
где ϕi(x) /emdash.cyr произвольные нелинейные функции от признаков (базисные функции).
Проблема заключается в том, что на практике заранее нельзя с казать, какие именно
базисные функции нужно взять, чтобы добиться линейной разд елимости, поэтому
приходится брать сразу большой набор таких функций (наприм ер, все мономы не
больше определенной степени). В этом случае число признако в оказывается очень
большим, из-за чего процесс обучения становится трудоемки м как по времени, так и
по памяти. Однако в некоторых случаях оказывается, что дост аточно уметь быстро
вычислять скалярные произведения объектов друг на друга.
§1.2 Двойственное представление для линейной регрессии
Рассмотрим задачу построения линейной регрессии с квадрат ичной функцией
потерь и квадратичным регуляризатором:
Q(w) = 1
2
ℓ∑
i=1
{ m∑
j=1
wjϕj(xi) − yi
}2
+ λ
2 ∥w∥2 = 1
2∥Φ w − y∥2 + λ
2 ∥w∥2 → min
w
,
где Φ /emdash.cyr матрица, в которойi-я строка представлена вектором (ϕ1(xi), . . . , ϕ m(xi)).
Дифференцируя функционал Q(w) и приравнивая его нулю, получаем
w = − 1
λΦ T (Φ w − y).
Отсюда следует , что решение является линейной комбинацией строк матрицы Φ :
w = Φ T a,
где за a мы обозначили вектор − 1
λ (Φ w − y). Подставим это представление в функци-
онал:
Q(a) = 1
2∥ΦΦ T a − y∥2 + λ
2 aT ΦΦ T a → min
a
.
3
Заметим, что теперь функционал зависит не от самой матрицы п ризнаков Φ , а от
ее произведения на саму себя ΦΦ T . Это матрица скалярных произведений всех воз-
можных пар объектов, называемая также матрицей Грама. Будем обозначать ее
через
K = ΦΦ T = ( ⟨ϕ(xi), ϕ(xj)⟩)ℓ
i,j=1 = ( k(xi, xj))ℓ
i,j=1,
где ϕ(xi) = ( ϕ1(xi), . . . , ϕ m(xi)), а k(xi, xj) /emdash.cyr скалярное произведение объектов, на-
зываемое также функцией ядра.
Можно показать, что оптимальный вектор a имеет вид
a = ( K + λI)− 1y.
Функция регрессии при этом запишется как
y(x) = ⟨w, ϕ(x)⟩ = wT ϕ(x) = aT Φ ϕ(x) = k(x)T (K + λI)− 1y,
где k(x) = ( k(x, x1), . . . , k (x, xℓ)) /emdash.cyr вектор скалярных произведений нового объектаx
на объекты обучающей выборки.
Итак, нам удалось переписать функционал и модель так, что он и зависят лишь
от скалярных произведений объектов. В этом случае при росте размерности ново-
го (спрямляющего) признакового пространства количество т ребуемой памяти остает-
ся константным и имеет порядок ℓ2. Далее мы покажем, что и вычисление скалярного
произведения можно организовать так, что оно будет зависет ь лишь от размерности
исходного признакового пространства.
§1.3 SVM и kernel trick
Переход к новому признаковому пространству можно применят ь и в задачах
классификации:
a(x) = sign( ⟨w, ϕ(x)⟩+ b).
В частности, к задаче метода опорных векторов можно построи ть двойствен-
ную:

















ℓ∑
i=1
λi − 1
2
ℓ∑
i,j=1
λiλjyiyj⟨xi, xj ⟩ → max
λ
0 ⩽ λi ⩽ C, i = 1 , . . . , ℓ,
ℓ∑
i=1
λiyi = 0 .
После того, как она решена, новые объекты классифицируются с помощью алгоритма
a(x) = sign
( ℓ∑
i=1
λiyi⟨xi, x⟩+ b
)
.
Заметим, что как оптимизационная задача, так и итоговый кла ссификатор зависят
лишь от скалярных произведений объектов. Подставляя вмест о скалярного произ-
ведения функцию ядра, мы будем настраивать классификатор в произвольном при-
знаковом пространстве. Т акая подмена получила в англоязыч ной литературе назва-
ние kernel trick.
4
§1.4 Ядра
Ядром мы будем называть функцию K(x, z), представимую в виде скалярно-
го произведения в некотором пространстве: K(x, z) = ⟨ϕ(x), ϕ(z)⟩, где ϕ : X → H /emdash.cyr
отображение из исходного признакового пространства в неко торое спрямляющее про-
странство. На семинарах будет показано, что ядро содержит в себе много инфор-
мации о спрямляющем пространстве и позволяет производить в нем различные опе-
рации, не зная самого отображения ϕ(x) /emdash.cyr например, находить расстояния между
векторами ϕ(x).
1.4.1 Построение ядер
Самый простой способ задать ядро /emdash.cyr в явном виде построить отображение ϕ(x)
в спрямляющее признаковое пространство. Т огда ядро опреде ляется как скалярное
произведение в этом пространстве: K(x, z) = ⟨ϕ(x), ϕ(z)⟩. При таком способе, одна-
ко, возникают проблемы с ростом вычислительной сложности, о которых уже было
сказано выше.
Допустим, в качестве новых признаков мы хотим взять всевозм ожные произ-
ведения исходных признаков. Определим соответствующее от ображение
ϕ(x) = ( xixj )d
i,j=1 ∈ Rd2
и найдем ядро:
K(x, z) = ⟨ϕ(x), ϕ(z)⟩ = ⟨(xixj )d
i,j=1, (zizj )d
i,j=1⟩ =
=
d∑
i,j=1
xixj zizj =
=
d∑
i=1
xizi
d∑
j=1
xj zj =
= ⟨x, z⟩2.
Т аким образом, ядро выражается через скалярное произведен ие в исходном про-
странстве, и для его вычисления необходимо порядка d операций (в то время как
прямое вычисление ядра потребовало бы O(d2) операций).
1.4.2 Неявное задание ядра
Пример с мономами показал, что можно определить ядро так, чт о оно не будет
в явном виде использовать отображение объектов в новое приз наковое пространство.
Но как убедиться, что функция K(x, z) определяет скалярное произведение в неко-
тором пространстве? Ответ на этот вопрос дает теорема Мерсера: функция K(x, z)
является ядром тогда и только тогда, когда:
1. Она симметрична: K(x, z) = K(z, x).
2. Она неотрицательно определена, то есть для любой конечно й выборки (x1, . . . , x ℓ)
матрица K =
(
K(xi, xj )
)ℓ
i,j=1 неотрицательно определена.
5
Проверять условия теоремы Мерсера, однако, может быть дост аточно трудно.
Поэтому для построения ядер, как правило, пользуются неско лькими базовыми яд-
рами и операциями над ними, сохраняющими симметричность и н еотрицательную
определенность.
Т еорема 1.1 ([
2]). Пусть K1(x, z) и K2(x, z) /emdash.cyr ядра, заданные на множестве X,
f(x) /emdash.cyr вещественная функция на X, ϕ : X → RN /emdash.cyr векторная функция на X, K3 /emdash.cyr
ядро, заданное на RN . Т огда следующие функции являются ядрами:
1. K(x, z) = K1(x, z) + K2(x, z),
2. K(x, z) = αK1(x, z), α > 0,
3. K(x, z) = K1(x, z)K2(x, z),
4. K(x, z) = f(x)f(z),
5. K(x, z) = K3(ϕ(x), ϕ(z)).
Т еорема 1.2 ([3]). Пусть K1(x, z), K2(x, z), . . . /emdash.cyr последовательность ядер, причем
предел
K(x, z) = lim
n→∞
Kn(x, z)
существует для всех x и z. Т огда K(x, z) /emdash.cyr ядро.
Рассмотрим некоторые примеры построения ядер.
1.4.3 Полиномиальные ядра
Пусть p(v) /emdash.cyr многочлен с положительными коэффициентами. На семинарах
будет показано, что K(x, z) = p(⟨x, z⟩) /emdash.cyr ядро. Т акже известно, чтоp(K(x, z)) /emdash.cyr
ядро для любого ядра K(x, z).
Рассмотрим частный случай полиномиального ядра:
Km(x, z) = ( ⟨x, z⟩+ R)m .
Распишем степень, воспользовавшись формулой бинома Ньюто на:
Km(x, z) =
m∑
i=0
Ci
mRm− i⟨x, z⟩i.
Поскольку коэффициенты при скалярных произведениях Ci
mRm− i положительны, то
данное ядро действительно является ядром. Если расписать с калярные произведе-
ния, то можно убедиться, что оно соответствует переводу наб ора признаков во все-
возможные мономы над признаками степени не больше m, причем моном степени i
имеет вес
√
CimRm− i.
Заметим, что параметр R контролирует относительный вес при мономах боль-
ших степеней. Например, отношение веса при мономе степени m−1 к весу при мономе
первой степени равно
√
Cm− 1
m R
C1mRm− 1 =
√
1
Rm− 2 ,
6
то есть по мере увеличения R вес при мономах старших степеней будет становиться
очень небольшим по сравнению с весом при остальных мономах. Можно сказать, что
параметр R контролирует сложность модели.
1.4.4 Гауссовские ядра
Г ауссовское ядро определяется как
K(x, z) = exp
(
−∥x − z∥2
2σ2
)
.
Покажем, что оно действительно является ядром.
Покажем сначала, что функция exp(⟨x, z⟩) является ядром. Представим ее в
виде предела последовательности:
exp(⟨x, z⟩) =
∞∑
k=0
⟨x, z⟩k
k! = lim
n→∞
n∑
k=0
⟨x, z⟩k
k! .
Каждый член предельной последовательности является много членом с положитель-
ными коэффициентами, и поэтому является ядром. Предел суще ствует во всех точ-
ках (x, z), поскольку ряд Т ейлора для функции ex сходится на всей числовой прямой.
Значит , по теореме 1.2 данная функция является ядром.
Аналогично можно доказать, что функция exp(⟨x, z⟩/σ2) также является яд-
ром. Г ауссовское ядро легко получить из данного путём замен ы преобразования ϕ(x)
на ϕ(x)/∥ϕ(x)∥.
Заметим, что можно построить гауссово ядро, используя любо е другое яд-
ро K(x, z). В этом случае оно примет вид
exp
(
−∥ϕ(x) − ϕ(z)∥2
2σ2
)
= exp
(
−K(x, x) − 2K(x, z) + K(z, z)
2σ2
)
.
Здесь мы расписали расстояние между векторами ∥ϕ(x) − ϕ(z)∥2 в спрямляющем
пространстве через функцию ядра.
Спрямляющее пространство. Какому спрямляющему пространству соответству-
ет гауссовское ядро? Оно является пределом последовательн ости полиномиальных
ядер при стремлении степени ядра к бесконечности, что натал кивает на мысль, что
и спрямляющее пространство будет бесконечномерным. Чтобы показать это фор-
мально, нам понадобится следующее утверждение.
Утв. 1.3 ([
4]). Пусть x1, . . . , x ℓ /emdash.cyr различные точки пространства Rd. Т огда матри-
ца
G =
[
exp
(
−∥xi − xj ∥2
2σ2
)] ℓ
i,j=1
является невырожденной при σ > 0.
7
Вспомним также факт из линейной алгебры: матрица Грама сист емы то-
чек x1, . . . , x ℓ невырождена тогда и только тогда, когда эти точки линейно не за-
висимы. Поскольку матрица из утверждения 1.3 является матрицей Грама для то-
чек x1, . . . , x ℓ в спрямляющем пространстве гауссова ядра, то заключаем, чт о в дан-
ном пространстве существует сколь угодно много линейно нез ависимых точек. Зна-
чит , данное пространство является бесконечномерным.
Можно показать это и менее формально. Распишем функцию ядра :
K(x, z) = exp
(
−∥x − z∥2
2σ2
)
= exp
(
−∥x∥2
2σ2
)
exp
(
−∥z∥2
2σ2
)
exp
(
−⟨x, z⟩
σ2
)
=
= {раскладываем экспоненту в ряд } =
= exp
(
−∥x∥2
2σ2
)
exp
(
−∥z∥2
2σ2
) ∞∑
k=0
⟨x, z⟩k
k!σ2k .
Легко видеть, что для получения k-го слагаемого в спрямляющем пространстве
должны быть все мономы степени k над исходными признаками. Поскольку всего
в сумме бесконечное число слагаемых, то и размерность спрям ляющего простран-
ства должна быть бесконечной.
Роль параметра σ. Заметим, что параметр σ в разложении гауссова ядра в ряд
Т ейлора входит в коэффициент перед слагаемым ⟨x, z⟩k как 1/σ2k. Его роль анало-
гична параметру R в полиномиальных ядрах. Маленькие значения σ соответствуют
большим значениям R: чем меньше σ, тем больше вес при мономах большой степени,
тем больше риск переобучения.
Список литературы
[1] Bishop, C.M. Pattern Recognition and Machine Learning. // Springer, 200 6.
[2] Shawe-Taylor, J., Cristianini, N.Kernel Methods for Pattern Analysis. // Cambridge
University Press, 2004.
[3] Sholkopf, B.A., Smola, A.J. Learning with kernels. // MIT Press, 2002.
[4] Micchelli, C.A. Algebraic aspects of interpolation. // Proceedings of Symp osia in
Applied Mathematics, 36:81-102, 1986.
