Лекция 14
Ядра в машинном обучении
Е. А. Соколов
ФКН ВШЭ
27 января 2022 г.
1 Ядровой SVM
Вспомним, что метод опорных векторов сводится к решению зад ачи оптимиза-
ции











1
2∥w∥2 + C
ℓ∑
i=1
ξi → min
w,b,ξ
yi (⟨w, xi⟩+ b) ⩾ 1 − ξi, i = 1 , . . . , ℓ,
ξi ⩾ 0, i = 1 , . . . , ℓ.
(1.1)
Построим двойственную к ней. Запишем лагранжиан:
L(w, b, ξ, λ, µ ) = 1
2∥w∥2 + C
ℓ∑
i=1
ξi −
ℓ∑
i=1
λi [yi (⟨w, xi⟩+ b) − 1 + ξi] −
ℓ∑
i=1
µiξi.
Выпишем условия Куна-Т аккера:
∇wL = w −
ℓ∑
i=1
λiyixi = 0 = ⇒ w =
ℓ∑
i=1
λiyixi (1.2)
∇bL = −
ℓ∑
i=1
λiyi = 0 = ⇒
ℓ∑
i=1
λiyi = 0 (1.3)
∇ξi L = C − λi − µi =⇒ λi + µi = C (1.4)
λi [yi (⟨w, xi⟩+ b) − 1 + ξi] = 0 = ⇒ (λi = 0) или (yi (⟨w, xi⟩+ b) = 1 − ξi)
(1.5)
µiξi = 0 = ⇒ (µi = 0) или (ξi = 0) (1.6)
ξi ⩾ 0, λi ⩾ 0, µi ⩾ 0. (1.7)
Проанализируем полученные условия. Из ( 1.2) следует , что вектор весов, по-
лученный в результате настройки SVM, можно записать как лин ейную комбинацию
объектов, причем веса в этой линейной комбинации можно найт и как решение двой-
ственной задачи. В зависимости от значений ξi и λi объекты xi разбиваются на три
категории:
1
2
1. ξi = 0 , λi = 0 .
Т акие объекты не влияют решение w (входят в него с нулевым весом λi), пра-
вильно классифицируются ( ξi = 0 ) и лежат вне разделяющей полосы. Объекты
этой категории называются периферийными.
2. ξi = 0 , 0 < λ i < C .
Из условия ( 1.5) следует , что yi (⟨w, xi⟩+ b) = 1 , то есть объект лежит строго на
границе разделяющей полосы. Поскольку λi > 0, объект влияет на решение w.
Объекты этой категории называются опорными граничными.
3. ξi > 0, λi = C.
Т акие объекты могут лежать внутри разделяющей полосы ( 0 < ξ i < 2) или
выходить за ее пределы ( ξi ⩾ 2). При этом если 0 < ξ i < 1, то объект клас-
сифицируется правильно, в противном случае /emdash.cyr неправильно.Объекты этой
категории называются опорными нарушителями.
Отметим, что варианта ξi > 0, λi < C быть не может , поскольку при ξi > 0
из условия дополняющей нежесткости ( 1.6) следует , что µi = 0 , и отсюда из уравне-
ния ( 1.4) получаем, что λi = C.
Итак, итоговый классификатор зависит только от объектов, л ежащих на гра-
нице разделяющей полосы, и от объектов-нарушителей (с ξi > 0).
Построим двойственную функцию. Для этого подставим выраже ние ( 1.2) в
лагранжиан, и воспользуемся уравнениями ( 1.3) и ( 1.4) (данные три уравнения вы-
полнены для точки минимума лагранжиана при любых фиксирова нных λ и µ):
L = 1
2





ℓ∑
i=1
λiyixi





2
−
ℓ∑
i,j=1
λiλj yiyj⟨xi, xj ⟩ − b
ℓ∑
i=1
λiyi

 
0
+
ℓ∑
i=1
λi +
ℓ∑
i=1
ξi (C − λi − µi)  
0
=
ℓ∑
i=1
λi − 1
2
ℓ∑
i,j=1
λiλjyiyj⟨xi, xj⟩.
Мы должны потребовать выполнения условий ( 1.3) и ( 1.4) (если они не вы-
полнены, то двойственная функция обращается в минус бескон ечность), а также
неотрицательность двойственных переменных λi ⩾ 0, µi ⩾ 0. Ограничение на µi и
условие ( 1.4), можно объединить, получив λi ⩽ C. Приходим к следующей двой-
ственной задаче:

















ℓ∑
i=1
λi − 1
2
ℓ∑
i,j=1
λiλjyiyj⟨xi, xj ⟩ → max
λ
0 ⩽ λi ⩽ C, i = 1 , . . . , ℓ,
ℓ∑
i=1
λiyi = 0 .
(1.8)
Она также является вогнутой, квадратичной и имеет единстве нный максимум.
Двойственная задача SVM зависит только от скалярных произв едений объек-
тов /emdash.cyr отдельные признаковые описания никак не входят в неё. Значит , можно легко
3
сделать ядровой переход:

















ℓ∑
i=1
λi − 1
2
ℓ∑
i,j=1
λiλjyiyjK(xi, xj ) → max
λ
0 ⩽ λi ⩽ C, i = 1 , . . . , ℓ,
ℓ∑
i=1
λiyi = 0 .
(1.9)
Вернемся к тому , какое представление классификатора дает д войственная за-
дача. Из уравнения ( 1.2) следует , что вектор весов w можно представить как линей-
ную комбинацию объектов из обучающей выборки. Подставляя э то представление w
в классификатор, получаем
a(x) = sign
( ℓ∑
i=1
λiyi⟨xi, x⟩+ b
)
. (1.10)
Т аким образом, классификатор измеряет сходство нового объ екта с объектами из
обучения, вычисляя скалярное произведение между ними. Это выражение также
зависит только от скалярных произведений, поэтому в нём тож е можно перейти к
ядру .
В представлении ( 1.10) фигурирует переменная b, которая не находится непо-
средственно в двойственной задаче. Однако ее легко восстан овить по любому гра-
ничному опорному объекту xi, для которого выполнено ξi = 0 , 0 < λ i < C . Для него
выполнено yi (⟨w, xi⟩+ b) = 1 , откуда получаем
b = yi − ⟨w, xi⟩.
Как правило, для численной устойчивости берут медиану данн ой величины по всем
граничным опорным объектам:
b = med {yi − ⟨w, xi⟩ |ξi = 0 , 0 < λ i < C }.
Связь с kNN. Если использовать гауссовское ядро (или, как его еще называ ют ,
RBF-ядро) в методе опорных векторов, то получится следующе е решающее правило:
a(x) = sign
ℓ∑
i=1
yiλi exp
(
−∥x − xi∥2
2σ2
)
.
Вспомним теперь, что решающее правило в методе k ближайших соседей вы-
глядит как
a(x) = arg max
y∈ Y
Γy(x, Xℓ); Γ y(x, Xℓ) =
ℓ∑
i=1
[y(i)
x = y]w(i, x),
где w(i, x) /emdash.cyr оценка важностиi-го соседа для классификации объекта x, а y(i)
x /emdash.cyr
метка i-го ближайшего соседа. Для случая двух классов {+1, −1} решающее правило
4
можно записать как знак разности оценок за эти классы:
a(x) = sign
(
Γ+1(x, Xℓ) − Γ− 1(x, Xℓ)
)
=
= sign
( ℓ∑
i=1
[y(i)
x = +1] w(i, x) −
ℓ∑
i=1
[y(i)
x = −1]w(i, x)
)
=
= sign
ℓ∑
i=1
([y(i)
x = +1] − [y(i)
x = −1])w(i, x) =
= sign
ℓ∑
i=1
y(i)
x w(i, x).
Заметим, что решающие правила метода опорных векторов с RBF -ядром и ме-
тода k ближайших соседей совпадут , если положить
w(i, x) = λ(i) exp
(
−∥x − x(i)∥2
2σ2
)
.
Т о есть SVM-RBF /emdash.cyr это методℓ ближайших соседей, использующий гауссово ядро в
качестве функции расстояния, и настраивающий веса объекто в путем максимизации
отступов.
2 Аппроксимация спрямляющего пространства
Все ядровые методы используют матрицу Грама G = XX T вместо матри-
цы /guillemotleft.cyrобъекты-признаки/guillemotright.cyrX. Это позволяет сохранять сложность методов при сколь
угодно большой размерности спрямляющего пространства, но работа с матрицей
Грама для больших выборок может стать затруднительной. Т ак , уже при выбор-
ках размером в сотни тысяч объектов хранение этой матрицы по требует большого
количества памяти, а обращение станет трудоёмкой задачей, поскольку требует O(ℓ3)
операций.
Решением данной проблемы может быть построение в явном виде такого пре-
образования ˜ϕ(x), которое переводит объекты в пространство не очень большой раз-
мерности, и в котором можно напрямую обучать любые модели. М ы разберём метод
случайных признаков Фурье (иногда также называется Random Kitchen Sinks) [ 1],
который обладает свойством аппроксимации скалярного прои зведения:
⟨˜ϕ(x), ˜ϕ(z)⟩ ≈ K(x, z).
Из комплексного анализа известно, что любое непрерывное яд ро вида K(x, z) =
= K(x − z) является преобразованием Фурье некоторого вероятностног о распреде-
ления (теорема Бохнера):
K(x − z) =
∫
Rd
p(w)eiwT (x− z)dw.
Преобразуем интеграл:∫
Rd
p(w)eiwT (x− z)dw =
∫
Rd
p(w) cos(wT (x − z))dw + i
∫
Rd
p(w) sin(wT (x − z))dw =
=
∫
Rd
p(w) cos(wT (x − z))dw.
5
Поскольку значение ядра K(x − z) всегда вещественное, то и в правой части мнимая
часть равна нулю /emdash.cyr а значит , остаётся лишь интеграл от косинуса cos wT (x − z). Мы
можем приблизить данный интеграл методом Монте-Карло:
∫
Rd
p(w) cos(wT (x − z))dw ≈ 1
n
n∑
j=1
cos(wT
j (x − z)),
где векторы w1, . . . , w n генерируются из распределения p(w). Используя эти векторы,
мы можем сформировать аппроксимацию преобразования ϕ(x):
˜ϕ(x) = 1√n(cos(wT
1 x), . . . , cos(wT
n x), sin(wT
1 x), . . . , sin(wT
n x)).
Действительно, в этом случае скалярное произведение новых признаков будет иметь
вид
˜K(x, z) = ⟨˜ϕ(x), ˜ϕ(z)⟩ = 1
n
n∑
j=1
(
cos(wT
j x) cos(wT
j z) + sin( wT
j x) sin(wT
j z)
)
= 1
n
n∑
j=1
cos(wT
j (x − z)).
Данная оценка является несмещённой для K(x, z) в силу свойств метода Монте-
Карло. Более того, с помощью неравенств концентрации меры м ожно показать, что
дисперсия данной оценки достаточно низкая. Например, для г ауссова ядра будет
иметь место неравенство для некоторых констант C и ε:
P
[
sup
x,z
| ˜K(x, z) − K(x, z)| ⩾ ε
]
⩽ (C/ε)2 exp(−nε2/4(d + 2)).
Разумеется, найти распределение p(w) можно не для всех ядер K(x − z). Как
правило, данный метод используется для гауссовых ядер exp(∥x−z∥2/2σ2) /emdash.cyr для них
распределение p(w) будет нормальным с нулевым матожиданием и дисперсией σ2.
Список литературы
[1] Rahimi, Ali and Recht, Benjamin Random Features for Large-scale Kernel
Machines. // Proceedings of the 20th International Confere nce on Neural Information
Processing Systems, 2007.
