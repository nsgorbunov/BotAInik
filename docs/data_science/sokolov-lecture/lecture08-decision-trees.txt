Лекция 8
Решающие деревья
Е. А. Соколов
ФКН ВШЭ
8 ноября 2021 г.
Мы уделили достаточно много внимания линейным методам, кот орые обла-
дают рядом важных достоинств: быстро обучаются, способны р аботать с большим
количеством объектов и признаков, имеют небольшое количес тво параметров, легко
регуляризуются. При этом у них есть и серьёзный недостаток /emdash.cyrони могут восста-
навливать только линейные зависимости между целевой перем енной и признаками.
Конечно, можно добавлять в выборку новые признаки, которые нелинейно зависят
от исходных, но этот подход является чисто эвристическим, т ребует выбора типа
нелинейности, а также всё равно ограничивает сложность мод ели сложностью при-
знаков (например, если признаки квадратичные, то и модель с может восстанавливать
только зависимости второго порядка).
В данной лекции мы разберёмся с решающими деревьями /emdash.cyr семейством мо-
делей, которые позволяют восстанавливать нелинейные зави симости произвольной
сложности.
Решающие деревья хорошо описывают процесс принятия решени я во многих
ситуациях. Например, когда клиент приходит в банк и просит в ыдать ему кредит ,
то сотрудник банка начинает проверять условия:
1. Какой возраст у клиента? Если меньше 18, то отказываем в кр едите, иначе
продолжаем.
2. Какая зарплата у клиента? Если меньше 50 тысяч рублей, то п ереходим к шагу
3, иначе к шагу 4.
3. Какой стаж у клиента? Если меньше 10 лет , то не выдаем креди т , иначе выдаем.
4. Есть ли у клиента другие кредиты? Если есть, то отказываем , иначе выдаем.
Т акой алгоритм, как и многие другие, очень хорошо описывает ся решающим дере-
вом. Это отчасти объясняет их популярность.
Первые работы по использованию решающих деревьев для анали за данных по-
явились в 60-х годах, и с тех пор несколько десятилетий им уде лялось очень большое
внимание. Несмотря на свою интерпретируемость и высокую вы разительную способ-
ность, деревья крайне трудны для оптимизации из-за свой дис кретной структуры /emdash.cyr
дерево нельзя продифференцировать по параметрам и найти с п омощью градиент-
ного спуска хотя бы локальный оптимум. Более того, даже числ о параметров у них
1
2
не является постоянным и может меняться в зависимости от глу бины, выбора кри-
териев дробления и прочих деталей. Из-за этого все методы по строения решающих
деревьев являются жадными и эвристичными.
На сегодняшний день решающие деревья не очень часто использ уются как от-
дельные методы классификации или регрессии. В то же время, к ак оказалось, они
очень хорошо объединяются в композиции /emdash.cyr решающие леса, которые являются од-
ними из наиболее сильных и универсальных моделей.
1 Определение решающего дерева
Рассмотрим бинарное дерево, в котором:
• каждой внутренней вершине v приписана функция (или предикат) βv : X →
{0, 1};
• каждой листовой вершине v приписан прогноз cv ∈ Y (в случае с классифика-
цией листу также может быть приписан вектор вероятностей).
Рассмотрим теперь алгоритм a(x), который стартует из корневой вершины v0 и вы-
числяет значение функции βv0 . Если оно равно нулю, то алгоритм переходит в левую
дочернюю вершину , иначе в правую, вычисляет значение преди ката в новой вершине
и делает переход или влево, или вправо. Процесс продолжаетс я, пока не будет до-
стигнута листовая вершина; алгоритм возвращает тот класс, который приписан этой
вершине. Т акой алгоритм называется бинарным решающим деревом.
На практике в большинстве случаев используются одномерные предикаты βv,
которые сравнивают значение одного из признаков с порогом:
βv(x; j, t ) = [xj < t ].
Существуют и многомерные предикаты, например:
• линейные βv(x) = [⟨w, x ⟩ < t ];
• метрические βv(x) = [ρ(x, x v) < t ], где точка xv является одним из объектов
выборки любой точкой признакового пространства.
Многомерные предикаты позволяют строить ещё более сложные разделяющие по-
верхности, но очень редко используются на практике /emdash.cyr например, из-за того, что
усиливают и без того выдающиеся способности деревьев к пере обучению. Далее мы
будем говорить только об одномерных предикатах.
2 Построение деревьев
Легко убедиться, что для любой выборки можно построить реша ющее дерево,
не допускающее на ней ни одной ошибки /emdash.cyr даже с простыми одномерными преди-
катами можно сформировать дерево, в каждом листе которого н аходится ровно по
одному объекту выборки. Скорее всего, это дерево будет пере обученным и не смо-
жет показать хорошее качество на новых данных. Можно было бы поставить задачу
3
поиска дерева, которое является минимальным (с точки зрени я количества листьев)
среди всех деревьев, не допускающих ошибок на обучении /emdash.cyr в этом случае можно
было бы надеяться на наличие у дерева обобщающей способност и. К сожалению, эта
задача является NP-полной, и поэтому приходится ограничив аться жадными алго-
ритмами построения дерева.
Опишем базовый жадный алгоритм построения бинарного решаю щего дерева.
Начнем со всей обучающей выборки X и найдем наилучшее ее разбиение на две
части R1(j, t ) = {x | xj < t } и R2(j, t ) = {x | xj ⩾ t} с точки зрения заранее задан-
ного функционала качества Q(X, j, t ). Найдя наилучшие значения j и t, создадим
корневую вершину дерева, поставив ей в соответствие предик ат [xj < t ]. Объекты
разобьются на две части /emdash.cyr одни попадут в левое поддерево, другие в правое. Для
каждой из этих подвыборок рекурсивно повторим процедуру , п остроив дочерние вер-
шины для корневой, и так далее. В каждой вершине мы проверяем , не выполнилось
ли некоторое условие останова /emdash.cyr и если выполнилось, то прекращаем рекурсию и
объявляем эту вершину листом. Когда дерево построено, кажд ому листу ставится в
соответствие ответ . В случае с классификацией это может быт ь класс, к которому
относится больше всего объектов в листе, или вектор вероятн остей (скажем, веро-
ятность класса может быть равна доле его объектов в листе). Д ля регрессии это
может быть среднее значение, медиана или другая функция от ц елевых переменных
объектов в листе. Выбор конкретной функции зависит от функц ионала качества в
исходной задаче.
Решающие деревья могут обрабатывать пропущенные значения /emdash.cyr ситуации, в
которых для некоторых объектов неизвестны значения одного или нескольких при-
знаков. Для этого необходимо модифицировать процедуру раз биения выборки в вер-
шине, что можно сделать несколькими способами.
После того, как дерево построено, можно провести его стрижку (pruning) /emdash.cyr
удаление некоторых вершин с целью понижения сложности и пов ышения обобщаю-
щей способности. Существует несколько подходов к стрижке, о которых мы немного
упомянем ниже.
Т аким образом, конкретный метод построения решающего дере ва определяется:
1. Видом предикатов в вершинах;
2. Функционалом качестваQ(X, j, t );
3. Критерием останова;
4. Методом обработки пропущенных значений;
5. Методом стрижки.
Т акже могут иметь место различные расширения, связанные с у четом весов объ-
ектов, работой с категориальными признакам и т .д. Ниже мы об судим варианты
каждого из перечисленных пунктов.
3 Критерии информативности
При построении дерева необходимо задать функционал качества, на основе ко-
торого осуществляется разбиение выборки на каждом шаге. Об означим через Rm
4
множество объектов, попавших в вершину , разбиваемую на дан ном шаге, а через Rℓ
и Rr /emdash.cyr объекты, попадающие в левое и правое поддерево соответственно при задан-
ном предикате. Мы будем использовать функционалы следующе го вида:
Q(Rm, j, s ) =H(Rm) − |Rℓ|
|Rm|H(Rℓ) − |Rr|
|Rm|H(Rr).
Здесь H(R) /emdash.cyr этокритерий информативности (impurity criterion), который оцени-
вает качество распределения целевой переменной среди объе ктов множества R. Чем
меньше разнообразие целевой переменной, тем меньше должно быть значение кри-
терия информативности /emdash.cyr и, соответственно, мы будем пытаться минимизировать
его значение. Функционал качества Q(Rm, j, s ) мы при этом будем максимизировать.
Как уже обсуждалось выше, в каждом листе дерево будет выдава ть констан-
ту /emdash.cyr вещественное число, вероятность или класс. Исходя из этого, можно предложить
оценивать качество множества объектов R тем, насколько хорошо их целевые пере-
менные предсказываются константой (при оптимальном выбор е этой константы):
H(R) = min
c∈ Y
1
|R|
∑
(xi,yi)∈ R
L(yi, c ),
где L(y, c ) /emdash.cyr некоторая функция потерь. Далее мы обсудим, какие именно критерии
информативности часто используют в задачах регрессии и кла ссификации.
§3.1 Регрессия
Как обычно, в регрессии выберем квадрат отклонения в качест ве функции по-
терь. В этом случае критерий информативности будет выгляде ть как
H(R) = min
c∈ Y
1
|R|
∑
(xi,yi)∈ R
(yi − c)2.
Как известно, минимум в этом выражении будет достигаться на среднем значении
целевой переменной. Значит , критерий можно переписать в сл едующем виде:
H(R) = 1
|R|
∑
(xi,yi)∈ R

yi − 1
|R|
∑
(xj ,yj )∈ R
yj


2
.
Мы получили, что информативность вершины измеряется её дис персией /emdash.cyr чем ниже
разброс целевой переменной, тем лучше вершина. Разумеется , можно использовать
и другие функции ошибки L /emdash.cyr например, при выборе абсолютного отклонения мы
получим в качестве критерия среднее абсолютное отклонение от медианы.
§3.2 Классификация
Обозначим через pk долю объектов класса k (k ∈ { 1, . . . , K }), попавших в вер-
шину R:
pk = 1
|R|
∑
(xi,yi)∈ R
[yi = k].
Через k∗ обозначим класс, чьих представителей оказалось больше все го среди объ-
ектов, попавших в данную вершину: k∗ = arg max
k
pk.
5
3.2.1 Ошибка классификации
Рассмотрим индикатор ошибки как функцию потерь:
H(R) = min
c∈ Y
1
|R|
∑
(xi,yi)∈ R
[yi ̸= c].
Легко видеть, что оптимальным предсказанием тут будет наиб олее популярный
класс k∗ /emdash.cyr значит , критерий будет равен следующей доле ошибок:
H(R) = 1
|R|
∑
(xi,yi)∈ R
[yi ̸= k∗] = 1− pk∗ .
Данный критерий является достаточно грубым, поскольку учи тывает часто-
ту pk∗ лишь одного класса.
3.2.2 Критерий Джини
Рассмотрим ситуацию, в которой мы выдаём в вершине не один кл асс, а распре-
деление на всех классах c = (c1, . . . , c K ), ∑ K
k=1 ck = 1. Качество такого распределения
можно измерять, например, с помощью критерия Бриера (Brier score):
H(R) = min∑
k ck=1
1
|R|
∑
(xi,yi)∈ R
K∑
k=1
(ck − [yi = k])2.
Можно показать, что оптимальный вектор вероятностей состо ит из долей клас-
сов pk:
c∗ = (p1, . . . , p K )
Если подставить эти вероятности в исходный критерий информ ативности и провести
ряд преобразований, то мы получим критерий Джини:
H(R) =
K∑
k=1
pk(1 − pk).
3.2.3 Энтропийный критерий
Мы уже знакомы с более популярным способом оценивания качес тва вероятно-
стей /emdash.cyr логарифмическими потерями, или логарифмом правдоподобия:
H(R) = min∑
k ck=1

− 1
|R|
∑
(xi,yi)∈ R
K∑
k=1
[yi = k] logck

.
Для вывода оптимальных значений ck вспомним, что все значения ck должны сум-
мироваться в единицу . Как известного из методов оптимизаци и, для учёта этого
ограничения необходимо искать минимум лагранжиана:
L(c, λ ) =− 1
|R|
∑
(xi,yi)∈ R
K∑
k=1
[yi = k] logck + λ
K∑
k=1
ck → min
ck
6
Дифференцируя, получаем:
∂
∂ck
L(c, λ ) =− 1
|R|
∑
(xi,yi)∈ R
[yi = k] 1
ck
+ λ = −pk
ck
+ λ = 0,
откуда выражаем ck = pk/λ . Суммируя эти равенства по k, получим
1 =
K∑
k=1
ck = 1
λ
K∑
k=1
pk = 1
λ ,
откуда λ = 1. Значит , минимум достигается при ck = pk, как и в предыдущем случае.
Подставляя эти выражения в критерий, получим, что он будет п редставлять собой
энтропию распределения классов:
H(R) =−
K∑
k=1
pk log pk.
Из теории вероятностей известно, что энтропия ограничена с низу нулем, при-
чем минимум достигается на вырожденных распределениях ( pi = 1, pj = 0для i ̸= j).
Максимальное же значение энтропия принимает для равномерн ого распределения.
Отсюда видно, что энтропийный критерий отдает предпочтени е более /guillemotleft.cyrвырожден-
ным/guillemotright.cyr распределениям классов в вершине.
4 Критерии останова
Можно придумать большое количестве критериев останова. Пе речислим неко-
торые ограничения и критерии:
• Ограничение максимальной глубины дерева.
• Ограничение минимального числа объектов в листе.
• Ограничение максимального количества листьев в дереве.
• Останов в случае, если все объекты в листе относятся к одному классу .
• Требование, что функционал качества при дроблении улучшал ся как минимум
на s процентов.
С помощью грамотного выбора подобных критериев и их парамет ров можно
существенно повлиять на качество дерева. Т ем не менее, тако й подбор является тру-
дозатратным и требует проведения кросс-валидации.
5 Методы стрижки дерева
Стрижка дерева является альтернативой критериям останова , описанным вы-
ше. При использовании стрижки сначала строится переобучен ное дерево (например,
7
до тех пор, пока в каждом листе не окажется по одному объекту) , а затем произво-
дится оптимизация его структуры с целью улучшения обобщающ ей способности. Су-
ществует ряд исследований, показывающих, что стрижка позв оляет достичь лучшего
качества по сравнению с ранним остановом построения дерева на основе различных
критериев.
Т ем не менее, на данный момент методы стрижки редко использу ются и не
реализованы в большинстве библиотек для анализа данных. Пр ичина заключается
в том, что деревья сами по себе являются слабыми алгоритмами и не представля-
ют большого интереса, а при использовании в композициях они либо должны быть
переобучены (в случайных лесах), либо должны иметь очень не большую глубину (в
бустинге), из-за чего необходимость в стрижке отпадает .
Одним из методов стрижки является cost-complexity pruning. Обозначим де-
рево, полученное в результате работы жадного алгоритма, че рез T0. Поскольку в
каждом из листьев находятся объекты только одного класса, з начение функциона-
ла R(T ) будет минимально на самом дереве T0 (среди всех поддеревьев). Однако
данный функционал характеризует лишь качество дерева на об учающей выборке, и
чрезмерная подгонка под нее может привести к переобучению. Чтобы преодолеть эту
проблему , введем новый функционал Rα(T ), представляющий собой сумму исходного
функционала R(T ) и штрафа за размер дерева:
Rα(T ) =R(T ) +α |T |, (5.1)
где |T | /emdash.cyr число листьев в поддеревеT , а α ⩾ 0 /emdash.cyr параметр. Это один из приме-
ров регуляризованных критериев качества, которые ищут баланс между качеством
классификации обучающей выборки и сложностью построенной модели.
Можно показать, что существует последовательность вложен ных деревьев с
одинаковыми корнями:
TK ⊂ TK− 1 ⊂ · · · ⊂ T0,
(здесь TK /emdash.cyr тривиальное дерево, состоящее из корня дереваT0), в которой каждое
дерево Ti минимизирует критерий ( 5.1) для α из интервала α ∈ [αi, α i+1), причем
0 =α0 < α 1 < · · · < α K < ∞.
Эту последовательность можно достаточно эффективно найти путем обхода дерева.
Далее из нее выбирается оптимальное дерево по отложенной вы борке или с помощью
кросс-валидации.
6 Обработка пропущенных значений
Одним из основных преимуществ решающих деревьев является в озможность
работы с пропущенными значениями. Рассмотрим некоторые ва рианты.
Пусть нам нужно вычислить функционал качества для предикат а β(x) =
= [xj < t ], но в выборке R для некоторых объектов не известно значение призна-
ка j /emdash.cyr обозначим их черезVj. В таком случае при вычислении функционала можно
просто проигнорировать эти объекты, сделав поправку на пот ерю информации от
этого:
Q(R, j, s ) ≈ |R \ Vj|
|R| Q(R \ Vj, j, s ).
8
Затем, если данный предикат окажется лучшим, поместим объе кты из Vj как в левое,
так и в правое поддерево. Т акже можно присвоить им при этом ве са |Rℓ|/ |R| в левом
поддереве и |Rr|/ |R| в правом. В дальнейшем веса можно учитывать, добавляя их
как коэффициенты перед индикаторами [yi = k] во всех формулах.
На этапе применения дерева необходимо выполнять похожий тр юк. Если объ-
ект попал в вершину , предикат которой не может быть вычислен из-за пропуска, то
прогнозы для него вычисляются в обоих поддеревьях, и затем у средняются с весами,
пропорциональными числу обучающих объектов в этих поддере вьях. Иными слова-
ми, если прогноз вероятности для класса k в поддереве Rm обозначается через amk(x),
то получаем такую формулу:
amk(x) =





aℓk(x), β m(x) = 0;
ark (x), β m(x) = 1;
|Rℓ|
|Rm|aℓk(x) + |Rr|
|Rm|ark (x), β m(x) нельзя вычислить .
Другой подход заключается в построении суррогатных предикатов в каждой
вершине. Т ак называется предикат , который использует друг ой признак, но при этом
дает разбиение, максимально близкое к данному .
Отметим, что нередко схожее качество показывают и гораздо б олее простые
способы обработки пропусков /emdash.cyr например, можно заменить всепропуски на ноль.
Для деревьев также разумно будет заменить пропуски в призна ке на числа, кото-
рые превосходят любое значение данного признака. В этом слу чае в дереве можно
будет выбрать такое разбиение по этому признаку , что все объ екты с известными
значениями пойдут в левое поддерево, а все объекты с пропуск ами /emdash.cyr в правое.
7 Учет категориальных признаков
Самый очевидный способ обработки категориальных признако в /emdash.cyr разбивать
вершину на столько поддеревьев, сколько имеется возможных значений у призна-
ка (multi-way splits). Т акой подход может показывать хорош ие результаты, но при
этом есть риск получения дерева с крайне большим числом лист ьев.
Рассмотрим подробнее другой подход. Пусть категориальный признак xj имеет
множество значений Q = {u1, . . . , u q}, |Q| = q. Разобьем множество значений на два
непересекающихся подмножества: Q = Q1 ⊔Q2, и определим предикат как индикатор
попадания в первое подмножество: β(x) = [xj ∈ Q1]. Т аким образом, объект будет
попадать в левое поддерево, если признак xj попадает в множество Q1, и в первое
поддерево в противном случае. Основная проблема заключает ся в том, что для по-
строения оптимального предиката нужно перебрать 2q− 1 − 1 вариантов разбиения,
что может быть не вполне возможным.
Оказывается, можно обойтись без полного перебора в случаях с бинарной клас-
сификацией и регрессией [ 1]. Обозначим через Rm(u) множество объектов, которые
попали в вершину m и у которых j-й признак имеет значение u; через Nm(u) обозна-
чим количество таких объектов.
9
В случае с бинарной классификацией упорядочим все значения категориально-
го признака на основе того, какая доля объектов с таким значе нием имеет класс +1:
1
Nm(u(1))
∑
xi∈ Rm(u(1))
[yi = +1]⩽ . . . ⩽ 1
Nm(u(q))
∑
xi∈ Rm(u(q))
[yi = +1],
после чего заменим категорию u(i) на число i, и будем искать разбиение как для
вещественного признака. Можно показать, что если искать оп тимальное разбиение по
критерию Джини или энтропийному критерию, то мы получим так ое же разбиение,
как и при переборе по всем возможным 2q− 1 − 1 вариантам.
Для задачи регрессии с MSE-функционалом это тоже будет верн о, если упоря-
дочивать значения признака по среднему ответу объектов с та ким значением:
1
Nm(u(1))
∑
xi∈ Rm(u(1))
yi ⩽ . . . ⩽ 1
Nm(u(q))
∑
xi∈ Rm(u(q))
yi.
Именно такой подход используется в библиотеке Spark MLlib 1.
8 Методы построения деревьев
Существует несколько популярных методов построения дерев ьев:
• ID3: использует энтропийный критерий. Строит дерево до тех пор, пока в каж-
дом листе не окажутся объекты одного класса, либо пока разби ение вершины
дает уменьшение энтропийного критерия.
• C4.5: использует критерий Gain Ratio (нормированный энтро пийный крите-
рий). Критерий останова /emdash.cyr ограничение на число объектов в листе. Стриж-
ка производится с помощью метода Error-Based Pruning, кото рый использует
оценки обобщающей способности для принятия решения об удал ении вершины.
Обработка пропущенных значений осуществляется с помощью м етода, который
игнорирует объекты с пропущенными значениями при вычислен ии критерия
ветвления, а затем переносит такие объекты в оба поддерева с определенными
весами.
• CAR T: использует критерий Джини. Стрижка осуществляется с помощью Cost-
Complexity Pruning. Для обработки пропусков используется метод суррогатных
предикатов.
9 Решающие деревья и линейные модели
Как следует из определения, решающее дерево a(x) разбивает всё признаковое
пространство на некоторое количество непересекающихся по дмножеств {J1, . . . , J n},
1http://spark.apache.org/docs/latest/mllib-decision-tree.html
10
и в каждом подмножестве Jj выдаёт константный прогноз wj. Значит , соответству-
ющий алгоритм можно записать аналитически:
a(x) =
n∑
j=1
wj[x ∈ Jj].
Обратим внимание, что это линейная модель над признаками ([x ∈ Jj ])n
j=1 /emdash.cyr а ведь
в начале лекции мы хотели избавиться от линейности! Получае тся, что решающее
дерево с помощью жадного алгоритма подбирает преобразован ие признаков для дан-
ной задачи, а затем просто строит линейную модель над этими п ризнаками. Далее
мы увидим, что многие нелинейные методы машинного обучения можно представить
как совокупность линейных методов и хитрых способов порожд ения признаков.
Список литературы
[1] Hastie T., Tibshirani R., Friedman J. (2009). The Elements of Statistical Learning.
