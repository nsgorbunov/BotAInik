Лекция 12
Обучение без учителя
Е. А. Соколов
ФКН ВШЭ
30 декабря 2021 г.
До сих пор мы изучали методы обучения с учителем /emdash.cyr то есть мето ды, кото-
рые восстанавливают зависимость по объектам с известными о тветами. Если задать
семейство моделей и функционал ошибки, то обучение сводитс я к выбору лучшей
модели с точки зрения этого функционала.
Т акже существует большой класс обучения без учителя (unsupervised learning),
в которых отсутствует целевая переменная, и требуется восс тановить некую скрытую
структуру в данных. Примером может служить визуализация /emdash.cyr задача изображения
многомерной выборки на двухмерной плоскости. Чтобы визуал изация была осмыс-
ленной, при таком отображении нужно сохранить основные зак ономерности данных.
Формализовать требование /guillemotleft.cyrсохранить основные закономерности/guillemotright.cyr тяжело, и поэто-
му строго оценить качество решения данной задачи не предста вляется возможным.
Мы рассмотрим несколько типов задач обучения без учителя, о бсудим методы
их решения и подходы к измерению качества.
1 Кластеризация
Пусть дана выборка объектов X = (xi)ℓ
i=1, xi ∈ X. В задаче кластеризации
требуется выявить в данных K кластеров /emdash.cyr таких областей, что объекты внутри
одного кластера похожи друг на друга, а объекты из разных кла стеров друг на друга
не похожи. Более формально, требуется построить алгоритм a : X → { 1, . . . , K },
определяющий для каждого объекта номер его кластера; число кластеров K может
либо быть известно, либо являться параметром.
Кластеризовать можно много что: новости по сюжетам, пиксел и на изображе-
нии по принадлежности объекту , музыку по жанрам, сообщения на форуме по темам,
клиентов по типу поведения.
В нашей постановке задачи много неточностей /emdash.cyr в частности, м ы не указа-
ли, как измеряется сходство объектов. Как и раньше, начнём о бсуждение задачи с
метрик качества.
§1.1 Метрики качества кластеризации
Существует два подхода к измерению качества кластеризации : внутренний и
внешний. Внутренний основан на некоторых свойствах выборк и и кластеров, а внеш-
1
2
ний использует дополнительные данные /emdash.cyr например, информацию об истинных кла-
стерах.
Приведём несколько примеров внутренних метрик качества. Б удем считать, что
каждый кластер характеризуется своим центром ck.
1. Внутрикластерное расстояние:
K∑
k=1
ℓ∑
i=1
[a(xi) =k]ρ(xi, ck), (1.1)
где ρ(x, z) /emdash.cyr некоторая функция расстояния. Данный функционал требуется
минимизировать, поскольку в идеале все объекты кластера до лжны быть оди-
наковыми.
2. Межкластерное расстояние:
ℓ∑
i,j=1
[a(xi) ̸= a(xj )]ρ(xi, xj ).
Данный функционал нужно максимизировать, поскольку объек ты из разных
кластеров должны быть как можно менее похожими друг на друга .
3. Индекс Данна (Dunn Index):
min1⩽ k<k′⩽ K d(k, k′)
max1⩽ k⩽ K d(k) ,
где d(k, k′) /emdash.cyr расстояние между кластерамиk и k′ (например, евклидово рас-
стояние между их центрами), а d(k) /emdash.cyr внутрикластерное расстояние дляk-го
кластера (например, сумма расстояний от всех объектов этог о кластера до его
центра). Данный индекс необходимо максимизировать.
Внешние метрики возможно использовать, если известно исти нное распределе-
ние объектов по кластерам. В этом случае задачу кластеризац ии можно рассмат-
ривать как задачу многоклассовой классификации, и использ овать любую метрику
оттуда /emdash.cyr F-меру с микро- или макро-усреднением.
§1.2 K-Means
Одним из наиболее популярных методов кластеризации являет ся K-Means, ко-
торый оптимизирует внутрикластерное расстояние ( 1.1), в котором используется
квадрат евклидовой метрики.
Заметим, что в данном функционале имеется две степени свобо ды: центры кла-
стеров ck и распределение объектов по кластерам a(xi). Выберем для этих величин
произвольные начальные приближения, а затем будем оптимиз ировать их по очере-
ди:
1. Зафиксируем центры кластеров. В этом случае внутрикласт ерное расстояние
будет минимальным, если каждый объект будет относиться к то му кластеру ,
чей центр является ближайшим:
a(xi) = arg min
1⩽ k⩽ K
ρ(xi, ck).
3
2. Зафиксируем распределение объектов по кластерам. В этом случае внутрикла-
стерное расстояние с квадратом евклидовой метрики можно пр одифференци-
ровать по центрам кластеров и вывести аналитические формул ы для них:
ck = 1∑ ℓ
i=1[a(xi) =k]
ℓ∑
i=1
[a(xi) =k]xi.
Повторяя эти шаги до сходимости, мы получим некоторое распр еделение объ-
ектов по кластерам. Новый объект относится к тому кластеру , чей центр является
ближайшим.
Результат работы метода K-Means существенно зависит от нач ального прибл-
жения. Существует большое количество подходов к инициализ ации; одним из наибо-
лее успешных считается k-means++.
§1.3 Графовые методы
Графовые методы кластеризации /emdash.cyr это простейшие методы, которые основаны
на построении графа близости. Его вершинами являются объек ты, а выбор рёбер
зависит от конкретного алгоритма. Например, рёбра могут бы ть проведены между
объектами, расстояния между которыми меньше определённог о порога. Кластерами
же объявляются группы объектов, попадающих в одну компонен ту связности.
Т акие подходы очень простые, но при грамотном выборе функци и расстоя-
ния (скажем, обученной под конкретную задачу) могут показы вать очень хорошие
результаты.
§1.4 Иерархическая кластеризация
Описанные выше методы кластеризации находят /guillemotleft.cyrплоскую/guillemotright.cyr структуру класте-
ров. В некоторых задачах возникает потребность в построени и иерархии кластеров,
в которой верхним уровнем является один большой кластер, а н ижним /emdash.cyrℓ класте-
ров, каждый из которых состоит из одного объекта. Например, при кластеризации
новостей можно рассчитывать, что чем ниже мы спускаемся по и ерархии, тем более
тонкие различия между сюжетами будут выделяться.
Одним из подходов является восходящая кластеризация. Она н ачинается с ниж-
него уровня, на котором все объекты принадлежат к отдельным кластерам: Cℓ =
= {{x1}, . . . , {xℓ}}. Каждый следующий уровень Cj получается путём объединения
двух наиболее похожих кластеров с предыдущего уровня Cj+1 = {X1, . . . , X j+1}.
Схожесть кластеров определяется с помощью некоторой функц ии d(Xm, Xn) /emdash.cyr на-
пример, это может быть расстояние между центрами кластеров .
2 Визуализация
Как уже упоминалось выше, задача визуализации состоит в отображении объ-
ектов в двух- или трёхмерное пространство с сохранением отн ошений между ними.
Под сохранением отношений обычно понимают близость попарн ых расстояний в ис-
ходном и в новом пространствах.
4
Т ак, в методе многомерного шкалирования (multidimensiona l scaling, MDS) ми-
нимизируются квадраты отклонений между исходными и новыми попарными рас-
стояниями:
ℓ∑
i̸=j
(ρ(xi, xj ) − ρ(zi, zj ))2 → min
z1,...,zℓ
,
где xi ∈ RD /emdash.cyr исходные объекты, аzi ∈ Rd, 2 ⩽ d ⩽ 3 /emdash.cyr их низкоразмерные проекции.
Обратим внимание на две особенности данного подхода:
• Исходные объекты не обязаны принадлежать евклидову простр анству /emdash.cyr доста-
точно лишь уметь вычислять расстояния между ними. Благодар я этому можно
визуализировать даже сложные объекты вроде строк.
• Проекции объектов ищутся непосредственно, без какой-либо параметрической
зависимости между ними и исходными представлениями объект ов. Из-за это-
го затруднительно добавить к визуализации новые данные. Вп рочем, это и не
нужно /emdash.cyr мы ведь хотим просто нарисовать объекты и посмотретьна них. Если
же требуется отображать и новые, тестовые данные, то следуе т пользоваться
методами понижения размерности, которые преобразуют объе кты с помощью
некоторой модели.
Одним из наиболее популярных методов визуализации на сегод няшний день
является t-distributed stochastic neighbor embedding (t-SNE) , который исправляет
несколько ключевых проблем многомерного шкалирования.
Для начала заметим, что нам не так важно точное сохранение ра сстояний после
проецирования /emdash.cyr достаточно лишь сохранить пропорции. Например, если ρ(x1, x2) =
= αρ(x1, x3), то в новом пространстве достаточно выполнения такого же ра вен-
ства, чтобы соотношения между этими тремя объектами были со хранены: ρ(z1, z2) =
= αρ(z1, z3). Будем использовать нормальную плотность для измерения сх одства объ-
ектов в исходном пространстве:
ρ(xi, xj) = exp
(
−∥xi − xj ∥2
2σ2
)
.
Отнормируем эти близости так, чтобы получить вектор распре делений расстояний
от объекта xj до всех остальных объектов:
p(i | j) = exp(−∥xi − xj ∥2/2σ2
j )∑
k̸=j exp(−∥xk − xj ∥2/2σ2
j )
Данные величины не являются симметричными, что может добав ить нам дополни-
тельных сложностей при дальнейшей работе. Симметризуем их :
pij = p(i | j) +p(j | i)
2ℓ .
Благодаря данному способу симметризации невозможно ситуа ция, в которой для
некоторого отдалённого объекта xi все близости pij будут близки к нулю /emdash.cyr можно
показать, что всегда выполнено ∑
j pij > 1
2ℓ .
5
Перейдём теперь к измерению сходства в новом низкоразмерно м пространстве.
Известно, что в пространствах высокой размерности можно ра зместить объекты так,
что их попарные расстояния будут близки /emdash.cyr а вот сохранить этосвойство в низкораз-
мерном пространстве вряд ли возможно. Поэтому будем измеря ть сходства между
объектами с помощью распределения Коши, которое имеет тяжё лые хвосты и не так
сильно штрафует за увеличение расстояний между объектами:
qij = (1 +∥zi − zj∥2)−1
∑
k̸=m(1 +∥zk − zm∥2)−1
Т еперь мы умеем измерять расстояния между объектами как в ис ходном, так и
в новом пространствах, и осталось лишь задать функционал ош ибки проецирования.
Будем измерять ошибку с помощью дивергенции Кульбака-Лейб лера, которая часто
используется для измерения расстояний между распределени ями:
KL(p || q) =
∑
i̸=j
pij log pij
qij
→ min
z1,...,zℓ
Решать данную задачу оптимизации можно, как всегда, с помощ ью стохастического
градиентного спуска.
3 Обучение представлений
Ещё одной разновидностью задач обучения без учителя являет ся обучение пред-
ставлений (representation learning), которое состоит в построении некоторых число-
вых представлений исходных объектов с сохранением свойств этих объектов.
Мы уже сталкивались с этой областью /emdash.cyr выходы одного из послед них сло-
ёв свёрточной сети являются представлениями изображения, и их можно использо-
вать как признаки при решении той или иной задачи. В данном ра зделе мы разбе-
рём word2vec, способ обучения представлений для слов.
В лингвистике существует дистрибутивная гипотеза, соглас но которой слова,
встречающиеся в похожих контекстах, имеют похожие смыслы. Будем строить пред-
ставления для слов, опираясь на эту гипотезу: чем в более пох ожих контекстах встре-
чаются два слова, тем ближе должны быть соответствующие им в екторы.
Итак, мы хотим для каждого слова w из словаря W найти вектор ⃗ w∈
Rd. Пусть дан некоторый текст x = (w1 . . . w n). Контекстом слова wj будем на-
зывать слова, находящиеся от него на расстоянии не более K /emdash.cyr то есть сло-
ва wj−K , . . . , w j−1, wj+1, . . . , w j+K. Определим через векторы слов вероятность встре-
тить слово wi в контексте слова wj :
p(wi | wj) = exp(⟨⃗ wi, ⃗ wj⟩)∑
w∈W exp(⟨⃗ w, ⃗ wj⟩)
Т огда для выборки текстов X = {x1, . . . , x ℓ}, где текст xi имеет длину ni, можно
определить правдоподобие и максимизировать его:
ℓ∑
i=1
ni∑
j=1
K∑
k=−K
k̸=0
log p(⃗ wj+k | ⃗ wj) → max
{ ⃗ w}w∈W
6
Данный функционал можно оптимизировать стохастическим гр адиентным спуском.
В результате обучения мы получим представления для слов, ко торые, как показы-
вает практика, будут обладать многими интересными свойств ами /emdash.cyr и, в том числе,
близкие по смыслу слова будут иметь близкие векторы.
