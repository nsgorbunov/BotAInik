Лекция 2
Линейная регрессия
Е. А. Соколов
ФКН ВШЭ
25 сентября 2021 г.
1 Линейные модели
На предыдущей лекции мы уже упоминали линейные регрессионн ые модели.
Т акие модели сводятся к суммированию значений признаков с н екоторыми весами:
a(x) = w0 +
d∑
j=1
wj xj . (1.1)
Параметрами модели являются веса или коэффициенты wj. Вес w0 также называется
свободным коэффициентом или сдвигом (bias). Заметим, что сумма в формуле ( 1.1)
является скалярным произведением вектора признаков на век тор весов. Воспользу-
емся этим и запишем линейную модель в более компактном виде:
a(x) = w0 + ⟨w, x⟩, (1.2)
где w = ( w1, . . . , w d) /emdash.cyr вектор весов.
Достаточно часто используется следующий приём, позволяющ ий упростить за-
пись ещё сильнее. Добавим к признаковому описанию каждого о бъекта (d+1)-й при-
знак, равный единице. Вес при этом признаке как раз будет име ть смысл свободного
коэффициента, и необходимость в слагаемом w0 отпадёт:
a(x) = ⟨w, x⟩.
Т ем не менее, при такой форме следует соблюдать осторожност ь и помнить о нали-
чии в выборке специального признака. Например, мы столкнём ся со сложностями,
связанными с этим, когда будем говорить о регуляризации.
За счёт простой формы линейные модели достаточно быстро и ле гко обучают-
ся, и поэтому популярны при работе с большими объёмами данны х. Т акже у них
мало параметров, благодаря чему удаётся контролировать ри ск переобучения и ис-
пользовать их для работы с зашумлёнными данными и с небольши ми выборками.
1
2
2 Области применимости линейных моделей
Сложно представить себе ситуацию, в которой мы берём данные , обучаем ли-
нейную модель и получаем хорошее качество работы. В линейно й модели предпо-
лагается конкретный вид зависимости /emdash.cyr а именно, что каждый признак линейно
влияет на целевую переменную, и что целевая переменная не за висит от каких-либо
комбинаций признаков. Вряд ли это будет выполнено по умолча нию, поэтому обычно
данные требуют специальной подготовки, чтобы линейные мод ели оказались адек-
ватными задаче. Приведём несколько примеров.
Категориальные признаки. Представим себе задачу определения стоимости квар-
тиры по её характеристикам. Одним из важных признаков являе тся район, в котором
находится квартира. Этот признак является категориальным /emdash.cyr его значения нельзя
сравнивать между собой на больше/меньше, их нельзя складыв ать или вычитать.
Непосредственно такие признаки нельзя использовать в лине йных моделях, но есть
достаточно распространённый способ их преобразования.
Допустим, категориальный признак fj (x) принимает значения из множе-
ства C = {c1, . . . , c m}. Заменим его на m бинарных признаков b1(x), . . . , b m(x), каж-
дый из которых является индикатором одного из возможных кат егориальных значе-
ний:
bi(x) = [ fj (x) = ci].
Т акой подход называется one-hot кодированием.
Отметим, что признаки b1(x), . . . , b m(x) являются линейно зависимыми: для лю-
бого объекта выполнено
b1(x) + · · · + bm(x) = 1 .
Чтобы избежать этого, можно выбрасывать один из бинарных пр изнаков. Впрочем,
такое решение имеет и недостатки /emdash.cyr например, если на тестовой выборке появится
новая категория, то её как раз можно закодировать с помощью н улевых бинарных
признаков; при удалении одного из них это потеряет смысл.
Вернёмся к задаче про стоимость квартиры. Если мы применим л инейную мо-
дель к данным после one-hot кодирования признака о районе (д опустим, это f(x)),
то получится такая формула:
a(x) = w1[f(x) = c1]+· · ·+wm[f(x) = cm]+{взаимодействие с другими признаками }.
Т акая зависимость кажется логичной /emdash.cyr каждый район задаёт некоторый базовый
уровень стоимости (например, для района c1 имеем базовую цену w1), а остальные
факторы корректируют его.
Работа с текстами. Перейдём к предсказанию стоимости квартиры по её текстово-
му описанию. Есть простой способ кодирования, который назы вается мешок слов (bag
of words).
Найдём все слова, которые есть в нашей выборке текстов, и про нумеруем
их: {c1, . . . , c m}. Будем кодировать текст m признаками b1(x), . . . , b m(x), где bj (x) ра-
вен количеству вхождений слова cj в текст . Линейная модель над такими признаками
3
будет иметь вид
a(x) = w1b1(x) + · · · + wmbm(x) + . . . ,
и такой вид тоже кажется разумным. Каждое вхождение слова cj меняет прогноз
стоимости на wj . В самом деле, можно ожидать, что слово /guillemotleft.cyrпрестижный/guillemotright.cyr скорее
говорит о том, что квартира дорогая, а слово /guillemotleft.cyrплохой/guillemotright.cyr вряд либудут использовать
при описании приличной квартиры.
Бинаризация числовых признаков. Наконец, подумаем о предсказании стоимо-
сти квартиры по расстоянию до ближайшей станции метро xj . Может оказаться,
что самые дорогие квартиры расположены где-то в 5-10 минута х ходьбы от метро,
а те, что ближе или дальше, стоят не так дорого. В этом случае з ависимость це-
левой переменной от признака не будет линейной. Чтобы сдела ть линейную модель
подходящей, мы можем бинаризовать признак. Для этого выбер ем некоторую сетку
точек {t1, . . . , t m}. Это может быть равномерная сетка между минимальным и мак-
симальным значением признака или, например, сетка из эмпир ических квантилей.
Добавим сюда точки t0 = −∞ и tm+1 = + ∞. Новые признаки зададим как
bi(x) = [ ti− 1 < x j ⩽ ti], i = 1 , . . . , m + 1.
Линейная модель над этими признаками будет выглядеть как
a(x) = w1[t0 < x j ⩽ t1] + · · · + wm+1[tm < x j ⩽ tm+1] + . . . ,
то есть мы найдём свой прогноз стоимости квартиры для каждог о интервала рас-
стояния до метро. Т акой подход позволит учесть нелинейную з ависимость между
признаком и целевой переменной.
3 Измерение ошибки в задачах регрессии
Чтобы обучать регрессионные модели, нужно определиться, к ак именно изме-
ряется качество предсказаний. Будем обозначать через y значение целевой перемен-
ной, через a /emdash.cyr прогноз модели. Рассмотрим несколько способов оценить отклоне-
ние L(y, a) прогноза от истинного ответа.
MSE и R2. Основной способ измерить отклонение /emdash.cyr посчитать квадрат разности:
L(y, a) = ( a − y)2
Благодаря своей дифференцируемости эта функция наиболее ч асто используется в
задачах регрессии. Основанный на ней функционал называетс я среднеквадратичным
отклонением (mean squared error, MSE):
MSE(a, X) = 1
ℓ
ℓ∑
i=1
(a(xi) − yi)2 .
Отметим, что величина среднеквадратичного отклонения пло хо интерпретируется,
поскольку не сохраняет единицы измерения /emdash.cyr так, если мы предсказываем цену
4
в рублях, то MSE будет измеряться в квадратах рублей. Чтобы и збежать этого,
используют корень из среднеквадратичной ошибки (root mean squared error, RMSE):
RMSE(a, X) =



√
1
ℓ
ℓ∑
i=1
(a(xi) − yi)2.
Среднеквадратичная ошибка подходит для сравнения двух мод елей или для
контроля качества во время обучения, но не позволяет сделат ь выводы о том, на-
сколько хорошо данная модель решает задачу . Например, MSE = 10 является очень
плохим показателем, если целевая переменная принимает зна чения от 0 до 1, и очень
хорошим, если целевая переменная лежит в интервале (10000, 100000). В таких си-
туациях вместо среднеквадратичной ошибки полезно использ овать коэффициент де-
терминации (или коэффициент R2):
R2(a, X) = 1 −
∑ ℓ
i=1(a(xi) − yi)2
∑ ℓ
i=1(yi − ¯y)2
,
где ¯y = 1
ℓ
∑ ℓ
i=1 yi /emdash.cyr среднее значение целевой переменной. Коэффициент детерми-
нации измеряет долю дисперсии, объяснённую моделью, в обще й дисперсии целевой
переменной. Фактически, данная мера качества /emdash.cyr это нормированная среднеквад-
ратичная ошибка. Если она близка к единице, то модель хорошо объясняет данные,
если же она близка к нулю, то прогнозы сопоставимы по качеств у с константным
предсказанием.
MAE. Заменим квадрат отклонения на модуль:
L(y, a) = |a − y|
Соответствующий функционал называется средним абсолютны м отклонением (mean
absolute error, MAE):
MAE(a, X) = 1
ℓ
ℓ∑
i=1
|a(xi) − yi| .
Модуль отклонения не является дифференцируемым, но при это м менее чув-
ствителен к выбросам. Квадрат отклонения, по сути, делает о собый акцент на объ-
ектах с сильной ошибкой, и метод обучения будет в первую очер едь стараться умень-
шить отклонения на таких объектах. Если же эти объекты являю тся выбросами (то
есть значение целевой переменной на них либо ошибочно, либо относится к друго-
му распределению и должно быть проигнорировано), то такая р асстановка акцентов
приведёт к плохому качеству модели. Модуль отклонения в это м смысле гораздо
более терпим к сильным ошибкам.
Рассмотрим для примера данные из таблицы 1. Один из объектов /emdash.cyr выброс,
значение целевой переменной на нём радикально отличается о т остальных объектов.
Модель a1(x) почти не ошибается на /guillemotleft.cyrнормальных/guillemotright.cyr объектах, но сильно ошибается
на выбросе. Модель a2(x) подгоняется под выброс ценой ухудшения прогнозов на
остальных объектах. Видно, что первая модель оказывается л учше с точки зрения
5
y a1(x) (a1(x) − y)2 |a1(x) − y| a2(x) (a2(x) − y)2 |a2(x) − y|
1 2 1 1 4 9 3
2 1 1 1 5 9 3
3 2 1 1 6 9 3
4 5 1 1 7 9 3
5 6 1 1 8 9 3
100 7 8649 93 10 8100 90
7 6 1 1 10 9 3
MSE = 1236 MAE = 14 .14 MSE = 1164 MAE = 15 .43
Т аблица 1. Поведение MSE и MAE при наличии выбросов.
MAE, но хуже с точки зрения MSE. Это логично /emdash.cyr у квадратичной функции потерь
штраф за ошибку растёт нелинейно с ростом отклонения прогно за от ответа, а для
абсолютной функции потерь равносильно снижение отклонени я на одну и ту же
величину для нормального объекта и для выброса. Заметим, чт о такая особенность
MAE пропадёт , если в выборке будет много выбросов. Скажем, е сли будет около
половины объектов с аномальными значениями целевой переме нной, то вполне может
стать выгоднее оптимизировать отклонение именно на них.
Приведём ещё одно объяснение того, почему модуль отклонени я устойчив к
выбросам, на простом примере. Допустим, все ℓ объектов выборки имеют одинаковые
признаковые описания, но разные значения целевой переменн ой y1, . . . , y ℓ. В этом
случае модель должна на всех этих объектах выдать один и тот ж е ответ . Если мы
выбрали MSE в качестве функционала ошибки, то получаем след ующую задачу:
1
ℓ
ℓ∑
i=1
(a − yi)2 → min
a
Легко показать, что минимум достигается на среднем значени и всех ответов:
a∗
MSE= 1
ℓ
ℓ∑
i=1
yi.
Если один из ответов на порядки отличается от всех остальных (то есть является
выбросом), то среднее будет существенно отклоняться в его с торону .
Рассмотрим теперь ту же ситуацию, но с функционалом MAE:
1
ℓ
ℓ∑
i=1
|a − yi| → min
a
Т еперь решением будет медиана ответов:
a∗
MAE= median{yi}ℓ
i=1.
Небольшое количество выбросов никак не повлияет на медиану /emdash.cyr она существенно
более устойчива к величинам, выбивающимся из общего распре деления.
6
−4 −3 −2 −1 0 1 2 3 4
y − a
0
1
2
3
4
5
6loss
Robust losses
delta = 0.1
delta = 0.5
delta = 1
delta = 2
log-cosh
abs
Рис. 1. Функция потерь Хубера и Log-Cosh.
В заключение отметим одну проблему , связанную с абсолютной функцией по-
терь. Рассмотрим производные для неё и квадратичной функци и:
∂
∂a |a − y| = sign( a − y), a ̸= y;
∂
∂a (a − y)2 = 2( a − y).
Дальше в курсе мы будем изучать градиентные методы обучения , где параметры мо-
дели постепенно изменяются на основе значений производных функции потерь. Вид-
но, что производная абсолютной функции потерь не зависит от близости прогноза
к правильному ответу , по её значению нельзя понять, насколь ко мы близки к опти-
мальному прогнозу . Из-за этого при оптимизации MAE можно ле гко /guillemotleft.cyrперескочить/guillemotright.cyr
экстремум. Поэтому , как правило, использование этой функц ии потерь приводит к
более долгой и сложной процедуре обучения.
Huber loss. Выше мы обсудили, что абсолютная функция потерь более устой чива
к выбросам, а квадратичная функция лучше с точки зрения опти мизации. Почему
бы не попробовать их объединить? Для прогнозов, близких к от вету , нам бы при-
годились свойства гладкой квадратичной функции, а для плох их прогнозов важнее
свойства абсолютного отклонения. Одним из вариантов таког о объединения является
функция потерь Хубера:
Lδ(y, a) =







1
2(y − a)2, |y − a| < δ
δ
(
|y − a| − 1
2δ
)
, |y − a| ⩾ δ
7
У этой функции потерь есть параметр δ, который регулирует , что мы считаем
за выбросы. Если сделать этот параметр маленьким, то функци я будет вести себя
квадратично только в маленькой окрестности нуля. Если же ув еличивать δ, то даже
для значительных отклонений (a − y) штраф будет вести себя квадратично, и при
обучении мы будем делать большой акцент на их уменьшение. Да нный параметр
надо подбирать, поскольку он может сильно повлиять на решен ие.
Т акже легко, что при δ → 0 функция потерь Хубера вырождается в абсолютную
функцию потерь, а при δ → ∞ /emdash.cyr в квадратичную.
Log-Cosh У функции потерь Хубера есть недостаток: её вторая производ ная имеет
разрывы. Т акого недостатка нет у функции потерь log-cosh:
L(y, a) = log cosh( a − y).
Как и в случае с функцией потерь Хубера, для маленьких отклон ений здесь
имеет место квадратичное поведение, а для больших /emdash.cyr линейное.
Обсужденные нами /guillemotleft.cyrгибридные/guillemotright.cyr функции потерь изображены нарис. 1. От-
метим, что существуют достаточно широкие обобщения этих фу нкций потерь [ 1].
MSLE. Перейдём теперь к логарифмам ответов и прогнозов:
L(y, a) = (log( a + 1) − log(y + 1))2
Соответствующий функционал называется среднеквадратичн ой логарифмической
ошибкой (mean squared logarithmic error, MSLE). Данная мет рика подходит для за-
дач с неотрицательной целевой переменной и неотрицательны ми прогнозами модели.
За счёт логарифмирования ответов и прогнозов мы скорее штра фуем за отклонения
в порядке величин, чем за отклонения в их значениях. Т акже сл едует помнить, что
логарифм не является симметричной функцией, и поэтому данн ая функция потерь
штрафует заниженные прогнозы сильнее, чем завышенные.
MAPE и SMAPE. В задачах прогнозирования нередко измеряется относительн ая
ошибка. Во-первых, это удобно для интерпретации /emdash.cyr легко понять, что /guillemotleft.cyrошибка
50%/guillemotright.cyr соответствует отклонению в полтора раза от целевой переменной. Во-вторых,
это позволяет работать с разными мастштабами. Например, мы можем решать за-
дачу прогнозирования спроса на товары в магазине, и какие-т о товары могут про-
даваться штуками, а какие-то /emdash.cyr тысячами. Чтобы при усреднении ошибок более
популярные товары не оказывали большее влияние на результа т , следует использо-
вать функции потерь, не зависящие от масштаба. Типичный при мер относительной
функции потерь:
L(y, a) =
⏐
⏐
⏐
⏐
y − a
y
⏐
⏐
⏐
⏐
Соответствующий функционал называется средней абсолютно й процентной ошиб-
кой (mean absolute percentage error, MAPE).
У MAPE есть проблем с несимметричностью: скажем, если y = 1 и все прогнозы
неотрицательные, то максимальная ошибка при занижении про гноза ( a < y ) равна
8
единице, а ошибка при завышении прогноза ( a > y ) никак не ограничена сверху .
Это исправляется в симметричной модификации (symmetric me an absolute percentage
error, SMAPE):
L(y, a) = |y − a|
(|y| + |a|)/2
Квантильная функция потерь. В некоторых задачах цены занижения и завы-
шения прогнозов могут отличаться друг от друга. Например, п ри прогнозировании
спроса на товары интернет-магазина гораздо опаснее заниже нные предсказания, по-
скольку они могут привести к потере клиентов. Завышенные же прогнозы приводят
лишь к издержкам на хранение товара на складе. Функционал в э том случае можно
записать как
Q(a, Xℓ) =
ℓ∑
i=1
ρτ (yi − a(xi)),
где
ρτ (z) = ( τ − 1)[z < 0]z + τ[z ⩾ 0]z = ( τ − 1
2)z + 1
2|z|,
а параметр τ лежит на отрезке [0, 1] и определяет соотношение важности занижения
и завышения прогноза. Чем больше здесь τ, тем выше штраф за занижение прогноза.
Обсудим вероятностный смысл данного функционала. Будем сч итать, что в
каждой точке x ∈ X пространства объектов задано вероятностное распределе-
ние p(y | x) на возможных ответах для данного объекта. Т акое распределе ние может
возникать, например, в задаче предсказания кликов по рекла мным баннерам: один
и тот же пользователь может много раз заходить на один и тот же сайт и видеть
данный баннер; при этом некоторые посещения закончатся кли ком, а некоторые /emdash.cyr
нет .
Известно, что при оптимизации квадратичного функционала а лгоритм a(x) бу-
дет приближать условное матожидание ответа в каждой точке п ространства объек-
тов: a(x) ≈ E[y | x]; если же оптимизировать среднее абсолютное отклонение, то ито-
говый алгоритм будет приближать медиану распределения: a(x) ≈ median[p(y | x)].
Рассмотрим теперь некоторый объект x и условное распределение p(y | x). Найдем
число q, которое будет оптимальным с точки зрения нашего функциона ла:
Q =
∫
Y
ρτ (y − q)p(y | x)dy.
Продифференцируем его (при этом необходимо воспользовать ся правилами диффе-
ренцирования интегралов, зависящих от параметра):
∂Q
∂q = (1 − τ)
∫ q
−∞
p(y | x)dy − τ
∫ ∞
q
p(y | x)dy = 0 .
Получаем, что
τ
1 − τ =
∫ q
−∞ p(y | x)dy∫ ∞
q p(y | x)dy .
9
Данное уравнение будет верно, если q будет равно τ-квантили распределения p(y | x).
Т аким образом, использование функции потерь ρτ (z) приводит к тому , что алго-
ритм a(x) будет приближать τ-квантиль распределения ответов в каждой точке про-
странства объектов.
Список литературы
[1] Jonathan T. Barron (2019). A General and Adaptive Robust Loss Function. //
https://arxiv.org/pdf/1701.03077.pdf.
