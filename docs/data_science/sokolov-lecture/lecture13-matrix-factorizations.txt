Лекция 12
Матричные разложения и рекомендательные
системы
Е. А. Соколов
ФКН ВШЭ
22 февраля 2021 г.
1 Понижение размерности и метод главных ком-
понент
В машинном обучении часто возникает задача уменьшения разм ерности при-
знакового пространства. Для этого можно, например, удалят ь признаки, которые
слабо коррелируют с целевой переменной; выбрасывать призн аки по одному и про-
верять качество модели на тестовой выборке; перебирать слу чайные подмножества
признаков в поисках лучших наборов. Ещё одним из подходов к р ешению задачи
является поиск новых признаков, каждый из которых является линейной комбина-
цией исходных признаков. В случае использования квадратич ной функции ошибки
при поиске такого приближения получается метод главных компонент (principal
component analysis, PCA), о котором и пойдет речь.
Пусть X ∈ Rℓ×D /emdash.cyr матрица /guillemotleft.cyrобъекты-признаки/guillemotright.cyr , гдеℓ /emdash.cyr число объектов, аD /emdash.cyr
число признаков. Поставим задачу уменьшить размерность пр остранства до d. Будем
считать, что данные являются центрированными /emdash.cyr то есть среднее в каждом столбце
матрицы X равно нулю.
Будем искать главные компоненты u1, . . . , u D ∈ RD, которые удовлетворяют
следующим требованиям:
1. Они ортогональны: ⟨ui, uj⟩ = 0 , i ̸= j;
2. Они нормированы: ∥ui∥2 = 1 ;
3. При проецировании выборки на компоненты u1, . . . , u d получается максималь-
ная дисперсия среди всех возможных способов выбрать d компонент .
Чтобы понизить размерность выборки до d, мы будем проецировать её на пер-
вые d компонент /emdash.cyr из последнего свойства следует , что это оптимальный способ
снижения размерности.
Дисперсия проецированной выборки показывает , как много ин формации нам
удалось сохранить после понижения размерности /emdash.cyr и поэтому мы требуем макси-
мальной дисперсии от проекций.
1
2
Проекция объекта x на компоненту ui вычисляется как ⟨x, ui⟩ui. Чтобы наши
записи были проще, будем работать не с самими векторами-про екциями, а с коэф-
фициентами ⟨x, ui⟩. Коэффициенты проекций всей выборки на компоненту ui вычис-
ляются как Xui. Если за Ud обозначить матрицу , столбцы которой равны первым d
компонентам, коэффициенты проекций всей выборки на эти ком поненты можно за-
писать в виде матрицы XUd. По сути, j-я строка этой матрицы содержит координаты
объекта xj в базисе u1, . . . , u d. Т огда дисперсия проецированной выборки будет вы-
числяться как след ковариационной матрицы:
tr UT
d XT XUd =
d∑
i=1
∥Xui∥2.
Начнём с первой компоненты. Сведём все требования к ней в опт имизационную
задачу:
{
∥Xu1∥2 → maxu1
∥u1∥2 = 1
Запишем лагранжиан:
L(u1, λ) = ∥Xu1∥2 + λ(∥u1∥2 − 1).
Продифференцируем его и приравняем нулю:
∂L
∂u1
= 2 XT Xu1 + 2λu1 = 0 .
Отсюда получаем, что u1 должен быть собственным вектором ковариационной мат-
рицы XT X. Учтём это и преобразуем функционал:
∥Xu1∥2 = uT
1XT Xu1 = λuT
1u1 = λ → max
u1
Значит , собственный вектор u1 должен соответствовать максимальному собственно-
му значению.
Для следующих компонент к оптимизационной задаче будут доб авляться тре-
бования ортогональности предыдущим компонентам. Решая эт и задачи, мы получим,
что главная компонента ui равна собственному вектору , соответствующему i-му соб-
ственному значению.
После того, как найдены главные компоненты, можно проециро вать на них
и новые данные. Если нам нужно работать с тестовой выборкой X′, то её проекции
вычисляются как Z′ = X′Ud. Отметим также, что в методе главных компонент новые
признаки вычисляются как линейные комбинации старых:
z′
ij =
D∑
k=1
x′
ikukj .
3
Альтернативные постановки. Существует несколько других постановок задачи по-
нижения размерности, приводящих к методу главных компонен т .
Первый способ основан на матричном разложении. Будем искат ь матрицу с
новыми признаковыми описаниями Z ∈ Rℓ×d и матрицу проецирования U ∈ RD×d,
произведение которых даёт лучшее приближение исходной мат рицы X:
∥X − ZU T ∥2 → min
Z,U
Решением данной задачи также являются собственные векторы ковариационной мат-
рицы.
Второй способ состоит в поиске такого линейного подпростра нства, что рас-
стояние от исходных объектов до их проекций на это подпростр анство будет мини-
мальным. В этом случае задача оказывается эквивалентной за даче максимизации
дисперсии проекций.
2 Рекомендательные системы
Рекомендательные системы используются в интернет-магази нах, музыкальных
сервисах, социальных сетях; с их помощью каждому пользоват елю можно подобрать
наиболее интересный товар или, например, фильм. В этой лекц ии мы поговорим об
основных подходах к построению рекомендательных систем (н а основе коллабора-
тивной фильтрации и на основе контента), обсудим методы оце нивания их качества
и некоторые проблемы.
Мы будем рассуждать в терминах пользователей (users, U) и товаров (items,
I), но все методы подходят для рекомендаций любых объектов. Б удем считать, что
для некоторых пар пользователей u ∈ U и товаров i ∈ I известны оценки rui, кото-
рые отражают степень заинтересованности пользователя в то варе. Вычисление таких
оценок /emdash.cyr отдельная тема. Например, в интернет-магазине заинтересованность может
складываться из покупок товара и просмотров его страницы, п ричём покупки долж-
ны учитываться с большим весом. В социальной сети заинтресо ванность в материале
может складываться из времени просмотра, кликов и явного от клика (лайки, репо-
сты); это всё тоже должно суммироваться с различными весами . Не будем сейчас
останавливаться на этом вопросе, а перейдём к основной зада че.
Требуется по известным рейтингам rui научиться строить для каждого пользо-
вателя u набор из k товаров I(u), наиболее подходящих данному пользователю /emdash.cyr то
есть таких, для которых рейтинг rui окажется максимальным.
Самый распространённый подход в данном случае /emdash.cyr сформировать призна-
ки, характеризующие пользователя, товар и их взаимодейств ия, и обучить модель,
которая по данным признакам будет предсказывать рейтинг . Э то может быть ран-
жирующая модель, которая сортирует все товары для данного п ользователя; может
быть и обычная поточечная модель. Ниже мы рассмотрим некото рые простые мето-
ды рекомендаций, оценки которых, как правило, используютс я в качестве признаков
для итоговой модели.
4
§2.1 Коллаборативная фильтрация
Методы коллаборативной фильтрации строят рекомендации дл я пользователя
на основе похожестей между пользователями и товарами. Мы ра ссмотрим два под-
хода к определению сходства.
2.1.1 Memory-based
Два пользователя похожи, если они ставят товарам одинаковы е оценки. Рас-
смотрим двух пользователей u и v Обозначим через Iuv множество товаров i, для
которых известны оценки обоих пользователей:
Iuv = {i ∈ I | ∃rui & ∃rvi}.
Т огда сходство двух данных пользователей можно вычислить ч ерез корреляцию Пир-
сона:
wuv =
∑
i∈Iuv (rui − ¯ru)(rvi − ¯rv)√ ∑
i∈Iuv (rui − ¯ru)2
√ ∑
i∈Iuv (rvi − ¯rv)2
,
где ¯ru и ¯rv /emdash.cyr средние рейтинги пользователей по множеству товаровIuv.
Чтобы вычислять сходства между товарами i и j, введём множество пользова-
телей Uij , для которых известны рейтинги этих товаров:
Uij = {u ∈ U | ∃rui & ∃ruj}.
Т огда сходство двух данных товаров можно вычислить через ко рреляцию Пирсона:
wij =
∑
u∈Uij (rui − ¯ri)(ruj − ¯rj)
√ ∑
u∈Uij (rui − ¯ri)2
√ ∑
u∈Uij (ruj − ¯rj )2
,
где ¯ri и ¯rj /emdash.cyr средние рейтинги товаров по множеству пользователейUij. Отметим, что
существуют и другие способы вычисления похожестей /emdash.cyr например, можно вычислять
скалярные произведения между векторами рейтингов двух тов аров.
Мы научились вычислять сходства товаров и пользователей /emdash.cyr разберём теперь
несколько способов определения товаров, которые стоит рек омендовать пользовате-
лю u0. В подходе на основе сходств пользователей (user-based col laborative ﬁltering)
определяется множество U(u0) пользователей, похожих на данного:
U(u0) = {v ∈ U | wu0v > α }.
После этого для каждого товара вычисляется, как часто он пок упался пользовате-
лями из U(u0):
pi = |{u ∈ U(u0) | ∃rui}|
|U(u0)| .
Пользователю рекомендуются k товаров с наибольшими значениями pi. Данный под-
ход позволяет строить рекомендации, если для данного польз ователя найдутся по-
хожие. Если же пользователь является нетипичным, то подобр ать что-либо не полу-
чится.
5
Т акже существует подход на основе сходств товаров (item-ba sed collaborative
ﬁltering). В нём определяется множество товаров, похожих н а те, которые интересо-
вали данного пользователя:
I(u0) = {i ∈ I | ∃ru0i0 , wi0i > α }.
Затем для каждого товара из этого множества вычисляется его сходство с пользова-
телем:
pi = max
i0:∃ru0i0
wi0i.
Пользователю рекомендуются k товаров с наибольшими значениями pi. Даже если
пользователь нетипичный, то данный подход может найти това ры, похожие на инте-
ресные ему /emdash.cyr и для этого необязательно иметь пользователя сосхожими интересами.
2.1.2 Модели со скрытыми переменными
Все описанные выше подходы требуют хранения разреженной ма трицы R =
= {rui}, которая может быть достаточно большой. Более того, они вес ьма эвристич-
ны и зависят от выбора способа вычисления сходства, способа генерации товаров-
кандидатов, способа их ранжирования. Альтернативой являю тся подходы на основе
моделей со скрытыми переменными (latent factor models).
Мы будем пытаться построить для каждого пользователя u и товара i векто-
ры pu ∈ Rd и qi ∈ Rd, которые будут характеризовать /guillemotleft.cyrкатегории интересов/guillemotright.cyr . На-
пример, каждую компоненту такого вектора можно интерпрети ровать как степень
принадлежности данного товара к определённой категории ил и степень заинтересо-
ванности данного пользователя в этой категории. Разумеетс я, никак не будет гаран-
тироваться, что эти компоненты соответствуют каким-то осм ысленным категориям,
если только мы специально не потребуем этого от модели. По су ти, векторы пользо-
вателей и товаров являются представлениями (embeddings), позволяющими свести
эти сущности в одно векторное пространство.
Сходство пользователя и товара будем вычислять через скаля рное произведе-
ние их представлений:
rui ≈ ⟨pu, qi⟩.
Т акже через скалярное произведение можно вычислять сходст во двух товаров или
двух пользователей.
Мы можем записать функционал ошибки, исходя из способа вычи сления сход-
ства:
∑
(u,i)∈R
(rui − ¯ru − ¯ri − ⟨pu, qi⟩)2 → min
P,Q
(2.1)
Суммирование здесь ведётся по всем парам пользователей и то варов, для которых из-
вестен рейтинг rui. Заметим, что если R′ /emdash.cyr матрицаR с центрированными строками
и столбцами, то данная задача сводится к низкоранговому мат ричному разложению:
∥R′ − P T Q∥2 → min
P,Q
6
Здесь представления пользователей и товаров записаны в сто лбцах матриц P и Q.
Существуют модификации, в которых к скалярным произведени ям добавляется мас-
штабирующий множитель α ∈ R:
∥R′ − αP T Q∥2 → min
P,Q,α
Данный функционал можно регуляризовать:
∑
(u,i)∈R
(rui − ¯ru − ¯ri − ⟨pu, qi⟩)2 + λ
∑
u∈U
∥pu∥2 + µ
∑
i∈I
∥qi∥2 → min
P,Q
(2.2)
Описанная модель носит название Latent Factor Model (LFM).
Отметим, что использование среднеквадратичной ошибки не в сегда имеет
смысл /emdash.cyr в рекомендациях требуется выдать более высокие предсказания для това-
ров, которые более интересны пользователю, но вовсе не треб уется точно предсказы-
вать рейтинги. Впрочем, среднеквадратичную ошибку удобно оптимизировать; более
того, именно она использовалась в качестве функционала в ко нкурсе Netﬂix Prize,
который во многом определил развитие рекомендательных сис тем и в котором было
предложено много популярных сейчас методов.
Существует два основных подхода к решению задачи (
2.1). Первый /emdash.cyr сто-
хастический градиентный спуск, который на каждом шаге случ айно выбирает па-
ру (u, i) ∈ R:
puk := puk + ηqik (rui − ¯ru − ¯ri − ⟨pu, qi⟩) ,
qik := qik + ηpuk (rui − ¯ru − ¯ri − ⟨pu, qi⟩) .
Второй подход основан на особенностях функционала ( 2.1) и называет-
ся ALS (alternating least squares). Можно показать, что это т функционал не явля-
ется выпуклым в совокупности по P и Q, но при это становится выпуклым, если
зафиксировать либо P , либо Q. Более того, оптимальное значение P при фиксиро-
ванном Q (и наоборот) можно выписать аналитически, /emdash.cyr но оно будет содержать
обращение матрицы:
pu =
(∑
i:∃rui
qiqT
i
) −1 ∑
i:∃rui
ruiqi;
qi =
(∑
u:∃rui
pupT
u
) −1
∑
u:∃rui
ruipu;
(здесь через pu и qi мы обозначили столбцы матриц P и Q).
Чтобы избежать сложной операции обращения, будем фиксиров ать всё, кроме
одной строки pk матрицы P или одной строки qk матрицы Q. В этом случае можно
найти оптимальное значение для pk и qk:
pk =
qk(R − ∑
s̸=k psqT
s )T
qkqT
k
,
qk =
pk(R − ∑
s̸=k psqT
s )
pkpT
k
.
Данный подход носит название Hierarchical alternating lea st squares (HALS) [ ?].
7
2.1.3 Учёт неявной информации
Выше мы обсуждали, что интерес пользователя к товару может в ыражаться
по-разному . Это может быть как явный (выставление рейтинга или лайк, написание
рецензии с оценкой), так и неявный (просмотр видео, посещен ие страницы) сигнал.
Неявным сигналам нельзя доверять слишком сильно /emdash.cyr пользователь мог по многим
причинам смотреть страницу товара. При этом неявной информ ации гораздо больше,
и поэтому имеет смысл использовать её при обучении моделей.
Один из способов учёта неявной информации предлагается в ме тоде Implicit
ALS (iALS) [ ?]. Введём показатель неявного интереса пользователя к това ру:
sui =
{
1, ∃rui,
0, иначе.
Здесь мы считаем, что даже если пользователь поставил низку ю оценку товару , то
это всё равно лучше ситуации, в которой пользователь совсем не поставил оценку .
Это не очень сильные рассуждения /emdash.cyr пользователь мог просто не найти товар, и в
таком случае неправильно судить об отсутствии интереса. По этому введём веса cui,
характеризующие уверенность в показателе интереса sui:
cui = 1 + αrui.
Коэффициент α позволяет регулировать влияние явного рейтинга на уверенн ость в
интересе.
Т еперь мы можем задать функционал:
∑
(u,i)∈D
cui (sui − ¯su − ¯si − ⟨pu, qi⟩)2 + λ
∑
u
∥pu∥2 + µ
∑
i
∥qi∥2 → min
P,Q
Как и раньше, обучать его можно с помощью стохастического гр адиентного
спуска, ALS или HALS. Предложенные способы вычисления sui и cui могут изме-
няться в зависимости от специфики задачи.
§2.2 Контентные модели
В коллаборативной фильтрации используется информация о пр едпочтении
пользователей и об их сходствах, но при этом никак не использ уются свойства самих
пользователей или товаров. При этом мы можем обладать допол нительными дан-
ными /emdash.cyr например, текстовыми описаниями или категориями товаров, данными из
профиля пользователя. Из этих данных можно сформировать пр изнаковое описание
пары (пользователь, товар) и пытаться предсказывать рейти нг по этим признакам с
помощью каких-либо моделей (линейных, композиций деревье в и т .д.).
Список литературы
[1] Bishop, C.M. Pattern Recognition and Machine Learning. // Springer, 200 6.
[2] Shawe-Taylor, J., Cristianini, N.Kernel Methods for Pattern Analysis. // Cambridge
University Press, 2004.
8
[3] Sholkopf, B.A., Smola, A.J. Learning with kernels. // MIT Press, 2002.
[4] Micchelli, C.A. Algebraic aspects of interpolation. // Proceedings of Symp osia in
Applied Mathematics, 36:81-102, 1986.
