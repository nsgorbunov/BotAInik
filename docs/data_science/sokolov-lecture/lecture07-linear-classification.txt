Лекция 6
Многоклассовая классификация и
категориальные признаки
Е. А. Соколов
ФКН ВШЭ
8 ноября 2021 г.
Ранее мы разобрались с общим подходом к решению задачи бинар ной класси-
фикации, а также изучили свойства двух конкретных методов: логистической регрес-
сии и метода опорных векторов. Т еперь мы перейдём к более общ ей, многоклассовой
постановке задачи классификации, и попытаемся понять, как можно свести её к уже
известным нам методам.
Т акже мы обсудим методы работы с категориальными признакам и и поймём,
почему на таких данных чаще всего используются линейные мод ели.
1 Многоклассовая классификация
В данном разделе будем считать, что каждый объект относится к одному из K
классов: Y = {1, . . . , K }.
§1.1 Сведение к серии бинарных задач
Мы уже подробно изучили задачу бинарной классификации, и по этому вполне
естественно попытаться свести многоклассовую задачу к наб ору бинарных. Суще-
ствует достаточно много способов сделать это /emdash.cyr мы перечислим лишь два самых
популярных, а об остальных можно почитать, например, [ 1, раздел 4.2.7]
Один против всех (one-versus-all).Обучим K линейных классификаторов b1(x), . . . , b K (x),
выдающих оценки принадлежности классам 1, . . . , K соответственно. Например, в
случае с линейными моделями эти модели будут иметь вид
bk(x) = ⟨wk, x⟩+ w0k.
Классификатор с номером k будем обучать по выборке (xi, 2[yi = k] − 1)ℓ
i=1; иными
словами, мы учим классификатор отличать k-й класс от всех остальных.
Итоговый классификатор будет выдавать класс, соответству ющий самому уве-
ренному из бинарных алгоритмов:
a(x) = arg max
k∈{1,...,K}
bk(x).
1
2
Проблема данного подхода заключается в том, что каждый из кл ассификато-
ров b1(x), . . . , b K (x) обучается на своей выборке, и выходы этих классификаторов
могут иметь разные масштабы, из-за чего сравнивать их будет неправильно [ 2]. Нор-
мировать вектора весов, чтобы они выдавали ответы в одной и т ой же шкале, не
всегда может быть разумным решением /emdash.cyr так, в случае с SVM весаперестанут яв-
ляться решением задачи, поскольку нормировка изменит норм у весов.
Все против всех (all-versus-all). Обучим C2
K классификаторов aij (x), i, j =
= 1 , . . . , K , i ̸= j. Например, в случае с линейными моделями эти модели будут
иметь вид
bk(x) = sign( ⟨wk, x⟩+ w0k).
Классификатор aij (x) будем настраивать по подвыборке Xij ⊂ X, содержащей только
объекты классов i и j:
Xij = {(xn, yn) ∈ X | [yn = i] = 1 или [yn = j] = 1 }.
Соответственно, классификатор aij(x) будет выдавать для любого объекта либо
класс i, либо класс j.
Чтобы классифицировать новый объект , подадим его на вход ка ждого из по-
строенных бинарных классификаторов. Каждый из них проголо сует за своей класс;
в качестве ответа выберем тот класс, за который наберется бо льше всего голосов:
a(x) = arg max
k∈{1,...,K}
K∑
i=1
∑
j̸=i
[aij (x) = k].
§1.2 Многоклассовая логистическая регрессия
Некоторые методы бинарной классификации можно напрямую об общить на
случай многих классов. Выясним, как это можно проделать с ло гистической регрес-
сией.
В логистической регрессии для двух классов мы строили линей ную мо-
дель b(x) = ⟨w, x⟩ + w0, а затем переводили её прогноз в вероятность с помощью
сигмоидной функции σ(z) = 1
1+exp(−z) . Допустим, что мы теперь решаем многоклас-
совую задачу и построили K линейных моделей bk(x) = ⟨wk, x⟩+ w0k, каждая из ко-
торых даёт оценку принадлежности объекта одному из классов . Как преобразовать
вектор оценок (b1(x), . . . , b K (x)) в вероятности? Для этого можно воспользоваться
оператором SoftMax (z1, . . . , z K ), который производит /guillemotleft.cyrнормировку/guillemotright.cyr вектора:
SoftMax(z1, . . . , z K ) =
(
exp(z1)∑ K
k=1 exp(zk)
, . . . , exp(zK )
∑ K
k=1 exp(zk)
)
.
В этом случае вероятность k-го класса будет выражаться как
P (y = k | x, w) = exp (⟨wk, x⟩+ w0k)
∑ K
j=1 exp (⟨wj, x⟩+ w0j )
.
3
Обучать эти веса предлагается с помощью метода максимально го правдоподобия /emdash.cyr
так же, как и в случае с двухклассовой логистической регресс ией:
ℓ∑
i=1
log P (y = yi | xi, w) → max
w1,...,wK
.
§1.3 Многоклассовый метод опорных векторов
(данный материал является опциональным)
В алгоритме /guillemotleft.cyrодин против всех/guillemotright.cyr мынезависимо строили свой классификатор
за каждый класс. Попробуем теперь строить эти классификато ры одновременно, в
рамках одной оптимизационной задачи. Подходов к обобщению метода опорных век-
торов на многоклассовый случай достаточно много; мы разбер ём способ, описанный
в работе [
3].
Для простоты будем считать, что в выборке имеется константн ый признак, и не
будет явно указывать сдвиг b. Будем настраивать K наборов параметров w1, . . . , w K,
и итоговый алгоритм определим как
a(x) = arg max
k∈{1,...,K}
⟨wk, x⟩.
Рассмотрим следующую функцию потерь:
max
k
{
⟨wk, x⟩+ 1 − [k = y(x)]
}
− ⟨wy(x), x⟩. (1.1)
Разберемся сначала с выражением, по которому берется макси мум. Если k = y(x),
то оно равно ⟨wk, x⟩; в противном же случае оно равно ⟨wk, x⟩ + 1 . Если оценка за
верный класс больше оценок за остальные классы хотя бы на еди ницу , то максимум
будет достигаться на k = y(x); в этом случае потеря будет равна нулю. Иначе же
потеря будет больше нуля. Здесь можно увидеть некоторую ана логию с бинарным
SVM: мы штрафуем не только за неверный ответ на объекте, но и з а неуверенную
классификацию (за попадание объекта в разделяющую полосу) .
Рассмотрим сначала линейно разделимую выборку /emdash.cyr т .е. такую, что существу-
ют веса w1∗, . . . , w K∗, при которых потеря ( 1.1) равна нулю. В бинарном SVM мы
строили классификатор с максимальным отступом. Известно, что аналогом отсту-
па для многоклассового случая является норма Фробениуса ма трицы W , k-я строка
которой совпадает с wk:
ρ = 1
∥W ∥2 = 1∑ K
k=1
∑ d
j=1 w2
kj
.
Получаем следующую задачу:



1
2∥W ∥2 → min
W
⟨wyi , xi⟩+ [yi = k] − ⟨wk, xi⟩ ⩾ 1, i = 1 , . . . , ℓ ; k = 1 , . . . , K.
(1.2)
Перейдем теперь к общему случаю. Как и в бинарном методе опор ных векторов,
перейдем к мягкой функции потерь, введя штрафы за неверную и ли неуверенную
4
классификацию. Получим задачу











1
2∥W ∥2 + C
ℓ∑
i=1
ξi → min
W,ξ
⟨wyi , xi⟩+ [yi = k] − ⟨wk, xi⟩ ⩾ 1 − ξi, i = 1 , . . . , ℓ ; k = 1 , . . . , K ;
ξi ⩾ 0, i = 1 , . . . , ℓ.
(1.3)
Решать задачу ( 1.3) можно, например, при помощи пакета SVM multiclass.
Отметим, что такой подход решает проблему с несоизмеримост ью величин, вы-
даваемых отдельными классификаторами (о которой шла речь в подходе /guillemotleft.cyrодин про-
тив всех/guillemotright.cyr): классификаторы настраиваются одновременно, ивыдаваемые ими оценки
должны правильно соотноситься друг с другом, чтобы удовлет ворять ограничениям.
§1.4 Метрики качества многоклассовой классификации
В многоклассовых задачах, как правило, стараются свести по дсчет качества к
вычислению одной из рассмотренных выше двухклассовых метр ик. Выделяют два
подхода к такому сведению: микро- и макро-усреднение.
Пусть выборка состоит из K классов. Рассмотрим K двухклассовых задач, каж-
дая из которых заключается в отделении своего класса от оста льных, то есть целевые
значения для k-й задаче вычисляются как yk
i = [ yi = k]. Для каждой из них можно
вычислить различные характеристики (TP , FP , и т .д.) алгори тма ak(x) = [ a(x) = k];
будем обозначать эти величины как TP k, FPk, FNk, TNk. Заметим, что в двухклас-
совом случае все метрики качества, которые мы изучали, выра жались через эти
элементы матрицы ошибок.
При микро-усреднении сначала эти характеристики усредняю тся по всем клас-
сам, а затем вычисляется итоговая двухклассовая метрика /emdash.cyr например, точность,
полнота или F-мера. Например, точность будет вычисляться п о формуле
precision(a, X) = TP
TP + FP ,
где, например, TP вычисляется по формуле
TP = 1
K
K∑
k=1
TPk.
При макро-усреднении сначала вычисляется итоговая метрик а для каждого
класса, а затем результаты усредняются по всем классам. Нап ример, точность будет
вычислена как
precision(a, X) = 1
K
K∑
k=1
precisionk(a, X); precisionk(a, X) = TPk
TPk + FPk
.
Если какой-то класс имеет очень маленькую мощность, то при м икро-
усреднении он практически никак не будет влиять на результа т , поскольку его вклад
в средние TP , FP , FN и TN будет незначителен. В случае же с макр о-вариантом
усреднение проводится для величин, которые уже не чувствит ельны к соотношению
размеров классов (если мы используем, например, точность и ли полноту), и поэтому
каждый класс внесет равный вклад в итоговую метрику .
5
2 Классификация с пересекающимися классами
У сложним постановку задачи. Будем считать, что в задаче K классов, но те-
перь они могут пересекаться /emdash.cyr каждый объект может относиться одновременно
к нескольким классам. Это означает , что каждому объекту x соответствует век-
тор y ∈ { 0, 1}K, показывающий, к каким классам данный объект относится. Со -
ответственно, обучающей выборке X будет соответствовать матрица Y ∈ { 0, 1}ℓ×K,
описывающая метки объектов; её элемент yik показывает , относится ли объект xi
к классу k. Данная задача в англоязычной литературе носит название multi-label
classiﬁcation. К ней может относиться, например, определение тэгов для фи льма или
категорий для статьи на Википедии.
§2.1 Независимая классификация (Binary relevance)
Самый простой подход к решению данной задачи /emdash.cyr предположить, что все
классы независимы, и определять принадлежность объекта к к аждому отдель-
ным классификатором. Это означает , что мы обучаем K бинарных классификато-
ров a1(x), . . . , a K (x), причём классификатор bk(x) обучается по выборке (xi, yik)ℓ
i=1.
Для нового объекта x целевая переменная оценивается как (a1(x), . . . , a K (x)).
Основная проблема данного подхода состоит в том, что никак н е учитываются
возможные связи между отдельными классами. Т ем не менее, та кие связи могут
иметь место /emdash.cyr например, категории на Википедии имеют древовидную структуру , и
если мы с большой уверенностью отнесли статью к некоторой ка тегории, то из этого
может следовать, что статья относится к одной из категорий- потомков.
§2.2 Стекинг классификаторов
Для учёта корреляций между классами можно воспользоваться следующим
несложным подходом. Разобьём обучающую выборку X на две части X1 и X2. На
первой части обучим K независимых классификаторов b1(x), . . . , b K (x). Далее сфор-
мируем для каждого объекта xi ∈ X2 из второй выборки признаковое описание,
состоящее из прогнозов наших классификаторов:
x′
ik = bk(xi), x i ∈ X2,
получив тем самым выборку X′
2. Обучим на ней новый набор классификато-
ров a1(x′), . . . , a K (x′), каждый из которых определяет принадлежность объекта к
одному из классов. При этом все новые классификаторы опираю тся на прогнозы
классификаторов первого этапа b1(x), . . . , b K (x), и поэтому могут обнаружить свя-
зи между различными классами. Т акой подход называется стекингом и достаточно
часто используется в машинном обучении для усиления моделе й.
Отметим, что обучать классификаторы bk(x) и ak(x) на одной и той же выборке
было бы плохой идеей. Прогнозы базовых моделей bk(x) содержат в себе информацию
об обучающей выборке X1; получается, что новые признаки x′
ik = bk(xi), посчитан-
ные по этой же выборке, по сути будут /guillemotleft.cyrподглядывать/guillemotright.cyr в целевую переменную, и
обучение на них новой модели просто приведёт к переобучению .
Посмотрим на эту проблему несколько иначе. Допустим, мы обу чили на выбор-
ке X1 алгоритм b(x), а затем на этой же выборке обучили второй алгоритм a(b(x)),
6
использующий в качестве единственного признака результат работы b(x). Если мо-
дель b(x) не переобучилась и будет показывать на новых данных такое же качество,
как и на обучающей выборк, то никаких проблем не будет . Т ем не менее, обычно
модели хотя бы немного переобучаются. Будем считать, что на обучении b(x) имеет
среднее отклонение от целевой переменной в 5%, а на новых данных она в сред-
нем ошибается на 10% из-за переобучения. Т огда модель a(x) будет рассчитывать
на среднее отклонение в 5%, но на новых данных ситуация будет другой /emdash.cyr фак-
тически, изменится распределение её признака, что приведё т к не самым лучшим
последствиям.
§2.3 Трансформация пространства ответов
Существуют подходы, которые пытаются в рамках одной модели учитывать
взаимосвязи между классами. Один из них [ 4] предлагает преобразовать простран-
ство ответов так, что классы оказались как можно менее завис имыми. Это можно
сделать с помощью сингулярного разложения матрицы Y :
Y = UΣ V T .
Известно, что если в этом разложении занулить все диагональ ные элементы матри-
цы Σ кроме m наибольших, то мы получим матрицу , наиболее близкую к Y с точки
зрения нормы Фробениуса среди всех матриц ранга m.
Обозначим через VM матрицу , состоящую из тех M столбцов матрицы V , ко-
торые соответствуют наибольшим сингулярным числам. Спрое цируем с её помощью
матрицу Y :
Y VM = Y ′ ∈ Rℓ×M .
Настроим на новые метки Y ′ независимые модели a1(x), . . . , a M (x). Обозначим мат-
рицу прогнозов для нашей выборки через A′ ∈ Rℓ×M . Чтобы получить оценки при-
надлежности исходным классам, переведём матрицу A′ в исходное пространство:
A = A′V T
M .
Далее в лекциях мы будем изучать метод главных компонент и ув идим, что
описанный подход, по сути, аналогичен применению данного м етода к матрице меток.
§2.4 Метрики качества классификации с пересекающимися
классами
Обозначим через Yi множество классов, которым объект xi принадлежит на
самом деле, а через Zi /emdash.cyr множество классов, к которым объект был отнесён алго-
ритмом a(x).
Вполне логичной мерой ошибки будет хэммингово расстояние м ежду этими
множествами /emdash.cyr то есть доля классов, факт принадлежности которым угадан невер-
но:
hamming(a, X) = 1
ℓ
ℓ∑
i=1
|Yi \ Zi| + |Zi \ Yi|
K .
7
Данную метрику необходимо минимизировать.
Стандартные метрики качества классификации можно обобщит ь на multilabel-
задачу так же, как и на случай с непересекающимися классами /emdash.cyrчерез микро- или
макро-усреднение. Есть и несколько другой подход к обобщен ию основных метрик
качества:
accuracy(a, X) = 1
ℓ
ℓ∑
i=1
|Yi ∩ Zi|
|Yi ∪ Zi|,
precision(a, X) = 1
ℓ
ℓ∑
i=1
|Yi ∩ Zi|
|Zi| ,
recall(a, X) = 1
ℓ
ℓ∑
i=1
|Yi ∩ Zi|
|Yi| .
Все эти метрики необходимо максимизировать.
3 Категориальные признаки
Допустим, в выборке имеется категориальный признак, значе ние которого на
объекте x будем обозначать через f(x). Будем считать, что он принимает значения из
множества U = {u1, . . . , u n}. Чтобы использовать такой признак в линейных моделях,
необходимо сначала его закодировать. Существует много под ходов к использованию
категориальных признаков /emdash.cyr о многих из них можно узнать в работе [ 5], мы же
рассмотрим несколько наиболее популярных.
§3.1 Бинарное кодирование (one-hot encoding)
Простейший способ /emdash.cyr создатьn индикаторов, каждый из которых будет отве-
чать за одно из возможных значений признака. Иными словами, мы формируем n
бинарных признаков g1(x), . . . , g n(x), которые определяются как
gj(x) = [ f(x) = uj].
Г лавная проблема этого подхода заключается в том, что на выб орках, где кате-
гориальные признаки имеют миллионы возможных значений, мы получим огромное
количество признаков. Линейные модели хорошо справляются с такими ситуациями
за счёт небольшого количества параметров и достаточно прос тых методов обучения,
и поэтому их часто используют на выборках с категориальными признаками.
§3.2 Бинарное кодирование с хэшированием
Рассмотрим модификацию бинарного кодирования, которая по зволяет уско-
рить процесс вычисления признаков. Выберем хэш-функцию h : U → { 1, 2, . . . , B },
которая переводит значения категориального признака в чис ла от 1 до B. После
этого бинарные признаки можно индексировать значениями хэ ш-функции:
gj(x) = [ h(f(x)) = j], j = 1 , . . . , B.
8
Основное преимущество этого подхода состоит в том, что отпа дает необходи-
мость в хранении соответствий между значениями категориал ьного признака и ин-
дексами бинарных признаков. Т еперь достаточно лишь уметь в ычислять саму хэш-
функцию, которая уже автоматически даёт правильную индекс ацию.
Т акже хэширование позволяет понизить количество признако в (если B < |U|),
причём, как правило, это не приводит к существенной потере к ачества. Это можно
объяснить и с помощью интуиции /emdash.cyr если у категориального признака много зна-
чений, то, скорее всего, большая часть этих значений крайне редко встречается в
выборке, и поэтому не несёт в себе много информации; основну ю ценность представ-
ляют значения u ∈ U, которые много раз встречаются в выборке, поскольку для
них можно установить связи с целевой переменной. Хэширован ие, по сути, случайно
группирует значения признака /emdash.cyr в одну группу попадают значения, получающие
одинаковые индексы h(u). Поскольку /guillemotleft.cyrчастых/guillemotright.cyr значений не так много, вероятность
их попадания в одну группу будет ниже, чем вероятность групп ировки редких зна-
чений.
§3.3 Счётчики
Попытаемся закодировать признаки более экономно. Заметим , что значения
категориального признака нужны нам не сами по себе, а лишь дл я предсказания
класса. Соответственно, если два возможных значения ui и uj характерны для одного
и того же класса, то можно их и не различать.
Определим наш способ кодирования. Вычислим для каждого зна чения u кате-
гориального признака (K + 1) величин:
counts(u, X ) =
∑
(x,y)∈X
[f(x) = u],
successesk(u, X ) =
∑
(x,y)∈X
[f(x) = u][y = k], k = 1 , . . . , K.
По сути, мы посчитали количество объектов с данным значение м признака, а также
количество объектов различных классов среди них.
После того, как данные величины подсчитаны, заменим наш кат егориальный
признак f(x) на K вещественных g1(x), . . . , g K (x):
gk(x, X) = successesk(f(x), X ) + ck
counts(f(x), X ) + ∑ K
m=1 cm
, k = 1 , . . . , K.
Здесь признак gk(x) фактически оценивает вероятность p(y = k | f(x)). Величины ck
являются своего рода регуляризаторами и предотвращают дел ение на ноль в случае,
если в выборке X нет ни одного объекта с таким значением признака. Для просто ты
можно полагать их все равными единице: c1 = · · · = cK = 1 . У признаков gk(x) есть
много названий: счётчики
1, правдоподобия и т .д.
Отметим, что gk(x) можно воспринимать как простейший классификатор /emdash.cyr
значит , при обучении полноценного классификатора на призн аках-счётчиках мы
1http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-
easy-with-counts.aspx
9
рискуем столкнуться с переобучением из-за /guillemotleft.cyrутечки/guillemotright.cyr целевой переменной в значе-
ния признаков (мы уже обсуждали эту проблему , разбираясь со стекингом). Что-
бы избежать переобучения, как правило, пользуются подходо м, аналогичным кросс-
валидации. Выборка разбивается на m частей X1, . . . , X m, и для подвыборки Xi зна-
чения признаков вычисляются на основе статистик, подсчита нных по всем остальным
частям:
x ∈ Xi ⇒ gk(x) = gk(x, X \ Xi).
Можно взять число блоков разбиения равным числу объектов m = ℓ /emdash.cyr в этом слу-
чае значения признаков для каждого объекта будут вычислять ся по статистикам,
подсчитанным по всем остальным объектам.
Для тестовой выборки значения целевой переменной неизвест ны, поэтому на
таких объектах признаки-счётчики вычисляются на основе ст атистик successes (u, X )
и counts (u, X ), подсчитанных по всей обучающей выборке.
При использовании счётчиков нередко используют следующие трюки:
1. К признакам можно добавлять не только дроби gk(x), но и значения counts (f(x), X )
и successes k(f(x), X ).
2. Можно сгенерировать парные категориальные признаки, т . е. для каждой па-
ры категориальных признаков fi(x) и fj (x) создать новый признак fij (x) =
= ( fi(x), fj (x)). После этого счётчики можно вычислить и для парных при-
знаков; при этом общее количество признаков существенно ув еличится, но при
этом, как правило, прирост качества тоже оказывается сущес твенным.
3. Если у категориальных признаков много возможных значени й, то хранение
статистик counts (u, X ) и successes k(u, X ) может потребовать существенного ко-
личества памяти. Для экономии памяти можно хранить статист ику не по самим
значениям категориального признака u ∈ U, а по хэшам от этих значений h(u).
Регулируя количество возможных значений хэш-функции, мож но ограничивать
количество используемой памяти.
4. Можно вычислять несколько счётчиков для разных значений парамет-
ров c1, . . . , c K .
5. Можно все редкие значения категориального признака объе динить в одно, по-
скольку скорее всего, для редких значений не получится каче ственно оценить
статистики successes k и counts. Благодаря этому можно будет сократить рас-
ходы на память при хранении статистики. Более того, можно пр едположить,
что все редкие значения похожи, и относить к данной /guillemotleft.cyrобъединённой/guillemotright.cyr группе
и новые значения признака, которые впервые встретятся на те стовой выборке.
Отметим, что данный подход работает только для задач класси фикации. В
то же время можно пытаться адаптировать его и для задач регре ссии, вычисляя
несколько бинаризаций целевой переменной по разным порога м, и для каждой такой
бинаризации вычисляя счётчики.
10
Список литературы
[1] Мерков. А. Б. Введение в методы статистического обучения. // http://www.
recognition.mccme.ru/pub/RecognitionLab.html/slbook.pdf
[2] Bishop, C.M. Pattern Recognition and Machine Learning. // Springer, 200 6.
[3] Crammer, K., Singer, Y. On the Algorithmic Implementation of Multiclass Kernel-
based V ector Machines. // Journal of Machine Learning Resea rch, 2:265-292, 2001.
[4] Tai, Farbound and Lin, Hsuan-Tien. Multilabel Classiﬁcation with Principal Label
Space T ransformation. // Neural Comput., 24-9, 2012.
[5] Дьяконов А. Г. Методы решения задач классификации с категориальными при-
знаками. // Прикладная математика и информатика. Труды фак ультета Вычис-
лительной математики и кибернетики МГУ имени М.В. Ломоносо ва. 2014.
