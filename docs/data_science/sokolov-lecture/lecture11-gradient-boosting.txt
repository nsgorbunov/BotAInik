Лекция 11
Градиентный бустинг
Е. А. Соколов
ФКН ВШЭ
19 ноября 2021 г.
Мы уже разобрались с двумя типами методов построения композ иций /emdash.cyr бу-
стингом и бэггингом, и познакомились с градиентным бустинг ом и случайным ле-
сом, которые являются наиболее яркими представителями эти х классов. На практике
реализация градиентного бустинга оказывается очень непро стой задачей, в которой
успех зависит от множества тонких моментов. Мы рассмотрим к онкретную реализа-
цию градиентного бустинга /emdash.cyr пакет XGBoost [1], который считается одним из лучших
на сегодняшний день. Это подтверждается, например, активн ым его использованием
в соревнованиях по анализу данных на kaggle.com.
1 Extreme Gradient Boosting (XGBoost)
§1.1 Градиентный бустинг
Вспомним, что на каждой итерации градиентного бустинга выч исляется вектор
сдвигов s, который показывает , как нужно скорректировать ответы ком позиции на
обучающей выборке, чтобы как можно сильнее уменьшить ошибк у:
s =
(
− ∂L
∂z
⏐
⏐
⏐
⏐
z=aN− 1(xi)
)ℓ
i=1
= −∇z
ℓ∑
i=1
L(yi, z i)
⏐
⏐
⏐
zi=aN− 1(xi)
(1.1)
После этого новый базовый алгоритм обучается путем минимиз ации среднеквадра-
тичного отклонения от вектора сдвигов s:
bN (x) = arg min
b∈A
ℓ∑
i=1
(b(xi) − si)2
§1.2 Альтернативный подход
На прошлой лекции мы аргументировали использование средне квадратичной
функции потерь тем, что она наиболее проста для оптимизации . Попробуем найти
более адекватное обоснование этому выбору .
1
2
Мы хотим найти алгоритм b(x), решающий следующую задачу:
ℓ∑
i=1
L(yi, a N− 1(xi) +b(xi)) → min
b
Разложим функцию L в каждом слагаемом в ряд Т ейлора до второго члена с центром
в ответе композиции aN− 1(xi):
ℓ∑
i=1
L(yi, a N− 1(xi) +b(xi)) ≈
≈
ℓ∑
i=1
(
L(yi, a N− 1(xi)) − sib(xi) +1
2hib2(xi)
)
,
где через hi обозначены вторые производные по сдвигам:
hi = ∂2
∂z2 L(yi, z )
⏐
⏐
⏐
⏐
aN− 1(xi)
Первое слагаемое не зависит от нового базового алгоритма, и поэтому его можно
выкинуть. Получаем функционал
ℓ∑
i=1
(
−sib(xi) +1
2hib2(xi)
)
→ min
b
(1.2)
Покажем, что он очень похож на среднеквадратичный из формул ы ( 1.1). Пре-
образуем его:
ℓ∑
i=1
(b(xi) − si)2
=
ℓ∑
i=1
(
b2(xi) − 2sib(xi) +s2
i
)
= {последнее слагаемое не зависит от b}
=
ℓ∑
i=1
(
b2(xi) − 2sib(xi)
)
= 2
ℓ∑
i=1
(
−sib(xi) +1
2b2(xi)
)
→ min
b
Видно, что последняя формула совпадает с ( 1.2) с точностью до константы, если
положить hi = 1. Т аким образом, в обычном градиентном бустинге мы использу ем
аппроксимацию второго порядка при обучении очередного баз ового алгоритма, и
при этом отбрасываем информацию о вторых производных (то ес ть считаем, что
функция имеет одинаковую кривизну по всем направлениям).
3
§1.3 Регуляризация
Будем далее работать с функционалом ( 1.2). Он измеряет лишь ошибку компо-
зиции после добавления нового алгоритма, никак при этом не ш трафуя за излишнюю
сложность этого алгоритма. Ранее мы решали проблему переоб учения путем ограни-
чения глубины деревьев, но можно подойти к вопросу и более ги бко. Мы выясняли,
что дерево b(x) можно описать формулой
b(x) =
J∑
j=1
bj [x ∈ Rj]
Его сложность зависит от двух показателей:
1. Число листьев J. Чем больше листьев имеет дерево, тем сложнее его разделяю-
щая поверхность, тем больше у него параметров и тем выше риск переобучения.
2. Норма коэффициентов в листьях ∥b∥2
2= ∑ J
j=1 b2
j. Чем сильнее коэффициенты
отличаются от нуля, тем сильнее данный базовый алгоритм буд ет влиять на
итоговый ответ композиции.
Добавляя регуляризаторы, штрафующие за оба этих вида сложн ости, получаем сле-
дующую задачу:
ℓ∑
i=1
(
−sib(xi) +1
2hib2(xi)
)
+ γJ + λ
2
J∑
j=1
b2
j→ min
b
Если вспомнить, что дерево b(x) дает одинаковые ответы на объектах, попадающих
в один лист , то можно упростить функционал:
J∑
j=1
{(
−
∑
i∈ Rj
si
)

 
=− Sj
bj + 1
2
(
λ +
∑
i∈ Rj
hi

=Hj
)
b2
j+ γ
}
→ min
b
Каждое слагаемое здесь можно минимизировать по bj независимо. Заметим, что от-
дельное слагаемое представляет собой параболу относитель но bj , благодаря чему
можно аналитически найти оптимальные коэффициенты в листь ях:
bj = Sj
Hj + λ
Подставляя данное выражение обратно в функционал, получае м, что ошибка дерева
с оптимальными коэффициентами в листьях вычисляется по фор муле
H(b) =−1
2
J∑
j=1
S2
j
Hj + λ + γJ (1.3)
4
§1.4 Обучение решающего дерева
Мы получили функционал H(b), который для заданной структуры дерева вы-
числяет минимальное значение ошибки ( 1.2), которую можно получить путем подбо-
ра коэффициентов в листьях. Заметим, что он прекрасно подхо дит на роль критерия
информативности /emdash.cyr с его помощью можно принимать решение, какое разбиение вер-
шины является наилучшим! Значит , с его помощью мы можем стро ить дерево. Будем
выбирать разбиение [xj < t ] в вершине R так, чтобы оно решало следующую задачу
максимизации:
Q = H(R) − H(Rℓ) − H(Rr) → max,
где информативность вычисляется по формуле
H(R) =−1
2

∑
(hi,si)∈ R
sj


2 / 
∑
(hi,si)∈ R
hj + λ

+ γ.
За счет этого мы будем выбирать структуру дерева так, чтобы о но как можно луч-
ше решало задачу минимизации исходной функции потерь. При э том можно ввести
вполне логичный критерий останова: вершину нужно объявить листом, если даже
лучшее из разбиений приводит к отрицательному значению фун кционала Q.
§1.5 Заключение
Итак, градиентный бустинг в XGBoost имеет ряд важных особен ностей.
1. Базовый алгоритм приближает направление, посчитанное с учетом вторых про-
изводных функции потерь.
2. Функционал регуляризуется /emdash.cyr добавляются штрафы за количество листьев и
за норму коэффициентов.
3. При построении дерева используется критерий информатив ности, зависящий
от оптимального вектора сдвига.
4. Критерий останова при обучении дерева также зависит от оп тимального сдвига.
2 Стекинг
Разумеется, существуют способы построения композиций пом имо бустинга и
бэггинга. Большую популярность имеет стекинг, в котором прогнозы алгоритмов
объявляются новыми признаками, и поверх них обучается ещё о дин алгоритм (кото-
рый иногда называют мета-алгоритмом). Стекинг очень попул ярен в соревнованиях
по анализу данных, поскольку позволяет агрегировать разны е модели (различные
композиции, линейные модели, нейросети и т .д.; иногда в кач естве базовых алго-
ритмов могут выступать результаты градиентного бустинга с разными значениями
гиперпараметров).
5
Допустим, мы независимо обучили N базовых алгоритмов b1(x), . . . , b N (x) на
выборке X, и теперь хотим обучить на их прогнозах мета-алгоритм a(x). Самым
простым вариантом будет обучить его на этой же выборке:
ℓ∑
i=1
L(yi, a (b1(xi), . . . , b N (xi))) → min
a
При таком подходе a(x) будет отдавать предпочтение тем базовым алгоритмам, ко-
торые сильнее всех подогнались под целевую переменную на об учении (поскольку по
их прогнозам лучше всего восстанавливаются истинные ответ ы). Если среди базовых
алгоритмов будет идеально переобученный (то есть запомнив ший ответы на всей обу-
чающей выборке), то мета-алгоритму будет выгодно использо вать только прогнозы
данного переобученного базового алгоритма, поскольку это позволит добиться луч-
ших результатов с точки зрения записанного функционала. Пр и этом такой мета-
алгоритм, конечно, будет показывать очень низкое качество на новых данных.
Чтобы избежать таких проблем, следует обучать базовые алго ритмы и мета-
алгоритм на разных выборках. Разобьём нашу обучающую выбор ку на K бло-
ков X1, . . . , X K , и обозначим через b− k
j (x) базовый алгоритм bj (x), обученный по
всем блокам, кроме k-го. Т огда функционал для обучения мета-алгоритма можно
записать как
K∑
k=1
∑
(xi,yi)∈ Xk
L
(
yi, a (b− k
1 (xi), . . . , b − k
N (xi))
)
→ min
a
В данном случае при вычислении ошибки мета-алгоритма на объ екте xi использу-
ются базовые алгоритмы, которые не видели этот объект при об учении, и поэтому
мета-алгоритм не может переобучиться на их прогнозах.
Блендинг .Частным случаем стекинга является блендинг , в котором мета -алгоритм
является линейным:
a(x) =
N∑
n=1
wnbn(x).
Это самый простой способ объединить несколько алгоритмов в композицию. Иногда
даже блендинг без обучения весов (то есть вариант с w1 = · · · = wN = 1/N ) позволяет
улучшить качество по сравнению с отдельными базовыми алгор итмами.
Категориальные и текстовые признаки. Категориальные и текстовые признаки
могут быть серьёзной помехой для использования композиций над деревьями. Стан-
дартным способом кодирования является бинаризация для кат егориальных призна-
ков и TF-IDF для текстовых, что приводит к очень большой разм ерности признако-
вого пространства. Случайный лес на таком наборе признаков будет обучаться долго
из-за большой глубины деревьев, а градиентный бустинг може т показать слишком
плохие результаты из-за небольшой глубины базовых деревье в (например, глубина 4
позволяет учитывать лишь зависимость целевой переменной о т наборов из 4-х при-
знаков; в случае с текстами ответы могут зависеть от существ енно более крупных
наборов слов).
6
Одним из решений этой проблемы может стать стекинг , в которо м градиентным
бустингом обучается мета-алгоритм a(x), а каждый категориальный и текстовый
признак схлопывается в одно число соответствующим базовым алгоритмом. Для ка-
тегориальных признаков базовый алгоритм, например, может вычислять счётчики /emdash.cyr
при этом обратим внимание, что мы уже отмечали важность разд еления обучающих
выборок для счётчиков и настраиваемых поверх них моделей. Т акже популярным
выбором для базовых алгоритмов являются линейные модели.
Список литературы
[1] Tianqi Chen, Carlos Guestrin (2016). XGBoost: A Scalable T ree Boosting System. //
http://arxiv.org/abs/1603.02754
