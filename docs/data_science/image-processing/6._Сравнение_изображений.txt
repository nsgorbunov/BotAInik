Сравнение изображений:
локальные признаки
Высшая школа цифровой культуры
Университет ИТМО
dc@itmo.ru
Содержание
1 Локальные признаки изображений: основные моменты 2
1.1 Сопоставление изображений с ключевыми точками и локаль-
ные признаки . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.1 Использование угловых точек в качестве точек интереса 5
1.1.2 Детектор Харриса: математика . . . . . . . . . . . . . . . 5
1.1.3 Выбор масштаба . . . . . . . . . . . . . . . . . . . . . . . 11
1.1.4 Детектор LoG . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.1.5 Обнаружение объектов округлой формы с помощью DoG 14
1.1.6 Локальные дескрипторы - сравнение элементов . . . . . . 17
1.1.7 Дескриптор SIFT: основные этапы . . . . . . . . . . . . . 17
1.2 Какую модель выбрать? . . . . . . . . . . . . . . . . . . . . . . . 20
1.2.1 Параметрическая модель . . . . . . . . . . . . . . . . . . 21
1.3 Параметры модели для поиска прямой . . . . . . . . . . . . . . 21
1.3.1 Как выбрать параметры модели: общие рекомендации . . 21
1.3.2 Методы поиска модели . . . . . . . . . . . . . . . . . . . . 22
1.3.3 Устойчивые оценки . . . . . . . . . . . . . . . . . . . . . . 25
1.3.4 Консенсус случайной выборки (RANSAC) . . . . . . . . . 26
1.3.5 Что если прямых несколько? . . . . . . . . . . . . . . . . 28
1.3.6 Методы голосования . . . . . . . . . . . . . . . . . . . . . 29
1.3.7 Последовательный консенсус случайной выборки
(RANSAC) . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
1.3.8 Преобразование Хафа . . . . . . . . . . . . . . . . . . . . 30
Высшая школа цифровой культуры Университет ИТМО
1 Локальные признаки изображений: основные
моменты
В начале нашего обсуждения видов признаков изображений мы упомя-
нули, что их можно разделить на глобальные и локальные. Как глобальные,
так и локальные признаки могут представлять определенные характеристики
изображения - цвет, текстуру, форму, ориентацию краев. Однако, глобальные
признаки характеризуют все изображение в целом, а локальные признаки
описывают конкретный район, только часть изображения. Давайте теперь
посмотрим, почему важны локальные признаки, и рассмотрим пару наибо-
лее ярких примеров локальных признаков.
Когда глобальных признаков недостаточноЕстьмногослучаев,когда
глобальные признаки недостаточно устойчивы для сопоставления изображе-
ний. Например, если у нас имеется два изображения одного и того же объ-
екта в разном масштабе, то сопоставить область одного изображения с дру-
гим изображением используя только глобальные признаки довольно сложно.
Если изображения сняты с разных точек обзора, под разными условиями
освещенности, с перекрытиями - нам нужно иметь возможность сопоставить
локальные участки изображения одного изображения с локальными участ-
ками другого изображения.
Использование локальных признаков С помощью локальных призна-
ков мы можем сопоставлять части изображений, даже если глобальные при-
знаки не совпадают. Хорошо, было бы неплохо уметь сопоставлять не це-
лые изображения, а их фрагменты. Но какие фрагменты нужно сравнивать?
Как правильно выбирать наиболее информативные фрагменты? А также как
лучше всего представить эти фрагменты? Нам нужно найти способ решить,
какой фрагмент изображения выбрать, а затем как построить векторы при-
знаков, представляющие эти фрагменты.
Как выбрать и сравнить фрагменты?Итак, как мы можем сопоста-
вить фрагменты разных изображений? Допустим, мы хотим доказать, что
объект на показанном на экране втором изображении меньшего размера со-
ответствует виду объекта с первого изображения, но сзади. Простейший спо-
соб это сделать - сканировать оба изображения с помощью скользящих окон
переменного размера и сравнить каждую возможную пару окон. Вероятно,
это сработает, и мы найдем совпадающие фрагменты, пройдя по каждой воз-
можной паре фрагментов. Но этот подход слишком медленный - только пред-
ставьте, сколько возможных положений и размеров скользящего окна нам
2
Высшая школа цифровой культуры Университет ИТМО
придется проверить! Кроме того, нам нужно будет сравнить каждый возмож-
ный фрагмент на одном изображении со всеми возможными фрагментами на
другом. Такой подход непомерно дорогостоящий с точки зрения вычислений.
Что, если мы выберем только подмножество фрагментов из обоих изоб-
раженийисделаемразреженноесопоставление?Очевидно,этобудетбыстрее!
Мы можем выбрать фрагменты таким образом, чтобы они были изолирова-
ны, чтобы не было пересечения между разными фрагментами одного и того
же изображения. Звучит неплохо. Но поскольку мы сравниваем не все воз-
можные фрагменты, мы должны убедиться, что выбрали наиболее важные
и репрезентативные фрагменты из всех возможных. Как это сделать? Как
мы можем найти те части изображения, в которых мы можем устойчиво най-
ти соответствия с другим изображением? Нам нужно найти такие области
изображения, которые могут совпадать с областями на других изображениях.
Участки без текстуры практически невозможно локализовать, поэтому в дан-
ном случае они не подходят. Участки со значительным изменением контраста
легче локализовать. Нам нужно найти четкие области на изображениях, кон-
кретные детали, такие как горные вершины, углы зданий, т.е. неоднородные,
особые, информативные точки. Такие точки часто называют ключевыми точ-
ками или точками интереса.
Признаки хороших ключевых точекКаковы некоторые важные свой-
ства хороших ключевых точек? Во-первых, их не должно быть слишком мно-
го.Намнужногораздоменьшеключевыхточек,чемпикселейвизображении.
Далее, каждая ключевая точка должна быть информативной, уникаль-
ной. Если мы будем рассматривать много похожих окрестностей, будет слож-
нонайтиправильныесовпадениянадвухизображениях.Каквпримере,кото-
рый вы видите сейчас на экране - невозможно сказать, какая из трех красных
точек на правом изображении соответствует красной точке на левом изобра-
жении.
Хорошие ключевые точки должны быть локальными. Ключевая точка
должна занимать небольшую область изображения, и быть устойчивой к ча-
стичному перекрытию другим объекотом.
И, наконец, ключевые точки должны быть повторяемыми. Нам нужно,
чтобы одна и та же точка находилась на разных изображениях, несмотря на
геометрические и фотометрические изменения объекта съемки. Если совпа-
дающих ключевых точек нет, сопоставить изображения будет невозможно.
Необходимо, чтобы хотя бы подмножество ключевых точек одного изображе-
ния соответствовало подмножеству ключевых точек на другом. Кроме того,
обнаружение ключевых точек должно происходить независимо на двух изоб-
ражениях. Это означает, что используемый алгоритм обнаружения ключевых
точек должен многократно обнаруживать одни и те же ключевые точки как
3
Высшая школа цифровой культуры Университет ИТМО
при применении к одному и тому же изображению, так и при применении к
разным изображениям.
Области применения Обнаружение и сопоставление ключевых точек до
сих пор являются важной задачей, встречающейся во многих сферах при-
менения компьютерного зрения. Представьте, что мы хотим выровнять два
изображения так, чтобы их можно было легко объединить в панорамный
снимок. Чтобы сделать это автоматически, нам нужно определить ключевые
точки на всех изображениях, которые мы хотим склеить, найти совпадающие
местоположения и правильно их соединить.
Еще одна важная область применения - это 3D-реконструкция из двух-
мерных изображений. Имея ключевые точки мы можем установить такой
набор соответствий, который позволит построить трехмерную модель или
создать промежуточную модель.
Многие алгоритмы отслеживания движения используют ключевые точ-
ки. Обнаруживая и сопоставляя ключевые точки в соседних кадрах, можно
обнаруживать и отслеживать движение.
Ключевые точки также используются в робототехнике для навигации
роботов. Локальные признаки, основанные на ключевых точках, все еще ис-
пользуются в некоторых методиках поиска изображений. Еще десять лет на-
зад они активно использовались для обнаружения и распознавания объектов,
но теперь большинство приложений для обнаружения объектов (как и многие
приложения для поиска изображений) пользуются преимуществами глубоко-
го обучения.
1.1 Сопоставление изображений с ключевыми
точками и локальные признаки
Процесс сопоставления изображений с использованием локальных при-
знаков можно разделить на три отдельных этапа. Первый этап - обнаружение
ключевых точек. На этом этапе на каждом изображении идет поиск таких об-
ластей, которые могут уверенно совпадать с областями на других изображе-
ниях. Второй этап - описание признаков. На этом этапе окрестности каждой
обнаруженной ключевой точки преобразуются в более компактные дескрип-
торы, которые можно сопоставить с другими дескрипторами. Третий этап -
это сопоставление признаков, когда алгоритм ищет подходящих кандидатов
на двух изображениях.
Давайте рассмотрим каждый из этих этапов один за другим, начиная с
обнаружения ключевых точек.
4
Высшая школа цифровой культуры Университет ИТМО
1.1.1 Использование угловых точек в качестве точек интереса
Интуитивно мы понимаем, что в угловых точках происходят быстрые
изменения направления кривой края. Углы - очень эффективные признаки,
поскольку они хорошо различимы и в разумной степени инвариантны к точке
обзора. Благодаря этим характеристикам углы часто используются в каче-
стве ключевых точек. Существует несколько алгоритмов обнаружения углов.
Основная идея в большинстве алгоритмов - использованть небольшое окно и
вычислить изменения яркости изображения при небольшом смещении окна.
Если пиксель находится в области с одинаковой яркостью, то значения
в соседних положениях окна будут одинаковыми. Если пиксель находится на
краю, то сдвигая окно в направлении, перпендикулярном краю, мы получим
совсем иные значения, а сдвиг окна в направлении, параллельном краю, при-
ведет лишь к небольшому изменению значений яркости. Если окно находится
в угловой точке, то смещение в любом направлении приведет к значительно-
му изменению интенсивности.
Один из самых ранних алгоритмов обнаружения углов, алгоритм Мо-
равица, основан на вычислении суммы квадратов разностей между двумя
соседними положениями окна. Харрис и Стивенс улучшили детектор углов
Моравица, используя градиенты изображения в каждой точке вместо сдвига
окна. По сути, они преобразовали эту простую идею об изменении интенсив-
ности в небольшой окрестности в математическую форму.
1.1.2 Детектор Харриса: математика
. Детектор угла Харриса находит разность в интенсивности значений
внутри окна, когда оно сдвигается на(𝑢, 𝑣). Пусть𝐼(𝑥, 𝑦) обозначает интен-
сивность изображения 𝐼 в точке (𝑥, 𝑦). Рассмотрим окно в области(𝑥, 𝑦) и
сместим его на(𝑢, 𝑣). Взвешенная сумма квадратов разностей (WSSD) между
этими двумя окнами определяется как:
𝐸(𝑢, 𝑣) =
∑︁
𝑥,𝑦
𝑤(𝑥, 𝑦) [𝐼(𝑥 + 𝑢, 𝑦+ 𝑣) −𝐼(𝑥, 𝑦)]2 (1)
где 𝑤(𝑥, 𝑦) - это оконная (или весовая) функция, которая присваивает
веса пикселям внутри его. Обычно это либо прямоугольное окно, когда оно
равно 1 внутри окна и 0 где-либо еще, либо гауссово окно. Прямоугольное ок-
но используется, когда важна скорость вычислений и уровень шума невысок.
Окно Гаусса используется, когда важно сглаживание данных.
𝐼(𝑥, 𝑦) - это интенсивность исходных точек, а𝐼(𝑥 + 𝑢, 𝑦+ 𝑣) - интенсив-
ность смещенных точек.
Для областей с практически постоянной интенсивностью, когда значе-
ния интенсивности в соседних окнах практически не меняются,𝐸(𝑢, 𝑣) бу-
5
Высшая школа цифровой культуры Университет ИТМО
дет около0. Для областей, в которых интенсивность сильно различается при
небольшом смещении окна,𝐸(𝑢, 𝑣) будет больше. Следовательно, нам нужны
окна, в которых𝐸(𝑢, 𝑣) велико.
Для малых(𝑢, 𝑣), 𝐼(𝑥 + 𝑢, 𝑦+ 𝑣) может быть аппроксимировано рядом
Тейлора. Пусть𝐼𝑥 и 𝐼𝑦 - частные производные, если𝐼 такое, что
𝐼(𝑥 + 𝑢, 𝑦+ 𝑣) ≈𝐼(𝑥, 𝑦) + 𝐼𝑥(𝑥, 𝑦)𝑢 + 𝐼𝑦(𝑥, 𝑦)𝑣 (2)
Это дает приближение для𝐸(𝑢, 𝑣):
𝐸(𝑢, 𝑣) ≈
∑︁
𝑥,𝑦
𝑤(𝑥, 𝑦) [𝐼𝑥(𝑥, 𝑦)𝑢 + 𝐼𝑦(𝑥, 𝑦)𝑣]2 (3)
Это уравнение может быть представлено в матричной форме:
𝐸(𝑢, 𝑣) ≈
[︀𝑢 𝑣]︀
𝑀
[︂
𝑢
𝑣
]︂
(4)
где
𝑀 =
∑︁
𝑥,𝑦
𝑤(𝑥, 𝑦)
[︂ 𝐼2
𝑥 𝐼𝑥𝐼𝑦
𝐼𝑥𝐼𝑦 𝐼2
𝑦
]︂
(5)
Матрицу M иногда называют матрицей Харриса.
Что показывает эта матрица? Как с ее помощью можно определить на-
личие области с монотонной интенсивностью, края или угла? Давайте снача-
ла рассмотрим выровненный по оси угол. Таким образом, изменения интен-
сивности происходят только по осям𝑥 и 𝑦. Это означает, что произведение
частных производных𝐼𝑥𝐼𝑦 равно нулю для всех позиций(𝑥, 𝑦). А матрица M
имеет вид
𝑀 =
∑︁
𝑥,𝑦
𝑤(𝑥, 𝑦)
[︂𝐼2
𝑥 0
0 𝐼2
𝑦
]︂
=
∑︁
𝑥,𝑦
𝑤(𝑥, 𝑦)
[︂
𝜆1 0
0 𝜆2
]︂
(6)
Поскольку значения равны 0 везде, кроме главной диагонали, это озна-
чает, что главная диагональ содержит собственные значения этой матрицы.
Угловые участки на изображении будут характеризоваться большими
значениями обоих собственных значений, что означает изменения интенсив-
ности в обоих направлениях, 𝑥 и 𝑦. Если какое-либо собственное значение
близко к 0, то значит, эта позиция не угловая.
В этом случае мы предположили, что угол выровнен по оси. Но что если
у нас есть угол, не совпадающий с осями изображения?
Поскольку 𝑀 симметрична, мы можем представить ее следующим обра-
зом:
6
Высшая школа цифровой культуры Университет ИТМО
𝑀 =
∑︁
𝑥,𝑦
𝑤(𝑥, 𝑦)𝑋
[︂
𝜆1 0
0 𝜆2
]︂
𝑋𝑇 (7)
Поскольку M является вещественной симметричной матрицей, ее соб-
ственные векторы указывают в направлении максимального разброса дан-
ных, а соответствующие собственные значения пропорциональны количеству
данных, распределенных в направлении собственных векторов. Таким обра-
зом, собственные значения показывают величину изменения интенсивности в
двух основных ортогональных направлениях градиента в окне.
Детектор Харриса: классификация по собственным значениямТе-
перь мы можем проанализировать собственные значения𝑀, чтобы обнару-
жить углы. Угол (или вообще точка интереса) характеризуется большим из-
менением 𝐸 во всех направлениях вектора
(︀𝑢 𝑣)︀
, что соответствует большим
значениям собственных значений𝑀. 𝑀 должен иметь два «больших» соб-
ственных значения, чтобы рассматриваемая точка являлась точкой интереса.
На основании величин собственных значений можно сделать следующие вы-
воды:
• Если оба собственных значения𝜆1 и 𝜆2 близки к 0, то𝐸 почти посто-
янна во всех направлениях, и это означает, что пиксель(𝑥, 𝑦) не имеет
интересующих нас свойств и принадлежит к плоской однородной обла-
сти.
• Если одно из собственных значений имеет большое положительное зна-
чение, а другое близко к 0, или если одно из собственных значений
намного больше другого (𝜆1 >> 𝜆2 or 𝜆2 >> 𝜆2), то это означает, что
найден край.
• И, наконец, когда оба собственных значения имеют большие положи-
тельные значения, но отличаются друг от друга незначительно, то это
означает, что𝐸 велико во всех направлениях, и найден угол.
Детектор Харриса: мера отклика для углаТаким образом, мы ви-
дим, что собственные значения матрицы, сформированной из производных
на фрагменте изображения, могут использоваться для определения плоских
областей, краев и углов. Но Харрис и Стивенс отметили, что точное вы-
числение собственных значений требует больших вычислительных ресурсов.
Поэтому вместо этого они предложили использовать следующую функцию
для измерения отклика угла:
7
Высшая школа цифровой культуры Университет ИТМО
𝑅 = 𝜆1𝜆2 −𝑘(𝜆1 + 𝜆2)2 = det 𝑀 −𝑘(trace𝑀)2 (8)
Дело в том, что след квадратной матрицы равен сумме ее собственных
значений, а ее определитель равен произведению ее собственных значений.
Таким образом, нам не нужно фактически вычислять разложение по соб-
ственным значениям матрицы 𝑀, а вместо этого, чтобы найти углы или,
скорее, точки интереса в целом, достаточно вычислить определитель и след
𝑀.
𝑘 - это эмпирическая константа. Диапазон ее значений зависит от обла-
сти применения. В литературе считаются допустимыми значения в диапазоне
0.04˘0.15.Можноинтерпретировать 𝑘 как«коэффициентчувствительности»:
чем он меньше, тем больше вероятность, что детектор найдет углы.
Мера 𝑅 принимает большие положительные значения, когда оба соб-
ственных значения велики, что указывает на наличие угла; она принимает
отрицательные значения, когда одно собственное значение большое, а дру-
гое маленькое, что указывает на край; и ее абсолютное значение мало, когда
оба собственных значения малы, что указывает на то, что рассматриваемый
фрагмент изображения является плоским. Обычно𝑅 используется с порогом
𝑇. Мы говорим, что в определенной области изображения был обнаружен
угол, только если𝑅 > 𝑇для рассматриваемой области.
Детектор Харриса На этом слайде описаны все этапы работы детектора
Харриса.
1. Вычислить производные изображения по горизонтали и вертикали𝐼𝑥 и
𝐼𝑦 в каждой точке изображения с использованием Гауссова сглажива-
ния. Как мы знаем из предыдущих лекций, это можно сделать, свернув
исходное изображение с производными от гауссианов.
2. Вычислить матрицу𝑀 в гауссовом окне для каждого пикселя.
3. Вычислить меру отклика угла𝑅.
4. Использовать порог R для того, чтобы сохранить только точки с доста-
точно большими значениями R.
5. Найти локальные максимумы меры отклика, используя немаксималь-
ное подавление (nonmaximum suppression).
Схема детектора угла Показанная на экране схема работы является об-
щей для многих детекторов угловых точек. Мы начинаем с входного изобра-
жения, применяем оператор угла, который создает карту «угловых точек».
8
Высшая школа цифровой культуры Университет ИТМО
Затем мы передаем эту карту угловых точек в оператор пороговой обработки
и получаем карту порогов угловых точек. Эта карта затем передается опе-
ратору немаксимального подавления, который выводит положения угловых
точек.
Пример 1 Наэкранепредставенодинизпримеровприменениярассмотрен-
ной схемы к уже знакомому нам изображению Лена. Сначала мы переходим
от исходного изображения к карте угловых точек, пороговой карте и, нако-
нец, карте координат углов. Последнее изображение на экране показывает
положение обнаруженных угловых точек, наложенных на входное изображе-
ние.
Пример 2 Рассмотрим еще один пример. Предположим, у нас есть два
изображения одного и того же объекта, но одно представляет собой поверну-
тую версию другого. Условия освещения тоже разные.
Давайте вычислим меру отклика угла Харриса𝑅 для обоих этих изобра-
жений. Красные точки на изображениях соответствуют высокому значению
𝑅, а синие точки соответствуют низким значениям𝑅, оранжевые, желтые и
зеленые точки имеют промежуточные значения между красным и синим.
Следующий шаг - оставить точки со значениями𝑅, которые превышают
выбранный порог. Таким образом, мы оставляем все точки, которые были
красными или оранжевыми на предыдущем слайде, и отбрасываем желтые,
зеленые и синие.
И последний шаг - выполнить немаксимальное подавление - найти и со-
хранить точки локальных максимумов𝑅.
Теперь давайте наложим ключевые точки, обнаруженные с помощью де-
тектора угловых точек Харриса, на исходные изображения. Вы можете за-
метить, что было найдено много пар соответствующих ключевых точек, и
это именно то, чего мы добивались. На обоих изображениях есть ключевые
точки, обнаруженные в местах расположения глаз и ноздрей, по границам
черных пятен, а также во многих других выделяющихся областях.
Свойства детектора Харриса Давайте теперь рассмотрим свойства де-
тектора Харриса. В идеале мы хотели бы, чтобы детекторы ключевых точек
были инвариантны относительно поворота, изменения интенсивности и мас-
штаба, чтобы одни и те же ключевые точки можно было найти в повернутых,
осветленных или затемненных, отмасштабированных версиях изображений.
А как насчет детектора Харриса? Инвариантен ли он ко всем этим измене-
ниям?
Детектор Харриса инвариантен относительно поворота, как мы только
9
Высшая школа цифровой культуры Университет ИТМО
что видели на примере с повернутой игрушкой. Собственные значения оста-
ются неизменными для повернутых версий изображения.
А как насчет сдвига интенсивности? Да, детектор Харриса инвариантен
и к этим изменениям тоже. Поскольку для вычисления углового отклика ис-
пользуются только производные, он инвариантен к сдвигу интенсивности. А
изменение шкалы интенсивности можно обрабатывать с помощью адаптив-
ного определения порога.
А что насчет масштаба? К сожалению, детектор Харриса не инвариантен
кмасштабу.Взависимостиотмасштабаодинитотжефрагментизображения
может рассматриваться как содержащий только края или содержащий углы.
Посмотрим на пример, представленный на экране. На изображении слева все
окна по контуру должны были бы определить угол, но углов в окне нет. Если
уменьшитьтотжефрагментдоизображения,представленногосправа,видно,
что теперь у нас есть угол, попадающий в окно.
Инвариантность к изменению масштабаОдним из решений проблемы
является извлечение признаков в различных масштабах. Если мы сможем из-
менить масштаб окна (или масштаб изображения), то мы сможем найти такие
масштабы, в которых фрагменты будут совпадать. Та же самая кривая слева,
что и на предыдущем слайде, может считаться краем и будет соответствовать
кривой справа, если выбран правильный масштаб.
Вариант 2. Одним из решений проблемы является извлечение призна-
ков в различных масштабах. Если мы сможем изменить масштаб окна (или
масштаб изображения), то мы сможем найти такие масштабы, в которых
фрагменты будут совпадать. Та же самая кривая слева, что и на предыду-
щем слайде, может считаться углом и будет соответствовать кривой справа,
если выбран правильный масштаб.
Это можно сделать, выполнив ту же операцию определения углов с
несколькими разрешениями в пирамиде изображений, полученных путем
сглаживания и субдискретизации входных изображений.
Однако, необходимо выполнить поиск углов для нескольких разрешений,
а затем для поиска лучшего совпадения выполнить𝑁 ×𝑁 попарных срав-
нений для 𝑁 масштабов, что обойдется дорого с точки зрения вычислений.
Вместо того, чтобы извлекать признаки в разных масштабах и затем сопо-
ставлять их все, более эффективно извлекать признаки, которые стабильны
как по местоположению, так и по масштабу. Детектор Харриса является при-
мером детектора, инвариантного относительно положения. Что же делать с
масштабом? Как выбрать лучший масштаб для каждого фрагмента?
10
Высшая школа цифровой культуры Университет ИТМО
1.1.3 Выбор масштаба
Итак, как можно автоматически определить этот лучший масштаб?
Нужно выбрать функцию, инвариантную к изменению масштаба, чтобы зна-
чение этой функции было одинаковым для соответствующих фрагментов,
даже если эти фрагменты имеют разный масштаб. Одним из примеров такой
функции является средняя интенсивность. Она будет одинаковой как для
увеличенной, так и для уменьшенной версии одного и того же фрагмента
изображения. Назовем ее характеристической функцией.
Затем мы можем рассмотреть эту функцию как на функцию от измене-
ния размера области. Например, если изображение 2 является уменьшенной
копией изображения 1 с коэффициентом масштабирования1/2, тогда такая
функция для изображений 1 и 2 будет выглядеть примерно так, как графики,
показанные на экране.
Затем давайте найдем локальный максимум этой функции и размер
фрагмента, на котором достигается данный локальный максимум. Локаль-
ный максимум инвариантен к масштабу, поэтому точка, в которой достига-
ется локальный максимум, является точкой характеристического размера.
Характеристический размер Давайте посмотрим на пример, представ-
ленный на экране: мы видим два изображения в разном масштабе. Возьмем
точку на левом изображении, отмеченную желтым крестом. Теперь давайте
изменим размер окна с центром в этой точке и вычислим некоторую функ-
цию 𝑓 для окон разного размера. Ниже приведен график этой функции𝑓 в
зависимости от размера окна. Мы видим, что сначала она возрастает, когда
мы увеличиваем размер области вокруг этого желтого креста, но затем она
начинает убывать. Теперь выполним аналогичные действия с изображением
справа. Теперь давайте посмотрим, при каком размере окна значение𝑓 было
наибольшим для левого изображения и при каком размере окна𝑓 прини-
мало наибольшее значение для правого изображения. Эта точка локального
максимума является точкой характеристического размера. А соотношение
размеров окон, соответствующих локальным максимумам на левом и правом
изображениях, является соотношением масштабов этих изображений.
Реализация Вместо вычисления функции 𝑓 для окон все большего раз-
мера, мы можем реализовать поиск характеристического размера, используя
фиксированный размер окна и пирамиду Гаусса. Пирамида Гаусса - это та-
кой вид представления изображения в разных масштабах, когда изображение
подвергается многократному сглаживанию и субдискретизации. В пирамиде
Гаусса последующие изображения получаются с использованием размытия
по Гауссу и уменьшением масштаба.
11
Высшая школа цифровой культуры Университет ИТМО
Как выбрать характеристческую функцию f - часть 1Как выбрать
правильную «характеристическую» функцию𝑓? Для определения масштаба
часто используется нормированный по шкале лапласиан (LoG). Как вы, воз-
можно, помните из предыдущих лекций, LoG - это оператор Лапласса∇2𝑓,
примененный к гауссовской функции𝐺. Двумерная функция Гаусса опреде-
ляется как
𝐺(𝑥, 𝑦) = 1
2𝜋𝜎2 𝑒−𝑥2+𝑦2
2𝜎2 (9)
1
2𝜋𝜎2 перед двумерным гауссовским ядром является нормирующей ко-
стантой. При использовании нормирующей костанты такое гауссовское ядро
является нормированным ядром, т.е. его интеграл по всей области определе-
ния равен единице для каждого𝜎.
LoG определяется как
∇2𝐺 = 𝜕2𝐺
𝜕𝑥2 + 𝜕2𝐺
𝜕𝑦2 (10)
LoG нормированного Гауссиана не инвариантен к масштабу - он зависит
от 𝜎 или величины размытия. Отклик производной гауссовского фильтра на
идеальный край уменьшается с увеличением𝜎. Чтобы реакция оставалась
неизменной (инвариантность к масштабу), мы должны умножить производ-
ную Гауссиана на𝜎. Лапласиан - это вторая производная Гауссиана, поэтому
ее необходимо умножить на𝜎2. Итак, чтобы получить независимость от мас-
штаба, нужно использовать нормированного по масштабу LoG:
𝐿(𝑥, 𝜎) = 𝜎2∇2𝐺 (11)
.
Как выбрать характеристическую функцию f - часть 2Детектор
Лапласиана Гауссиана представляет собой нормированный по масштабу LoG.
Как вы можете видеть на экране, маска фильтра LoG имеет концентриче-
скую структуру вокруг центральной точки, с положительными весами в цен-
тральной области и отрицательными весами в окружающем кольце. Таким
образом, отклик этого фильтра будет максимальным, если применить его
к окрестности изображения, которое имеет аналогичную (округлую или кап-
леобразную) структуру в соответствующем масштабе. Если мы посмотрим на
окружности, обозначенные img1, img2 и img3, они будут обнаружены филь-
тром LoG соответствующего масштаба.
12
Высшая школа цифровой культуры Университет ИТМО
Характеристический размер и Лапласиан Таким образом, чтобы
определить для данной точки изображения характеристический размер при
помощи лапласиана, нужно рассмотреть отклики лапласиана в разном мас-
штабе. И тот масштаб, при котором достигается пик отклика Лапласиана, и
является характеристическим размером.
1.1.4 Детектор LoG
Обратите внимание, что для больших объектов округлой формы повто-
ряющееся местоположение ключевой точки также может быть определено
как центр большого объекта округлой формы. Таким образом, LoG можно
применять как для поиска характеристического размера для данной области
изображения, так и для прямого обнаружения инвариантных к масштабу об-
ластей путем поиска трехмерных (местоположение + масштаб) экстремумов
LoG. Эта процедура показана на экране. Изображение свернуто с помощью
фильтров LoG разного масштаба, и создает трехмерную карту отклика. Точ-
ки максимума на такой трехмерной карте отклика будут указывать на размер
и расположение ключевых точек-капель. Детектор LoG также иногда назы-
вают детектором капель, потому что он обнаруживает в качестве областей
интереса капли, а не углы, как детектор Харриса.
Пример: детектор больших объектов округлой формыСейчас на
экране показан пример работы детектора больших объектов округлой формы
LoG. Обратите внимание на разные масштабы и расположение ключевых
точек.
Аппроксимация LoG при помощи разности гауссианов (DoG)Вы-
числение производных второго порядка в лапласиане гауссиана требует боль-
ших вычислительных ресурсов. Но можно аппроксимировать LoG разностью
двух гауссианов (DoG):
∇2𝐺 ≈𝐺(𝑥, 𝑦; 𝜎1) −𝐺(𝑥, 𝑦; 𝜎2) (12)
Если мы вычтем один гауссиан из другого, причем первый будет иметь
большее стандартное отклонение, чем второй, мы получим кривую, очень
похожую на LoG. На экране вы можете видеть графики двух гауссианов,
один с𝜎 = 3.2 показан голубым цветом, а другой с𝜎 = 2.0 показан темно-
синим цветом, их разность, нанесенная красным, очень похожа на функцию
мексиканской шляпы (или LoG), показанную на графике справа.
Наилучшее приближение к LoG может быть получено, когда стандарт-
ное отклонение первого гауссиана примерно в 1.6 раз больше стандартного
13
Высшая школа цифровой культуры Университет ИТМО
отклонения второго, поэтому𝜎1/ 𝑠𝑖𝑔𝑚𝑎2 = 1.6, как и для гауссианов на гра-
фике, показанном слева.
Этот подход был предложен Дэвидом Лоу и является частью разрабо-
танного им алгоритма масштабно-инвариантного преобразования признаков
(SIFT).
Разность по Гауссу Как и в случае с лапласианом гауссиана, разность
гауссианов может быть использована для увеличения видимости краев изоб-
ражения. Сейчас на экране показаны результаты свертки изначального изоб-
ражения с двумя разными ядрами Гаусса, а также с разностью между ними
(DoG). Обратите внимание, что свертка изображения с помощью ядра DoG
дает тот же результат, что и вычитание результатов свертки изображения с
двумя исходными гауссовскими ядрами:
𝐼(𝑥, 𝑦) ⋆ 𝐺1 −𝐼(𝑥, 𝑦) ⋆ 𝐺2 = 𝐼(𝑥, 𝑦) ⋆ (𝐺1 −𝐺2) (13)
Размытие изображения с использованием ядра Гаусса подавляет высоко-
частотную пространственную информацию. Вычитание одного ядра из дру-
гогосохраняетпространственнуюинформацию,котораянаходитсявтомдиа-
пазоне частот, которые сохраняются при использовании двух исходных филь-
тров. Таким образом, DoG представляет собой пространственный полосовой
фильтр.
1.1.5 Обнаружение объектов округлой формы с помощью DoG
Итак, как мы только что убедились, LoG можно аппроксимировать при
помощи DoG. Это означает, что различия гауссианов также можно использо-
вать для обнаружения больших объектов округлой формы при использова-
нииметодамасштабно-инвариантногопреобразованияпризнаков.Вычитание
смежных по масштабу уровней пирамиды Гаусса является эффективным ме-
тодом в данном случае. Давайте посмотрим, как именно это можно сделать.
Детектор DoG использует подход каскадной фильтрации для обнаруже-
ния областей в изображении, являющихся инвариантными к изменению мас-
штаба. Он ищет экстремумы в пространстве масштабов, причем пространство
масштабов является представлением изображения как набора сглаженных
изображений. Сглаженные изображения имитируют потерю деталей, кото-
рая может произойти при уменьшении масштаба изображения. Сглаживани-
ем управляет параметр, определяющий размер масштаба. Для реализации
сглаживания используются ядра Гаусса, а параметром масштаба является
стандартное отклонение, 𝜎. Таким образом, пространство масштабов пред-
ставляет собой пирамиду Гаусса: входное изображение последовательно сво-
рачивается с помощью ядер Гаусса, имеющих стандартные отклонения𝜎1,
14
Высшая школа цифровой культуры Университет ИТМО
𝑘𝜎1, 𝑘2𝜎1, 𝑘3𝜎1, . . ., чтобы создать «стек» отфильтрованных по Гауссу (сгла-
женных) изображений, разделенных постоянным коэффициентом𝑘, как по-
казано на экране.
Пространство масштабов разделено на октавы, где каждая октава соот-
ветствует удвоению𝜎, так же как в музыке октава соответствует удвоению
частоты звукового сигнала. Затем каждая октава делится на целое число ин-
тервалов 𝑠 так, чтобы𝑘 = 21/𝑠. В стеке размытых изображений для каждой
октавы создаются 𝑠 + 3 изображений. На экране октава 1 состоит из пяти
изображений, поэтому 𝑠 = 2 и 𝑘 =
√
2. Входное изображение последова-
тельно сглаживается сверткой с гауссианами со стандартными отклонения-
ми 𝜎1,
√
2𝜎1, 2𝜎1 и т.д. На экране показан полученный набор изображений
пространства масштабов. Затем производится вычитание соседних гауссовых
изображений, чтобы получить изображение с разностью гауссианов (DoG),
показанное справа. После обработки полной октавы мы передискретизируем
гауссовское изображение, которое имеет двойное начальное значение𝜎, взяв
каждый второй пиксель из каждой строки и столбца - это будут 2 изображе-
ния с вершины стека.
Можно показать, что когда два соседних масштаба шкалы разделены
постоянным множителем 𝑘, то необходимая нормировка масштаба уже вы-
полнена. Таким образом, в отличие от LoG, дополнительная нормировка мас-
штаба не требуется.
Как и в случае с детектором LoG, области интереса в соответствии с DoG
определяются как области, которые одновременно являются экстремумами
в плоскости изображения и вдоль координаты функции масштаба𝐷(𝑥, 𝜎).
Такие точки находятся путем сравнения значения𝐷(𝑥, 𝜎) каждой точки с
8 ее ближайшими соседями на том же уровне масштаба и с 9 ближайшими
соседями на каждом из двух смежных уровней, выше и ниже, как показано на
экране (справа). Подводя итог, детектор DoG ищет экстремумы трехмерного
пространства функции DoG.
Пример Сейчас на экране показаны изображения первых трех октав шка-
лы масштаба. В таблице показаны значения стандартного отклонения, ис-
пользуемые в каждой шкале каждой октавы. Например, стандартное откло-
нение, используемое для масштаба 2 октавы 1 равен𝑘𝜎1, что равно 1.
Сейчас на экране показаны примеры изображений DoG. Мы имеем всего
𝑠 + 3 изображений с использованием фильтра Гаусса,𝑠 + 2 разностей гаусси-
анов, полученных в каждой октаве из смежных пар изображений с гауссовой
фильтрацией в данной октаве. Разности можно рассматривать как изображе-
ния, и для каждых трех октав на экране показан один образец такого изоб-
ражения. Как и следовало ожидать, уровень детализации этих изображений
уменьшается по мере движения вверх в пространстве масштабов.
15
Высшая школа цифровой культуры Университет ИТМО
Обнаружение ключевых точек: выводы относительно инвариант-
ности Давайте подытожим, что же мы узнали об обнаружении ключевых
точек. Напомним, что для двух изображений, содержащих одну и ту же сце-
ну, целью любого алгоритма обнаружения ключевых точек является найти
одинаковые ключевые точки (или точки интереса) в каждом изображении
независимо друг от друга. В идеале мы хотели бы, чтобы этот алгоритм
был инвариантен к таким параметрам, как изменение освещения, поворот,
масштаб и точка обзора. Мы подробно обсудили три алгоритма. Во-первых,
детектор углов Харриса, который определяет углы в качестве ключевых то-
чек. Мы выяснили, что этот детектор инвариантен к изменениям освещения
и повороту, но не инвариантен к масштабу.
Затеммыобсудилидваинвариантныхкмасштабудетектора:LoGиDoG.
Оператор Лапласа редко используется сам по себе, так как он похож на DoG,
но более сложен с точки зрения вычислений. Однако, он иногда используется
в сочетании с оператором Харриса. Оператор Харриса-Лапласа был предло-
жен для увеличения различительной способности по сравнению с операто-
рами LoG или DoG. Он сочетает в себе специфику оператора Харриса для
определения угловых точек с механизмом выбора масштаба LoG.
Как упоминалось ранее, детектор DoG является частью предложенно-
го Дэвидом Лоу алгоритма извлечения признаков SIFT, поэтому его также
часто называют детектором SIFT.
Масштабно-инвариантный поиск: выводы Если сравнить два рас-
смотренных масштабно-инвариантных детектора, Харриса-Лапласа и SIFT,
оба они работают в тех случаях, когда мы рассматриваем изображения од-
ной и той же сцены с большой разницей в масштабе, и оба этих метода ищут
максимумы определенной функции (или функций) в данных пространствах
масштабов в изображениях.
Метод Харриса-Лапласа строит два отдельных пространства масштабов
для функции Харриса и лапласиана. Первая версия этого метода использу-
ет функцию Харриса для локализации точек-кандидатов на каждом уровне
шкалы и выбирает те точки, для которых лапласиан одновременно достига-
ет экстремума по шкалам. Полученные точки устойчивы к изменениям мас-
штаба, повороту изображения, изменениям освещения и шуму фотоаппарата.
Кроме того, они очень хорошо находят точки интереса. В качестве недостатка
можно упомянуть, что настоящий детектор Харриса-Лапласа обычно возвра-
щает гораздо меньшее количество точек, чем детекторы LoG или DoG. Это
результат использования дополнительного ограничения, которое заключает-
ся в том, что каждая точка должна одновременно удовлетворять двум раз-
личным условиям максимума. Для некоторых практических применений по-
иска объектов меньшее количество областей интереса может быть недостат-
16
Высшая школа цифровой культуры Университет ИТМО
ком, поскольку это снижает устойчивость к частичному перекрытию. По этой
причине была предложена обновленная версия детектора Харриса-Лапласа,
основанная на менее строгом критерии. Вместо того, чтобы искать точки,
в которых максимумы достигаются одновременно, этот алгоритм выбирает
максимумы масштаба лапласиана в таких точках, в которых Функция Хар-
риса также достигает максимума на любой шкале. В результате, модифици-
рованный таким образом детектор дает больше точек интереса при немного
более низкой точности, что приводит к повышению производительности для
приложений, где необходимо найти как можно больше областей интереса.
Детектор DoG (или SIFT) ищет максимумы разности гауссианов как в
масштабе, так и в пространстве.
1.1.6 Локальные дескрипторы - сравнение элементов
Теперь мы знаем, как независимо определять ключевые точки на разных
изображениях. Следующий вопрос - как сопоставить эти точки, т.е. как найти
соответствующие пары. Нам нужны локальные дескрипторы для ключевых
элементов, чтобы их можно было сравнивать на двух изображениях с помо-
щью некоторой функции подобия. Как и в случае с глобальными признаками,
мы хотим, чтобы локальные дескрипторы были инвариантны к изменениям
освещения, положения, масштаба и т.д. Но в то же время эти дескрипторы
должны быть хорошо различимыми, чтобы одному признаку можно было с
большой вероятностью найти правильное соответствие в большой базе дан-
ных признаков.
Какмыужезнаем,масштабно-инвариантныедетекторыключевыхточек
могут дать нам информацию о масштабе. Таким образом, мы можем исполь-
зовать эту информацию, чтобы выбрать окрестность правильного масштаба в
местоположении ключевой точки для вычисления дескриптора. Глобальные
дескрипторы, которые мы обсуждали ранее, такие как гистограммы цвета
или интенсивности, отклики фильтра и другие, также могут использовать-
ся в таком случае, если применить их к локальному участку изображения
заданного масштаба и в заданном положении, указанном алгоритмом обна-
ружения ключевых точек. Но многие из этих признаков либо чувствительны
даже к небольшим изменениям положения и позы, либо плохо работают, либо
и то, и другое. Один из наиболее удачных локальных дескрипторов был пред-
ложен Дэвидом Лоу вместе с детектором ключевых точек DoG. Это хорошо
известный дескриптор SIFT.
1.1.7 Дескриптор SIFT: основные этапы
Основные этапы работы дескриптора SIFT перечислены ниже:
17
Высшая школа цифровой культуры Университет ИТМО
1. Обнаружение экстремумов в пространстве масштабов. Мы подробно об-
судили этот этап. Он реализован с использованием разности гауссианов.
2. Локализация ключевых точек. В каждом потенциальном местоположе-
нии используется детализированная модель для определения местопо-
ложения и масштаба. Ключевые точки выбираются на основе мер их
стабильности, краевые отклики устраняются.
3. Назначение ориентации. Каждому местоположению ключевой точки
назначается одна или несколько ориентаций на основе локальных на-
правлений градиента изображения. Все будущие операции выполняют-
ся над данными изображения, которые были преобразованы относи-
тельно назначенной ориентации, масштаба и местоположения каждого
признака.
4. Дескриптор ключевой точки. Градиенты локального изображения вы-
числяются в выбранном масштабе в области вокруг каждой ключевой
точки. Затем они преобразуются в представление, допускающее измене-
нияуровнейосвещенностиизначительныеискажениялокальныхформ.
Рассмотрим подробнее этапы 3 и 4.
Ориентация ключевых точек Чтобы охарактеризовать изображение в
каждом потенциальном расположении ключевой точки, на каждом уровне
пирамиды сглаженное изображение обрабатывается для получения гради-
ентов и ориентации изображения. Для каждого пикселя𝐴𝑖𝑗, вычисляются
значения градиента изображения,𝑀𝑖𝑗, и ориентации,𝑅𝑖𝑗, при использовании
разности пикселей:
𝑀𝑖𝑗 =
√︁
(𝐴𝑖𝑗 −𝐴𝑖+1,𝑗)2 + (𝐴𝑖𝑗 −𝐴𝑖,𝑗+1)2 (14)
𝑅𝑖𝑗 = 𝑎𝑡𝑎𝑛2(𝐴𝑖𝑗 −𝐴𝑖+1,𝑗, 𝐴𝑖,𝑗+1 −𝐴𝑖𝑗) (15)
Для каждого положения ключевой точки выбирается каноническая ори-
ентация таким образом, чтобы дескрипторы изображения были инвариантны
к повороту. Чтобы сделать ее максимально устойчивой к изменениям осве-
щения или контрастности, ориентация определяется как пик на гистограмме
ориентаций локальных градиентов изображения. Такая гистограмма форми-
руется из ориентаций градиента точек выборки в окрестности каждой клю-
чевой точки. Гистограмма имеет 36 интервалов, охватывающих на плоскости
изображения диапазон ориентации360∘. Каждый образец, добавленный к ги-
стограмме, взвешивается по величине градиента и круговой функции Гаусса
18
Высшая школа цифровой культуры Университет ИТМО
со стандартным отклонением, в 1,5 раза превышающим масштаб ключевой
точки. Пики на гистограмме соответствуют доминирующим локальным на-
правлениям локальных градиентов. После чего обнаруживается самый высо-
кий пик на гистограмме, причем любой другой локальный пик, находящий-
ся в пределах 80% от самого высокого пика также используется для созда-
ния другой ключевой точки с такой же ориентацией. Таким образом, для
областей, содержащих несколько пиков одинаковой величины будет создано
несколько ключевых точек в одном месте и в одном масштабе, но с разной
ориентацией.
Изображение на экране показывает ключевые точки, наложенные на
изображение, причем ориентация ключевых точек показана стрелками. Об-
ратите внимание на согласованность ориентации похожих наборов ключевых
точек на изображении. Например, посмотрите на ключевые точки с правой
стороны здания в вертикальном направлении. Длина стрелок варьируется в
зависимости от освещения и содержания изображения, но их направление
безошибочно согласовано.
Дескриптор SIFT Итак, мы присвоили каждой ключевой точке признаки
ее расположения, масштаба и ориентации. Эти параметры создают повторя-
емую локальную двумерную систему координат, в которой описывается ло-
кальная область изображения, и, следовательно, обеспечивают неизменность
этих параметров. Следующим шагом является вычисление дескриптора для
выделяющейся области локального изображения, которая, в то же время, яв-
ляется максимально инвариантной по отношению к остальным изменениям,
таким как изменение освещения или трехмерной точки обзора.
Подход, который используется для вычисления дескрипторов в алгорит-
ме SIFT, основан на экспериментальных результатах, предполагающих, что
локальные градиенты изображения, по-видимому, выполняют функцию, ана-
логичную той, которую выполняет человеческое зрение для сопоставления
и распознавания трехмерных объектов с разных точек зрения. Сейчас на
экране представлено вычисление дескриптора ключевой точки.
Сначала вокруг местоположения ключевой точки выбираются величина
градиента изображения и направление ориентации, причем для выбора уров-
ня размытия изображения по Гауссу используется масштаб ключевой точки.
Чтобы достичь инвариантности ориентации, координаты дескриптора и ори-
ентации градиента поворачиваются относительно ориентации ключевой точ-
ки. Для повышения эффективности градиенты предварительно вычисляются
для всех уровней пирамиды и используются как для определения ориента-
ции ключевых точек, так и для вычисления дескриптора. В центре экрана
маленькими стрелками показаны градиенты изображения для каждой вы-
борки. Для присвоения веса каждой точке выборки используется гауссовская
19
Высшая школа цифровой культуры Университет ИТМО
функция с𝜎, равная половине ширины окна дескриптора, что и показано в
круглом синем окне в центре экрана, хотя, в реальности, вес снижается более
плавно. Назначение этого гауссовского окна - избежать внезапных изменений
дескриптора при небольших изменениях положения окна и сделать меньший
акцент на градиентах, которые находятся далеко от центра дескриптора.
Дескриптор ключевой точки показан в правой части экрана. Гистограм-
мы ориентации создаются по выборкам 4x4. На рисунке показаны восемь
направлений для каждой гистограммы ориентации, причем длина каждой
стрелки соответствует величине этой гистограммы. Градиент, показанный
на центральном изображении, может сдвигаться по 4 положениям, при этом
он будет по-прежнему вносить вклад в ту же гистограмму справа, посколь-
ку такой дескриптор допускает локальные позиционные сдвиги. Дескриптор
формируется из вектора, содержащего значения всех записей гистограммы
ориентации, соответствующие длинам стрелок на изображении справа. На
рисунке справа показан массив гистограмм ориентации 2x2, но, согласно мне-
нию автора метода, наилучшие результаты достигаются при использовании
массива гистограмм 4x4 с 8 ячейками ориентации в каждой. Следовательно,
дескриптор SIFT обычно представляет собой вектор признаков 4x4x8 = 128
элементов для каждой ключевой точки.
1.2 Какую модель выбрать?
Ключевые точки и локальные дескрипторы - полезные инструменты для
обнаружения и описания небольших выделяющихся областей изображения.
Но эти дескрипторы часто зашумлены и мало что могут сказать о картине
большего масштаба. Часто мы заранее знаем формы интересующих нас объ-
ектов. Одним из примеров наиболее распространенных форм, обнаружение
которых может быть нам интересно, являются прямые линии. Мир, создан-
ный человеком, полон прямых линий. Обнаружение и сопоставление прямых
может быть полезно во множестве приложений. Например, для определения
полосы движения и понимания общей дорожной разметки как для помощи
водителю, так и для беспилотных автомобилей. Например, поиск места для
парковки. Среди других распространенных примеров можно назвать обна-
ружение границ поля в анализе спортивных видеозаписей, обнаружение пря-
мых при архитектурном моделировании, оценивании положения камеры в
городских условиях, анализе макетов печатных документов и многих других
сферах. В число распространенных простых форм также входят круги. В
показанном на экране примере задача состоит в том, чтобы автоматически
сосчитать монеты, и в таком случа, можно было бы извлечь пользу из знания,
что монеты должны быть круглыми.
В случаях, когда мы заранее знаем, какой тип формы мы ищем, мы мо-
жем выбрать параметрическую модель для представления набора ключевых
20
Высшая школа цифровой культуры Университет ИТМО
точек,формирующихинтересующийнасобъект,азатемопределитьправиль-
ные параметры модели, чтобы она подходила для данных наблюдений.
1.2.1 Параметрическая модель
Так что же это за параметрическая модель? Параметрическая модель -
это функция от данных изображения и некоторых параметров. Эта функция
определяет класс фигур, которые мы ищем на изображении. Но точное рас-
положение этой формы и другие свойства этой формы, такие как, например,
размер, могут изменяться ти задаваться параметрами. Цель процесса выбора
модели - найти такие параметры, чтобы данная модель наилучшим образом
соответствовала данным наблюдения.
1.3 Параметры модели для поиска прямой
Например,мыищемпрямую,инамдаетсянаборточек-этоданныеизоб-
ражения, наша цель состоит в том, чтобы найти прямую, которая наилучшим
образомаппроксимируетданныеточки. Втакомслучае,параметрическаямо-
дель представляет собой линейное уравнение:𝑦 = 𝑚𝑥+𝑏. Нам известно много
пар (𝑥, 𝑦) - ключевых точек, являющихся данными изображения. Наша за-
дача - найти такие значения параметров𝑚 и 𝑏, чтобы линейное уравнение
𝑦 = 𝑚𝑥 + 𝑏 точно соответствовало всем (или большинству) ключевых точек.
Давайте представим, что вы обнаружили красные ключевые точки на
изображении, показанном на экране, и ваша задача - найти лучшую прямую,
которая может соответствовать этим точкам - например, это может быть
прямая, показаная синим цветом на экране.
1.3.1 Как выбрать параметры модели: общие рекомендации
Существует множество различных методов и алгоритмов выбора пара-
метров модели, но все они работают по одному алгоритму.
• Во-первых,нужновыбратьпараметрическуюмодель,которая,какожи-
дается, лучше всего соответствует имеющимся данным. Если вы ищете
прямые на изображении, вашей параметрической моделью будет пря-
мая. Если вы ищете круги (и ожидаете найти их на имеющемся изоб-
ражении), выберите параметрическое уравнение круга или эллипса в
качестве модели.
• После того, как вы выберете модель, следующим шагом будет подгонка
этой модели к вашим данным, поиск таких значений для всех парамет-
ров модели, чтобы она хорошо аппроксимировала ваши данные. Имейте
в виду, что реальные данные всегда неточны. Вы будете иметь дело с
21
Высшая школа цифровой культуры Университет ИТМО
шумом в местах обнаруженных ключевых точек, с выбросами, с отсут-
ствующими в результате перекрытия точками. Модель должна быть
устойчивой ко всем этим проблемам и хорошо описывать фактические
данные. Невозможно точно подогнать модель ко всем имеющимся у вас
точкам данных. Но лучшей моделью будет та, которая подходит для
описания максимально возможного количества точек. Таким образом,
вам необходимо максимизировать количество данных, «вписывающих-
ся» в модель и минимизировать количество «выбросов».
• Когда у вас уже есть параметрическая модель и параметры, которые
лучше всего подходят для ваших данных, вы можете отфильтровать те
точки данных, которые не соответствуют модели. Это выбросы.
Различные алгоритмы подгонки модели по-разному подходят к шагу но-
мер два. Это самый важный шаг из перечисленных трех. Итак, как найти
«оптимальные» параметры для выбранной модели?
1.3.2 Методы поиска модели
В качестве примера рассмотрим задачу поиска модели для прямой ли-
нии. Один из самых простых алгоритмов подгонки модели - это метод наи-
меньших квадратов. Его можно использовать, когда мы знаем, какие точки
принадлежат прямой, и перед нами стоит задача найти «оптимальные» па-
раметры прямой. Его можно использовать также для других типов моделей.
Но этот метод очень чувствителен к выбросам, поэтому он работает хорошо
только тогда, когда исходные данные не зашумлены и вы ожидаете, что все
точки данных впишутся в модель и не будут выбросами.
В случае, когда в данных есть выбросы, лучше выбрать более устой-
чивый алгоритм подгонки. Один из самых популярных алгоритмов в этой
категории - RANSAC (консенсус случайной выборки), т.е. стабильный метод
оценки параметров модели на основе случайных выборок.
Если вы ожидаете, что у вас несколько экземпляров объекта или
несколько разных прямых (в случае, если наша задача - поиск прямой, и
задача состоит в том, чтобы найти все линии), тогда можно использовать
такие методы, как RANSAC или преобразование Хафа (Hough transform).
Теперь давайте поближе познакомимся с этими популярными методами
подгонки.
Поиск модели для поиска прямой методом наименьших квадратов
Начнем с метода наименьших квадратов. Этот метод является стандартным
подходом в регрессионном анализе для аппроксимации решения переопреде-
ленных систем, когда уравнений больше, чем неизвестных. Ключевая идея
22
Высшая школа цифровой культуры Университет ИТМО
метода - минимизировать сумму квадратов отклонений, полученных в ре-
зультате каждого отдельного уравнения.
В применении к подгонке модели к данным, отклонение - это разница
между наблюдаемым и подобранным значением, полученными с помощью
модели. Метод наименьших квадратов - это особый вид минимизации, цель
которого - минимизировать квадраты отклонений.
Давайте начнем с самого начала. Наша цель - найти параметры моде-
лирующей функции, которые лучше всего подходят для имеющегося набора
данных. Набор данных состоит из𝑛 точек (пар данных):(𝑥1, 𝑦1), . . . ,(𝑥𝑛, 𝑦𝑛).
Моделирующаяя функция имеет форму линейного уравнения:𝑦 = 𝑚𝑥 + 𝑏.
Цель состоит в том, чтобы найти значения параметров(𝑚, 𝑏) для модели,
которые "наилучшим образом"будут соответствовать данным. Подгонка мо-
дели к точке данных(𝑥𝑖, 𝑦𝑖) измеряется ее отклонением, определяемым как
разность между фактическим значением𝑦𝑖 и значением, предсказанным мо-
делью: 𝑟𝑖 = 𝑦𝑖 −𝑚𝑥𝑖 −𝑏. Наша цель - минимизировать сумму квадратов
отклонений для всех точек данных:
𝐸 =
𝑛∑︁
𝑖=1
𝑟2
𝑖 =
𝑛∑︁
𝑖=1
(𝑦𝑖 −𝑚𝑥𝑖 −𝑏)2 (16)
Если мы переведем все в векторы и матрицы, мы можем представить это
уравнение в матричной форме:
𝐸 = ||𝑌 −𝑋𝐵||2 (17)
где
𝑌 =
⎡
⎣
𝑦1
...
𝑦𝑛
⎤
⎦, 𝑋 =
⎡
⎣
𝑥1 1
... ...
𝑥𝑛 1
⎤
⎦, 𝐵=
[︂
𝑚
𝑏
]︂
(18)
Это выражение можно переписать следующим образом:
𝐸 = ||𝑌 −𝑋𝐵||2 = (𝑌 −𝑋𝐵)𝑇 (𝑌 −𝑋𝐵) = 𝑌 𝑇 𝑌 −2(𝑋𝐵)𝑇 𝑌 + (𝑋𝐵)𝑇 (𝑋𝐵)
(19)
Минимум 𝐸 можно найти, установив частную производную по𝐵 равной
нулю:
𝜕𝐸
𝜕𝐵 = 2𝑋𝑇 𝑋𝐵 −2𝑋𝑇 𝑌 = 0 (20)
Первыйчленв 𝐸,𝑌 𝑇 𝑌 независитот 𝐵,поэтомуегочастнаяпроизводная
по 𝐵 равна нулю, второй член дает нам−2𝑋𝑇 𝑌 , а последний дает2𝑋𝑇 𝑋𝐵.
Мы приходим к нормальному уравнению вида𝑋𝑇 𝑋𝐵 = 𝑋𝑇 𝑌 . Такие
уравненияназываютсянормальнымм,потомучтоонизадают,чтоотклонение
должно быть нормально (перпендикулярно) каждому вектору на отрезке𝑋.
23
Высшая школа цифровой культуры Университет ИТМО
Подставив в уравнение, мы получим
𝐵 = (𝑋𝑇 𝑋)−1𝑋𝑇 𝑌 (21)
Традиционный метод наименьших квадратов: недостатки Хотя
традиционный (также называемый обычным) метод наименьших квадратов
очень распространен и прост в использовании, у него есть некоторые пробле-
мы. Во-первых, он не инвариантен относительно вращения. Поскольку мы
минимизируем отклонения по оси𝑦, при повороте изображения отклонения
по оси 𝑦 изменятся. Во-вторых, он совсем не подходит для вертикальных
линий.
Также отметим, что традиционный метод наименьших квадратов асим-
метричен. Мы рассматриваем𝑌 и 𝑋 как зависимые переменные, причем при
этом предполагается, что независимая переменная не содержит ошибок.
Все эти проблемы могут быть решены с помощью другого варианта ме-
тода наименьших квадратов, называемого Метод наименьших полных квад-
ратов (Total Least Squares).
Метод наименьших полных квадратовМетод наименьших квадратов
и метод наименьших полных квадратов оценивают точность аппроксимации
по-разному: метод наименьших квадратов минимизирует сумму квадратов
вертикальных расстояний от точек данных до линии аппроксимации, а метод
наименьших полных квадратов минимизирует сумму квадратов ортогональ-
ных расстояний от точек данных до аппроксимирующей линии.
Возвращаясь к нашей задаче подгонки модели прямой, теперь мы мо-
жем переформулировать ее с помощью метода наименьших квадратов следу-
ющим образом: минимизировать сумму расстояний между точками(𝑥𝑖, 𝑦𝑖) и
прямой 𝑎𝑥 + 𝑏𝑦 = 𝑑, где(𝑎, 𝑏) - отрезок, перпендикулярный прямой, поэтому
𝑎2 + 𝑏2 = 1. Наша цель - найти такие(𝑎, 𝑏, 𝑑), которые минимизируют сумму
перпендикулярных расстояний:
𝐸 =
𝑛∑︁
𝑖=1
(𝑎𝑥𝑖 + 𝑏𝑦𝑖 −𝑑)2 (22)
Метод наименьших квадратов vs метод наименьших полных квад-
ратов На экране показана разница между прямыми, полученными по мето-
ду наименьших квадратов и методу наименьших полных квадратов, аппрок-
симирующие прямыми один и тот же набор точек. Крестики, расположенные
на прямых, представляют собой аппроксимацию данных с использованием
этих двух подходов подгонки. В случае использования метода наименьших
24
Высшая школа цифровой культуры Университет ИТМО
квадратов аппроксимация данных достигается путем корректировки толь-
ко второй координаты: исходные точки данных и их приближения на пря-
мой имеют одинаковое значение𝑥, и только значения𝑦 различны. В случае
использования метода наименьших полных квадратов аппроксимация дан-
ных получается путем корректировки обеих координат. Метод наименьших
полных квадратов в регрессии - это такой метод моделирования данных с
использованием метода наименьших квадратов, когда ошибки в откликах и
предикторах учитываются как для зависимых, так и для независимых пере-
менных.
Метод наименьших квадратов: устойчивость к шумуПри исполь-
зовании методов наименьших квадратов, как традиционного, так и метода
наименьших полных квадратов, возникает еще одна проблема. Они очень
чувствительны к шуму. Эти методы очень хорошо подгоняют модель прямой
линии к данным в том случае, когда все точки данных близки к линии, как
показано сейчас на экране.
Ноесливнашихданныхестьвыбросы,аппроксимацияметодомнаимень-
ших квадратов будет далека от идеала. Среднеквадратичная ошибка сильно
зависит от выбросов. Таким образом, ошибки в данных будут сильнее влиять
при использовании метода наименьших полных квадратов, чем при исполь-
зовании метода наименьших квадратов.
1.3.3 Устойчивые оценки
Когда имеется всего несколько выбросов, проблема может быть решена
с помощью устойчивых оценок. Устойчивые оценки основываются на устой-
чивых штрафных функциях, которые более «снисходительны» к точкам дан-
ных, значения которых значительно отличаются от аппроксимированнх при
помощи данной модели.
Общий подход состоит в том, чтобы найти параметры модели𝜃, которые
минимизируют сумму значений устойчивой функции для всех точек данных
𝑥𝑖: ∑︀
𝑖 𝜌(𝑟𝑖(𝑥𝑖, 𝜃), 𝜎). Устойчивая функция𝜌 является функцией от отклоне-
ний 𝑟𝑖 и параметра масштаба𝜎. На экране показан пример такой устойчивой
функции, 𝜌(𝑢, 𝜎) = 𝑢2
𝜎2+𝑢2 . Она ведет себя как квадрат расстояния для малых
значений отклонений𝑢, но отлична от него при больших значениях𝑢. Таким
образом, если в ваших данных всего несколько выбросов, с помощью устой-
чивой оценки вы сможете подогнать модель к выбросам, и влияние выбросов
на параметры модели будет минимизированно.
Устойчивая подгонка модели - это нелинейная оптимизация, которая вы-
полняется итеративно (в отличие от метода наименьших квадратов, который
может быть решен в аналитическом виде). Для инициализации можно ис-
25
Высшая школа цифровой культуры Университет ИТМО
пользовать метод наименьших квадратов.
К выбору параметра масштаба устойчивой функции следует отнестись
с осторожностью - при слишком маленьком значении ошибка будет почти
одинаковой для каждой точки, и поэтому будет трудно найти подходящую
аппроксимацию. Если значение масштаба слишком велико, функция будет
вести себя аналогично методу наименьших квадратов.
1.3.4 Консенсус случайной выборки (RANSAC)
Устойчивые M-оценки определенно могут помочь уменьшить влияние
выбросов, однако, если выбросов слишком много, такими методами трудно
достичь глобального оптимума. Более хорошим подходом часто является по-
иск начального набора исходных соответствий, то есть точек, которые согла-
суются с оценкой доминирующих параметров. Широко используемый подход
для решения этого вопроса называется RANSAC или стабильный метод оцен-
ки параметров модели на основе случайных выборок. Этот метод - основа для
подгонки модели при наличии выбросов.
Основные этапы этого метода заключаются в следующем.
• Нужно случайным образом выбрать некоторое равномерной подмноже-
ство точек
• Подогнать модель к этому подмножеству
• Найти все оставшиеся точки, которые «близки» к модели - подсчитать
количество точек, которые находятся в пределах𝜖 от их положения,
предсказанного моделью. Отбросить остальные точки как выбросы.
• Повторить эти шаги некоторое количество раз и выбрать лучшую мо-
дель с наибольшим количеством точек, хорошо описываемых моделью.
RANSAC: подгонка модели прямой Давайте посмотрим на пример
подгонки прямой линии с помощью RANSAC. Предположим, наши точки
данных выглядят так, как показано на экране. Вы можете видеть четкую
диагональную линию, идущую слева направо вверх. Но изображение зашум-
лено и мы видим много выбросов.
Метод наименьших квадратов не сможет найти хорошее решение из-за
большого количества выбросов.
С помощью RANSAC мы сначала случайным образом выбираем ми-
нимальное подмножество точек, показанное красной линией на экране. По-
скольку мы хотим в качестве модели использовать прямую, то минимальное
подмножество точек для определения прямой равно двум. На следующем
шаге мы подбираем модель для этого подмножества. Затем мы вычисляем
26
Высшая школа цифровой культуры Университет ИТМО
функцию ошибок для этой модели и всех имеющихся у нас точек данных.
Далее мы выбираем точки, которые соответствуют модели. Это точки, на-
ходящиеся на расстоянии 𝜖 от прямой, определенной моделью. Эти точки
показаны на слайде зеленым цветом.
Далее, мы выбираем точки, соответствующие модели. Это точки на
небольшом расстоянии𝜖 от прямой, определенной нашей моделью. Такие точ-
ки показаны на слайде зеленым цветом.
Мы повторяем этот цикл, начиная со случайной выборки минимального
подмножества точек, затем подгоняем модель под них и подсчитываем точки,
соответствующие модели. Сейчас на экране показана еще одна гипотетиче-
ская прямая.
Повторим еще раз. Снова повторим. И еще раз.
Затем мы выберем такую модель из всех возможных гипотез, которой
соответствует наибольшее количество точек.
Консенсус случайной выборки (RANSAC) для поиска прямой ли-
нии Подводя итог, консенсус случайной выборки для поиска прямой линии
состоит из следующих шагов, повторяемых𝑁 раз.
• Случаным образом выберите𝑠 точек (равномерно). Обычно использу-
ется выборка из минимально возможного количества точек. Итак, для
выбора модели прямой выберем две точки.
• Выберите прямую, подходящую к этим𝑠 точкам.
• Среди оставшихся точек выберите точки, которые описываются моде-
лью: найдите точки, расстояние от которых до прямой меньше𝜖.
• Если вы получили𝑑 или больше точек, которые описываются моделью,
эта прямая принимается и нужно заново ее пересчитать, используя все
точки, принятые моделью.
Плюсы и минусы RANSAC Консенсус случайной выборки - это про-
стой, общеизвестный и устойчивый метод оценки. С его помошью можно с
высокой точностью оценить параметры модели даже в случае наличия боль-
шого количества выбросов в данных. Он широко используется и применяется
для решения множества различных задач, не только для подбора моделей.
Зачастую, он хорошо работает и на практике.
Но у него есть и недостатки. Как вы видели на предыдущем слайде на
экране, он зависит от большого количества параметров. Существуют под-
ходы для определения наилучшего количества итераций, количества точек,
27
Высшая школа цифровой культуры Университет ИТМО
которые нужно выбрать за итерацию, правильного расстояния для фильтра-
ции выбросов и т.д. И зачастую выбор правильных параметров очень важен,
т.к. неправильный выбор параметров может привести к слишком длитель-
ным вычислениям или некорректным результатам. Если количество итера-
ций ограничено, полученный результат может быть неоптимальным, и может
недостаточно точно описывать имеющиеся данные. RANSAC предлагает ком-
промисс; при большом количестве итераций вероятность создания хорошей
модели увеличивается.
Этот метод плохо работает, если выбросов слишком мало или слишком
много. Консенсус случайной выборки не всегда может найти оптимальный
набор даже для умеренно зашумленных наборов, и обычно он плохо работает,
когда количество данных, хорошо описываемых моделью, меньше 50
Для некоторых моделей может быть сложно найти хорошую инициали-
зацию модели на основе минимального количества элементов выборки.
Методы выбора и поиск моделиМы обсудили несколько самых попу-
лярных методов подбора модели. Метод наименьших квадратов и его моди-
фикации - одни из самых простых методов, но если выбросов слишком много,
они не будут работать хорошо. В случае аппроксимации прямой метод наи-
меньших квадратов хорошо работает, если большинство точек данных рас-
положены близко к одной прямой. Если в данных много выбросов или если
точки данных образуют несколько прямых, методы наименьших квадратов
не работают.
Методы устойчивых оценок менее чувствительны к выбросам, но все рав-
но не работают, если выбосов слишком много.
RANSAC - очень популярный подход, который достаточно хорошо справ-
ляется с выбросами. Но обычный консенсус случайной выборки может оце-
нить только одну модель для определенного набора данных. Если данные
предпологают существование двух (или более) моделей, консенсус случайной
выборки в лучшем случае обнаружит только одну из них или даже может не
найти ни одной.
1.3.5 Что если прямых несколько?
Каквыможетевидетьнаэтомэкране,методынаименьшихквадратовсо-
вершеннонеработаютприналичиинесколькихмоделейивыбросов,аконсен-
сус случайной выборки способен обнаруживать одну модель – одну прямую
из пяти. Вероятно, эта прямая описывает большее количество данных, чем
другие линии. Или этой прямой просто "повезло больше"и поэиому она была
обнаружена в течение заданного количества итераций алгоритмом RANSAC.
28
Высшая школа цифровой культуры Университет ИТМО
Выбор модели Метод RANSAC может быть адаптирован к случаю, когда
в данных можно обнаружить несколько моделей. Одна из наиболее простых
модификаций RANSAC для случая с несколькими моделями обычно называ-
ется последовательным консенсусом случайной выборки. Еще один альтерна-
тивныйустойчивыйметодоценки-этопреобразованиеХафа,которойобычно
используется, если в данных присутствует более одной модели. Существуют
и другие подходы для случая, когда в данных присутствует несколько моде-
лей, но сегодня мы сосредоточимся на двух вышеупомянутых методах, как
на наиболее широко известных. Их обычно можно назвать процедурами го-
лосования, поскольку они основаны на понятии «голосования» точек данных
за конкретную модель.
1.3.6 Методы голосования
Основной принцип всех методов голосования одинаков. Каждая точка
данных вносит свой вклад, т.е. голосует, за все модели, которые подходят для
описания этой точки. Побеждает модель, которая наберет наибольшее коли-
чество голосов. В случае, если у нас несколькими моделей в изображении, мы
можем выбрать не одну лучшую модель, а несколько, т.е. выбрать несколько
моделей с количеством голосов выше определенного порога. Все выбранные
модели будут лучшими и будут представлять собой несколько экземпляров
одного и того же объекта, присутствующего на изображении.
Методы голосования предполагают, что точки выбросов или шума не
будут голосовать последовательно за какую-либо отдельную модель, скорее
ихголосабудутраспределятьсяболееилименееравномерноповсеммоделям.
И «хорошая» модель получит больше голосов, потому что она получит голоса
точек, хорошо описываемых моделью, в дополнение к случайным голосам
точек шума.
Еще одним приятным свойством методов голосования является то, что
они устойчивы к пропускам в данных. Если имеется достаточно точек данных
для построения хорошей модели, отсутствующие данные не имеют значения.
1.3.7 Последовательный консенсус случайной выборки
(RANSAC)
RANSAC по своей методике является методом голосования. Но, как мы
видели, в первоначальной формулировке он подходит только для одной моде-
ли. Как мы можем изменить его, чтобы он обнаруживал и несколько экзем-
пляров модели? Последовательный консенсус случайной выборки - отличный
способ. Его идея очень проста. Мы будем использовать обычный консенсус
случайной выборки для создания модели. Когда у нас появится первая мо-
дель, давайте удалим все точки данных, которые описываются этой моделью.
29
Высшая школа цифровой культуры Университет ИТМО
Как показано на экране, если консенсус случайной выборки обнаружил крас-
ную линию, давайте удалим все точки, описываемые этой прямой и начнем с
нуля.
После удаления точек, соответствующих первой модели, второй запуск
RANSAC найдет нам другую модель со следующим по количеству числом
проголосовавших. Затем мы удалим точки данных, соответствующие этой
модели, и снова запустим RANSAC.
Думаю, вы уловили идею. Удалив все точки данных, соответствующие
уже обнаруженным моделям, мы используем RANSAC для поиска следую-
щей модели. Мы можем продолжать до тех пор, пока количество голосов за
лучшую модель не опустится ниже определенного порога, или до тех пор,
когда больше не будет явного победителя, т.е. когда все кандидаты в модели
получают примерно одинаковое количество голосов.
1.3.8 Преобразование Хафа
Другой очень популярный метод подбора модели, основанный на методе
голосования, - это преобразование Хафа. Вероятно, это наиболее известный
метод обнаружения прямых, и также его можно использовать для обнаруже-
ния других форм. Давайте посмотрим, как он работает.
В исходной формулировке каждая точка голосует за все возможные пря-
мые, проходящие через нее, а затем прямые, соответствующие большим зна-
чениям счетчика или ячейки, исследуются на предмет того, являются ли они
моделью.
Пусть(𝑥1, 𝑦1) обозначает точку на плоскости xy. Рассмотрим общее урав-
нение прямой с угловым коэффициентом: 𝑦 = 𝑚𝑥 + 𝑏. Бесконечно много
прямых проходит через точку(𝑥1, 𝑦1), но все они удовлетворяют уравнению
𝑦1 = 𝑚𝑥1 + 𝑏 для различных значений𝑚 и 𝑏. Перепишем это уравнение в ви-
де 𝑏 = −𝑥1𝑚 + 𝑦1 и рассмотрим mb-плоскость. Это пространство параметров
- пространство всех возможных значений параметров𝑚 и 𝑏. Мы получили
уравнение одной прямой в этом пространстве с фиксированными значениями
(𝑥1, 𝑦1).
Теперь добавим вторую точку на плоскости xy,(𝑥2, 𝑦2). С ней также свя-
зана одна прямая в пространстве параметров,𝑏 = −𝑥2𝑚 + 𝑦2. Эта прямая
пересекает прямую, связанную с первой точкой(𝑥1, 𝑦1) в некоторой точке
(𝑚0, 𝑏0) в пространстве параметров 𝑚𝑏. 𝑚0 - угловой коэффициент, а𝑏0 -
точка пересечения с осью ординат прямой, содержащей обе точки(𝑥1, 𝑦1) и
(𝑥 −2, 𝑦2) в плоскости xy. Итак, единственной точке(𝑚0, 𝑏0) в пространстве
параметров соответствует прямая𝑦 = 𝑚0𝑥 + 𝑏0. И все точки на этой пря-
мой в плоскости xy будут иметь прямые в пространстве параметров, которые
пересекаются в(𝑚0, 𝑏0).
30
Высшая школа цифровой культуры Университет ИТМО
Теперь легко понять основной принцип преобразования Хафа. Разобьем
пространство параметров𝑚𝑏 на ячейки. Затем для каждой характерной точ-
ки на изображении проголосуем за каждую ячейку в пространстве парамет-
ров, которая могла бы сгенерировать эту точку. И, наконец, найдем ячейки,
за которые было отдано больше всего голосов. Значения𝑚 и 𝑏, соответству-
ющие этим ячейкам, определяют обнаруженные на исходном изображении
прямые.
Представление пространства параметров Этот подход легко понять и
реализовать, но есть пара проблем с mb-пространством. Во-первых, оно име-
ет неограниченные области параметров. Оба параметра,𝑚 и 𝑏, могут иметь
любые значения от−∞до +∞. Это затрудняет разделение пространства на
ограниченное количество ячеек. Во-вторых,𝑚, угловой коэффициент, при-
ближается к бесконечности, когда прямая в xy-пространстве приближается к
вертикальному направлению.
Один из способов обойти эти трудности - использовать нормальное (или
полярное) представление прямой:𝑥 cos(𝜃) +𝑦 sin(𝜃) = 𝜌. На слайде представ-
лена геометрическая интерпретация параметров𝜌 и 𝜃. Вертикальная прямая
имеет 𝜃 = 0∘, где𝜌 - это пересечение с осью𝑥 при 𝑥 больше нуля. Точно так
же горизовнтальная прямая имеет𝜃 = 90∘, где 𝜌 - это пересечению с осью
𝑦 при 𝑦 больше нуля, или𝜃 = −90∘, где𝜌 - это пересечение с осью𝑦, при𝑦
меньше нуля.
Преобразование Хафа в полярных координатахПодобно простран-
ству параметров 𝑚𝑏, мы можем рассмотреть пространство параметров𝜃𝜌.
Каждая точка в плоскости𝑥𝑦 будет иметь соответствующую синусоидаль-
ную кривую в плоскости𝜃𝜌, и если взять несколько точек, лежащих на од-
ной прямой в пространстве𝑥𝑦, их соответствующие синусоидальные кривые
в пространстве𝜃𝜌 будут пересекаться в одной точке, и координаты(𝜃, 𝜌) этого
пересечения задают уравнние прямой в пространстве𝑥𝑦.
Чтобы преобразование Хафа было привлекательным с вычислительной
точки зрения, мы подразделяем пространство параметров 𝜃𝜌 на ячейки-
аккумуляторы. В отличие от пространства𝑚𝑏, теперь у нас есть четко опре-
деленные ожидаемые диапазоны значений𝜃 и 𝜌, (𝜃𝑚𝑖𝑛, 𝜃𝑚𝑎𝑥) и (𝜌𝑚𝑖𝑛, 𝜌𝑚𝑎𝑥).
Используя 𝜃𝑚𝑖𝑛 = −90∘и 𝜃𝑚𝑎𝑥 = 90∘, мы можем представить все возможные
углы наклона прямых. А с помощью−𝐷 ≤𝜌 ≤𝐷, где 𝐷 - максимальное
расстояние между противоположными углами изображения, мы можем най-
ти все возможные прямые на изображении.
Алгоритм преобразования Хафа Обрисуем алгоритм. Сначала зада-
дим все значения ячейки-аккумуляторов равными нулю. Затем для каждой
31
Высшая школа цифровой культуры Университет ИТМО
характерной точки (𝑥𝑘, 𝑦𝑘) в плоскости 𝑥𝑦 изображения мы перебираем все
допустимые значения𝜃 на оси𝜃 и ищем соответствующие𝜌 используя урав-
нение 𝜌 = 𝑥𝑘 cos(𝜃) + 𝑦𝑘 sin(𝜃). Затем полученные значения𝜌 округляются
до ближайшего допустимого значения ячейки по оси𝜌. Затем увеличьте зна-
чение аккумулятора ячейки(𝜃, 𝜌) на единицу. В конце процедуры значение
𝐾 в ячейке (𝜃𝑖, 𝜌𝑗) означает, что𝐾 точек в плоскости𝑥𝑦 лежат на прямой
𝑥 cos(𝜃𝑖) +𝑦 sin(𝜃𝑖) = 𝜌𝑗. После того, как мы повторили эту оперцию для всех
точек интереса, найдем аккумуляторы, соответствующие локальным макси-
мумам. Если ячейка(𝜃𝑖, 𝜌𝑗) соответствует локальному максимуму, то обнару-
женная на изображении прямая задается уравнением𝑥 cos(𝜃𝑖)+ 𝑦 sin(𝜃𝑖) = 𝜌𝑗.
Количество делений в плоскости𝜃𝜌 (или размер сетки) определяет точ-
ность коллинеарности точек. Можно показать, что количество вычислений в
этом методе линейно относительно𝑛, т.е. количества характерных точек на
плоскости 𝑥𝑦 изображения.
Хотя в данном случае мы искали прямые на изображении в качестве при-
мера применения преобразования Хафа, оно применимо к любой функции
вида 𝑔(𝑣, 𝑐) = 0 , где𝑣 - вектор координат, а𝑐 - вектор коэффициентов. Су-
ществуют также улучшенные версии этого метода, когда градиентные ориен-
тации краевых точек учитываются при переборе возможных значений𝑡ℎ𝑒𝑡𝑎.
До эры глубокого обучения метод обобщенного преобразования Хафа был
одним из популярных методов обнаружения объектов с помощью шаблонов.
32
